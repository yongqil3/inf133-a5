{"ast":null,"code":"import _asyncToGenerator from \"/Users/ryanliang/Downloads/main_movir_picker/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { SGDOptimizer } from './sgd_optimizer';\n/** @doclink Optimizer */\n\nexport class MomentumOptimizer extends SGDOptimizer {\n  constructor(learningRate, momentum, useNesterov = false) {\n    super(learningRate);\n    this.learningRate = learningRate;\n    this.momentum = momentum;\n    this.useNesterov = useNesterov;\n    this.accumulations = [];\n    this.m = scalar(this.momentum);\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n\n      if (this.accumulations[i] == null) {\n        const trainable = false;\n        this.accumulations[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const accumulation = this.accumulations[i].variable;\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      tidy(() => {\n        let newValue;\n        const newAccumulation = add(mul(this.m, accumulation), gradient);\n\n        if (this.useNesterov) {\n          newValue = add(mul(this.c, add(gradient, mul(newAccumulation, this.m))), value);\n        } else {\n          newValue = add(mul(this.c, newAccumulation), value);\n        }\n\n        accumulation.assign(newAccumulation);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    this.m.dispose();\n\n    if (this.accumulations != null) {\n      dispose(this.accumulations.map(v => v.variable));\n    }\n  }\n  /**\n   * Sets the momentum of the optimizer.\n   *\n   * @param momentum\n   */\n\n\n  setMomentum(momentum) {\n    this.momentum = momentum;\n  }\n\n  getWeights() {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      // Order matters for Python compatibility.\n      return [yield _this.saveIterations()].concat(_this.accumulations.map(v => ({\n        name: v.originalName,\n        tensor: v.variable\n      })));\n    })();\n  }\n\n  setWeights(weightValues) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      weightValues = yield _this2.extractIterations(weightValues);\n      const trainable = false;\n      _this2.accumulations = weightValues.map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'momentum': this.momentum,\n      'useNesterov': this.useNesterov\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['momentum'], config['useNesterov']);\n  }\n\n}\n/** @nocollapse */\n\nMomentumOptimizer.className = 'Momentum'; // Name matters for Python compatibility.\n\nregisterClass(MomentumOptimizer);","map":{"version":3,"sources":["/Users/ryanliang/Downloads/main_movir_picker/node_modules/@tensorflow/tfjs-core/dist/optimizers/momentum_optimizer.js"],"names":["ENGINE","dispose","tidy","add","mul","scalar","zerosLike","registerClass","SGDOptimizer","MomentumOptimizer","constructor","learningRate","momentum","useNesterov","accumulations","m","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","originalName","variable","accumulation","gradient","tensor","newValue","newAccumulation","c","assign","incrementIterations","v","setMomentum","getWeights","saveIterations","concat","setWeights","weightValues","extractIterations","getConfig","fromConfig","cls","config","className"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,MAAT,QAAuB,WAAvB;AACA,SAASC,OAAT,EAAkBC,IAAlB,QAA8B,YAA9B;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,MAAT,QAAuB,eAAvB;AACA,SAASC,SAAT,QAA0B,mBAA1B;AACA,SAASC,aAAT,QAA8B,kBAA9B;AACA,SAASC,YAAT,QAA6B,iBAA7B;AACA;;AACA,OAAO,MAAMC,iBAAN,SAAgCD,YAAhC,CAA6C;AAChDE,EAAAA,WAAW,CAACC,YAAD,EAAeC,QAAf,EAAyBC,WAAW,GAAG,KAAvC,EAA8C;AACrD,UAAMF,YAAN;AACA,SAAKA,YAAL,GAAoBA,YAApB;AACA,SAAKC,QAAL,GAAgBA,QAAhB;AACA,SAAKC,WAAL,GAAmBA,WAAnB;AACA,SAAKC,aAAL,GAAqB,EAArB;AACA,SAAKC,CAAL,GAASV,MAAM,CAAC,KAAKO,QAAN,CAAf;AACH;;AACDI,EAAAA,cAAc,CAACC,iBAAD,EAAoB;AAC9B,UAAMC,aAAa,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IAClBA,iBAAiB,CAACI,GAAlB,CAAsBC,IAAI,IAAIA,IAAI,CAACC,IAAnC,CADkB,GAElBC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;AAGAC,IAAAA,aAAa,CAACQ,OAAd,CAAsB,CAACH,IAAD,EAAOI,CAAP,KAAa;AAC/B,YAAMC,KAAK,GAAG5B,MAAM,CAAC6B,mBAAP,CAA2BN,IAA3B,CAAd;;AACA,UAAI,KAAKT,aAAL,CAAmBa,CAAnB,KAAyB,IAA7B,EAAmC;AAC/B,cAAMG,SAAS,GAAG,KAAlB;AACA,aAAKhB,aAAL,CAAmBa,CAAnB,IAAwB;AACpBI,UAAAA,YAAY,EAAG,GAAER,IAAK,WADF;AAEpBS,UAAAA,QAAQ,EAAE9B,IAAI,CAAC,MAAMI,SAAS,CAACsB,KAAD,CAAT,CAAiBI,QAAjB,CAA0BF,SAA1B,CAAP;AAFM,SAAxB;AAIH;;AACD,YAAMG,YAAY,GAAG,KAAKnB,aAAL,CAAmBa,CAAnB,EAAsBK,QAA3C;AACA,YAAME,QAAQ,GAAGf,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACU,CAAD,CAAjB,CAAqBQ,MADR,GAEblB,iBAAiB,CAACM,IAAD,CAFrB;;AAGA,UAAIW,QAAQ,IAAI,IAAhB,EAAsB;AAClB;AACH;;AACDhC,MAAAA,IAAI,CAAC,MAAM;AACP,YAAIkC,QAAJ;AACA,cAAMC,eAAe,GAAGlC,GAAG,CAACC,GAAG,CAAC,KAAKW,CAAN,EAASkB,YAAT,CAAJ,EAA4BC,QAA5B,CAA3B;;AACA,YAAI,KAAKrB,WAAT,EAAsB;AAClBuB,UAAAA,QAAQ,GAAGjC,GAAG,CAACC,GAAG,CAAC,KAAKkC,CAAN,EAASnC,GAAG,CAAC+B,QAAD,EAAW9B,GAAG,CAACiC,eAAD,EAAkB,KAAKtB,CAAvB,CAAd,CAAZ,CAAJ,EAA2Da,KAA3D,CAAd;AACH,SAFD,MAGK;AACDQ,UAAAA,QAAQ,GAAGjC,GAAG,CAACC,GAAG,CAAC,KAAKkC,CAAN,EAASD,eAAT,CAAJ,EAA+BT,KAA/B,CAAd;AACH;;AACDK,QAAAA,YAAY,CAACM,MAAb,CAAoBF,eAApB;AACAT,QAAAA,KAAK,CAACW,MAAN,CAAaH,QAAb;AACH,OAXG,CAAJ;AAYH,KA5BD;AA6BA,SAAKI,mBAAL;AACH;;AACDvC,EAAAA,OAAO,GAAG;AACN,SAAKc,CAAL,CAAOd,OAAP;;AACA,QAAI,KAAKa,aAAL,IAAsB,IAA1B,EAAgC;AAC5Bb,MAAAA,OAAO,CAAC,KAAKa,aAAL,CAAmBO,GAAnB,CAAuBoB,CAAC,IAAIA,CAAC,CAACT,QAA9B,CAAD,CAAP;AACH;AACJ;AACD;AACJ;AACA;AACA;AACA;;;AACIU,EAAAA,WAAW,CAAC9B,QAAD,EAAW;AAClB,SAAKA,QAAL,GAAgBA,QAAhB;AACH;;AACK+B,EAAAA,UAAU,GAAG;AAAA;;AAAA;AACf;AACA,aAAO,OAAO,KAAI,CAACC,cAAL,EAAP,EAA8BC,MAA9B,CAAqC,KAAI,CAAC/B,aAAL,CAAmBO,GAAnB,CAAuBoB,CAAC,KAAK;AAAElB,QAAAA,IAAI,EAAEkB,CAAC,CAACV,YAAV;AAAwBI,QAAAA,MAAM,EAAEM,CAAC,CAACT;AAAlC,OAAL,CAAxB,CAArC,CAAP;AAFe;AAGlB;;AACKc,EAAAA,UAAU,CAACC,YAAD,EAAe;AAAA;;AAAA;AAC3BA,MAAAA,YAAY,SAAS,MAAI,CAACC,iBAAL,CAAuBD,YAAvB,CAArB;AACA,YAAMjB,SAAS,GAAG,KAAlB;AACA,MAAA,MAAI,CAAChB,aAAL,GAAqBiC,YAAY,CAAC1B,GAAb,CAAiBoB,CAAC,KAAK;AAAEV,QAAAA,YAAY,EAAEU,CAAC,CAAClB,IAAlB;AAAwBS,QAAAA,QAAQ,EAAES,CAAC,CAACN,MAAF,CAASH,QAAT,CAAkBF,SAAlB;AAAlC,OAAL,CAAlB,CAArB;AAH2B;AAI9B;;AACDmB,EAAAA,SAAS,GAAG;AACR,WAAO;AACH,sBAAgB,KAAKtC,YADlB;AAEH,kBAAY,KAAKC,QAFd;AAGH,qBAAe,KAAKC;AAHjB,KAAP;AAKH;AACD;;;AACiB,SAAVqC,UAAU,CAACC,GAAD,EAAMC,MAAN,EAAc;AAC3B,WAAO,IAAID,GAAJ,CAAQC,MAAM,CAAC,cAAD,CAAd,EAAgCA,MAAM,CAAC,UAAD,CAAtC,EAAoDA,MAAM,CAAC,aAAD,CAA1D,CAAP;AACH;;AA7E+C;AA+EpD;;AACA3C,iBAAiB,CAAC4C,SAAlB,GAA8B,UAA9B,C,CAA0C;;AAC1C9C,aAAa,CAACE,iBAAD,CAAb","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { SGDOptimizer } from './sgd_optimizer';\n/** @doclink Optimizer */\nexport class MomentumOptimizer extends SGDOptimizer {\n    constructor(learningRate, momentum, useNesterov = false) {\n        super(learningRate);\n        this.learningRate = learningRate;\n        this.momentum = momentum;\n        this.useNesterov = useNesterov;\n        this.accumulations = [];\n        this.m = scalar(this.momentum);\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        variableNames.forEach((name, i) => {\n            const value = ENGINE.registeredVariables[name];\n            if (this.accumulations[i] == null) {\n                const trainable = false;\n                this.accumulations[i] = {\n                    originalName: `${name}/momentum`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            const accumulation = this.accumulations[i].variable;\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            tidy(() => {\n                let newValue;\n                const newAccumulation = add(mul(this.m, accumulation), gradient);\n                if (this.useNesterov) {\n                    newValue = add(mul(this.c, add(gradient, mul(newAccumulation, this.m))), value);\n                }\n                else {\n                    newValue = add(mul(this.c, newAccumulation), value);\n                }\n                accumulation.assign(newAccumulation);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        this.m.dispose();\n        if (this.accumulations != null) {\n            dispose(this.accumulations.map(v => v.variable));\n        }\n    }\n    /**\n     * Sets the momentum of the optimizer.\n     *\n     * @param momentum\n     */\n    setMomentum(momentum) {\n        this.momentum = momentum;\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        return [await this.saveIterations()].concat(this.accumulations.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        const trainable = false;\n        this.accumulations = weightValues.map(v => ({ originalName: v.name, variable: v.tensor.variable(trainable) }));\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'momentum': this.momentum,\n            'useNesterov': this.useNesterov\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['momentum'], config['useNesterov']);\n    }\n}\n/** @nocollapse */\nMomentumOptimizer.className = 'Momentum'; // Name matters for Python compatibility.\nregisterClass(MomentumOptimizer);\n"]},"metadata":{},"sourceType":"module"}