{"ast":null,"code":"import _asyncToGenerator from \"/Users/ryanliang/Downloads/main_movir_picker/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { KernelBackend } from './backends/backend';\nimport { Environment, setEnvironmentGlobal } from './environment';\nimport { getGlobalNamespace } from './global_util';\nimport { Add, Cast, Identity } from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\nimport { Profiler } from './profiler';\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\nimport { setTensorTracker, Tensor, Variable } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\nimport * as util from './util';\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\n\nfunction isRegisteredKernelInvocation(kernelInvocation) {\n  return kernelInvocation.kernelName != null;\n}\n\nclass EngineState {\n  constructor() {\n    // Public since optimizers will use it.\n    this.registeredVariables = {};\n    this.nextTapeNodeId = 0;\n    this.numBytes = 0;\n    this.numTensors = 0;\n    this.numStringTensors = 0;\n    this.numDataBuffers = 0; // Number of nested tf.grad() statements when computing higher-order\n    // gradients. E.g. `1` for first-order gradients and `2` for second-order\n    // gradients. Used to track if the tape should be removed after a backprop.\n\n    this.gradientDepth = 0; // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n    // off the tape.\n\n    this.kernelDepth = 0;\n    this.scopeStack = [];\n    /**\n     * Keeps track of the number of data moves during a kernel execution. We\n     * maintain a stack since kernels can call other kernels, recursively.\n     */\n\n    this.numDataMovesStack = [];\n    this.nextScopeId = 0;\n    this.tensorInfo = new WeakMap();\n    this.profiling = false;\n    this.activeProfile = {\n      newBytes: 0,\n      newTensors: 0,\n      peakBytes: 0,\n      kernels: [],\n      result: null,\n\n      get kernelNames() {\n        return Array.from(new Set(this.kernels.map(k => k.name)));\n      }\n\n    };\n  }\n\n  dispose() {\n    for (const variableName in this.registeredVariables) {\n      this.registeredVariables[variableName].dispose();\n    }\n  }\n\n}\n\nexport class Engine {\n  constructor(ENV) {\n    this.ENV = ENV;\n    this.registry = {};\n    this.registryFactory = {};\n    this.pendingBackendInitId = 0;\n    this.state = new EngineState();\n  }\n\n  ready() {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this.pendingBackendInit != null) {\n        return _this.pendingBackendInit.then(() => {});\n      }\n\n      if (_this.backendInstance != null) {\n        return;\n      }\n\n      const sortedBackends = _this.getSortedBackends();\n\n      for (let i = 0; i < sortedBackends.length; i++) {\n        const backendName = sortedBackends[i];\n        const success = yield _this.initializeBackend(backendName).success;\n\n        if (success) {\n          yield _this.setBackend(backendName);\n          return;\n        }\n      }\n\n      throw new Error(`Could not initialize any backends, all backend initializations ` + `failed.`);\n    })();\n  }\n\n  get backend() {\n    if (this.pendingBackendInit != null) {\n      throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make ` + `sure to await tf.ready() or await tf.setBackend() before calling ` + `other methods`);\n    }\n\n    if (this.backendInstance == null) {\n      const {\n        name,\n        asyncInit\n      } = this.initializeBackendsAndReturnBest();\n\n      if (asyncInit) {\n        throw new Error(`The highest priority backend '${name}' has not yet been ` + `initialized. Make sure to await tf.ready() or ` + `await tf.setBackend() before calling other methods`);\n      }\n\n      this.setBackend(name);\n    }\n\n    return this.backendInstance;\n  }\n\n  backendNames() {\n    return Object.keys(this.registryFactory);\n  }\n\n  findBackend(backendName) {\n    if (!(backendName in this.registry)) {\n      // If the backend hasn't been initialized but we have a registry entry for\n      // it, initialize it and return it.\n      if (backendName in this.registryFactory) {\n        const {\n          asyncInit\n        } = this.initializeBackend(backendName);\n\n        if (asyncInit) {\n          // Backend is not ready yet.\n          return null;\n        }\n      } else {\n        return null;\n      }\n    }\n\n    return this.registry[backendName];\n  }\n\n  findBackendFactory(backendName) {\n    if (!(backendName in this.registryFactory)) {\n      return null;\n    }\n\n    return this.registryFactory[backendName].factory;\n  }\n\n  registerBackend(backendName, factory, priority = 1) {\n    if (backendName in this.registryFactory) {\n      console.warn(`${backendName} backend was already registered. ` + `Reusing existing backend factory.`);\n      return false;\n    }\n\n    this.registryFactory[backendName] = {\n      factory,\n      priority\n    };\n    return true;\n  }\n\n  setBackend(backendName) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this2.registryFactory[backendName] == null) {\n        throw new Error(`Backend name '${backendName}' not found in registry`);\n      }\n\n      _this2.backendName = backendName;\n\n      if (_this2.registry[backendName] == null) {\n        _this2.backendInstance = null;\n\n        const {\n          success,\n          asyncInit\n        } = _this2.initializeBackend(backendName);\n\n        const result = asyncInit ? yield success : success;\n\n        if (!result) {\n          return false;\n        }\n      }\n\n      _this2.backendInstance = _this2.registry[backendName];\n\n      _this2.setupRegisteredKernels(); // Reset the profiler.\n\n\n      _this2.profiler = new Profiler(_this2.backendInstance);\n      return true;\n    })();\n  }\n\n  setupRegisteredKernels() {\n    const kernels = getKernelsForBackend(this.backendName);\n    kernels.forEach(kernel => {\n      if (kernel.setupFunc != null) {\n        kernel.setupFunc(this.backendInstance);\n      }\n    });\n  }\n\n  disposeRegisteredKernels(backendName) {\n    const kernels = getKernelsForBackend(backendName);\n    kernels.forEach(kernel => {\n      if (kernel.disposeFunc != null) {\n        kernel.disposeFunc(this.registry[backendName]);\n      }\n    });\n  }\n  /**\n   * Initializes a backend by looking up the backend name in the factory\n   * registry and calling the factory method. Returns a boolean representing\n   * whether the initialization of the backend suceeded. Throws an error if\n   * there is no backend in the factory registry.\n   */\n\n\n  initializeBackend(backendName) {\n    const registryFactoryEntry = this.registryFactory[backendName];\n\n    if (registryFactoryEntry == null) {\n      throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);\n    }\n\n    try {\n      const backend = registryFactoryEntry.factory();\n      /* Test if the factory returns a promise.\n      Done in a more liberal way than\n      previous 'Promise.resolve(backend)===backend'\n      as we needed to account for custom Promise\n      implementations (e.g. Angular) */\n\n      if (backend && !(backend instanceof KernelBackend) && typeof backend.then === 'function') {\n        const promiseId = ++this.pendingBackendInitId;\n        const success = backend.then(backendInstance => {\n          // Outdated promise. Another backend was set in the meantime.\n          if (promiseId < this.pendingBackendInitId) {\n            return false;\n          }\n\n          this.registry[backendName] = backendInstance;\n          this.pendingBackendInit = null;\n          return true;\n        }).catch(err => {\n          // Outdated promise. Another backend was set in the meantime.\n          if (promiseId < this.pendingBackendInitId) {\n            return false;\n          }\n\n          this.pendingBackendInit = null;\n          console.warn(`Initialization of backend ${backendName} failed`);\n          console.warn(err.stack || err.message);\n          return false;\n        });\n        this.pendingBackendInit = success;\n        return {\n          success,\n          asyncInit: true\n        };\n      } else {\n        this.registry[backendName] = backend;\n        return {\n          success: true,\n          asyncInit: false\n        };\n      }\n    } catch (err) {\n      console.warn(`Initialization of backend ${backendName} failed`);\n      console.warn(err.stack || err.message);\n      return {\n        success: false,\n        asyncInit: false\n      };\n    }\n  }\n\n  removeBackend(backendName) {\n    if (!(backendName in this.registryFactory)) {\n      throw new Error(`${backendName} backend not found in registry`);\n    }\n\n    if (this.backendName === backendName && this.pendingBackendInit != null) {\n      // There is a pending promise of the backend we want to remove. Make it\n      // obsolete.\n      this.pendingBackendInitId++;\n    }\n\n    if (backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    delete this.registryFactory[backendName]; // Unset the backend if it is active.\n\n    if (this.backendName === backendName) {\n      this.pendingBackendInit = null;\n      this.backendName = null;\n      this.backendInstance = null;\n    }\n  }\n\n  getSortedBackends() {\n    if (Object.keys(this.registryFactory).length === 0) {\n      throw new Error('No backend found in registry.');\n    }\n\n    return Object.keys(this.registryFactory).sort((a, b) => {\n      // Highest priority comes first.\n      return this.registryFactory[b].priority - this.registryFactory[a].priority;\n    });\n  }\n\n  initializeBackendsAndReturnBest() {\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const {\n        success,\n        asyncInit\n      } = this.initializeBackend(backendName);\n\n      if (asyncInit || success) {\n        return {\n          name: backendName,\n          asyncInit\n        };\n      }\n    }\n\n    throw new Error(`Could not initialize any backends, all backend initializations ` + `failed.`);\n  }\n\n  moveData(backend, dataId) {\n    const info = this.state.tensorInfo.get(dataId);\n    const srcBackend = info.backend;\n    const values = this.readSync(dataId);\n    const refCount = srcBackend.refCount(dataId); // Delete the tensor from the old backend and move it to the new\n    // backend.\n\n    srcBackend.disposeData(dataId, true);\n    info.backend = backend;\n    backend.move(dataId, values, info.shape, info.dtype, refCount);\n\n    if (this.shouldCheckForMemLeaks()) {\n      // Track the number of moves during a kernel execution to correctly\n      // detect memory leaks.\n      this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n    }\n  }\n\n  tidy(nameOrFn, fn) {\n    let name = null;\n\n    if (fn == null) {\n      // Called with only 1 argument.\n      if (typeof nameOrFn !== 'function') {\n        throw new Error('Please provide a function to tidy()');\n      }\n\n      fn = nameOrFn;\n    } else {\n      // Called with 2 arguments.\n      if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n        throw new Error('When calling with two arguments, the first argument ' + 'to tidy() must be a string');\n      }\n\n      if (typeof fn !== 'function') {\n        throw new Error('When calling with two arguments, the 2nd argument ' + 'to tidy() must be a function');\n      }\n\n      name = nameOrFn; // TODO(nsthorat,smilkov): Do operation logging and performance\n      // profiling.\n    }\n\n    let result;\n    return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {\n      result = fn();\n\n      if (result instanceof Promise) {\n        console.error('Cannot return a Promise inside of tidy.');\n      }\n\n      return result;\n    });\n  }\n\n  scopedRun(start, end, f) {\n    start();\n\n    try {\n      const res = f();\n      end();\n      return res;\n    } catch (ex) {\n      end();\n      throw ex;\n    }\n  }\n\n  nextTensorId() {\n    return Engine.nextTensorId++;\n  }\n\n  nextVariableId() {\n    return Engine.nextVariableId++;\n  }\n  /**\n   * This method is called instead of the public-facing tensor.clone() when\n   * saving a tensor for backwards pass. It makes sure to add the clone\n   * operation to the tape regardless of being called inside a kernel\n   * execution.\n   */\n\n\n  clone(x) {\n    const y = ENGINE.runKernel(Identity, {\n      x\n    });\n    const inputs = {\n      x\n    };\n\n    const grad = dy => ({\n      x: () => {\n        const dtype = 'float32';\n        const gradInputs = {\n          x: dy\n        };\n        const attrs = {\n          dtype\n        };\n        return ENGINE.runKernel(Cast, gradInputs, // tslint:disable-next-line: no-unnecessary-type-assertion\n        attrs);\n      }\n    });\n\n    const saved = [];\n    this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n    return y;\n  }\n  /**\n   * Execute a kernel with the given name and return the output tensor.\n   *\n   * @param kernelName The name of the kernel to execute.\n   * @param inputs A map of input names to tensors.\n   * @param attrs A map of attribute names to their values. An attribute is a\n   *     primitive (non-tensor) input to the kernel.\n   * @param inputsToSave A list of tensors, inputs to save for the backprop\n   *     computation.\n   * @param outputsToSave A list of booleans, specifying which output to save\n   *     for the backprop computation. These are booleans since the output\n   * tensors are not visible to the user.\n   */\n\n\n  runKernel(kernelName, inputs, attrs) {\n    const hasKernel = getKernel(kernelName, this.backendName) != null;\n\n    if (!hasKernel) {\n      throw new Error(`Kernel '${kernelName}' not registered for backend '${this.backendName}'`);\n    }\n\n    return this.runKernelFunc({\n      kernelName,\n      inputs,\n      attrs\n    });\n  }\n\n  shouldCheckForMemLeaks() {\n    return this.ENV.getBool('IS_TEST');\n  }\n\n  checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\n    const numDataIdsAfter = this.backend.numDataIds(); // Count the number of data ids associated with the result of the kernel.\n\n    let numOutputDataIds = 0;\n    outInfos.forEach(info => {\n      // Complex numbers allocate 3 data ids, one for 'real', one for\n      // 'imaginary', and one for the container that holds the former two.\n      numOutputDataIds += info.dtype === 'complex64' ? 3 : 1;\n    }); // Account for the number of moves during kernel execution. A \"data move\"\n    // can happen in the middle of a kernel execution, placing a new (key,value)\n    // pair in the data storage. Since data moves have net zero effect (we\n    // always remove the data from the old backend), we have to cancel them out\n    // when detecting memory leaks.\n\n    const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n\n    if (dataIdsLeaked > 0) {\n      throw new Error(`Backend '${this.backendName}' has an internal memory leak ` + `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n    }\n  }\n  /**\n   * Internal helper method to execute a kernel Func\n   *\n   * Use `runKernel` to execute kernels from outside of engine.\n   */\n\n\n  runKernelFunc(kernelParams) {\n    let outputs;\n    let saved = [];\n    const isTapeOn = this.isTapeOn();\n    const startingBytecount = this.state.numBytes;\n    const startingNumTensors = this.state.numTensors;\n\n    if (this.shouldCheckForMemLeaks()) {\n      this.state.numDataMovesStack.push(0);\n    }\n\n    let kernelFunc;\n\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n\n    let out;\n    const kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ? kernelParams.kernelName : this.state.activeScope != null ? this.state.activeScope.name : ''; // Create the kernelFunc from either a registered kernel OR passed in\n    // forward/backward functions (used by custom grad). In this context a\n    // kernelFunc wraps a kernel implementation with some bookkeeping.\n\n    if (isRegisteredKernelInvocation(kernelParams)) {\n      const {\n        kernelName,\n        inputs,\n        attrs\n      } = kernelParams;\n\n      if (this.backendName == null) {\n        // backend has not been initialized yet (backend initialization is lazy\n        // can be deferred until an op/ kernel is run).\n        // The below getter has side effects that will try to initialize the\n        // backend and set properties like this.backendName\n        // tslint:disable-next-line: no-unused-expression\n        this.backend;\n      }\n\n      const kernel = getKernel(kernelName, this.backendName);\n      util.assert(kernel != null, () => `Cannot find registered kernel '${kernelName}' for backend '${this.backendName}'`);\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = kernel.kernelFunc({\n          inputs,\n          attrs,\n          backend: this.backend\n        });\n        const outInfos = Array.isArray(out) ? out : [out];\n\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n        }\n\n        const outTensors = outInfos.map(outInfo => {\n          // todo (yassogba) remove this option (Tensor) when node backend\n          // methods have been modularized and they all return tensorInfo.\n          // TensorInfos do not have a rank attribute.\n          if (outInfo.rank != null) {\n            return outInfo;\n          }\n\n          const {\n            dataId,\n            shape,\n            dtype\n          } = outInfo;\n          return this.makeTensorFromDataId(dataId, shape, dtype);\n        }); // Save any required inputs and outputs.\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since there would be no backprop for these tensors\n        // (which would otherwise dispose them).\n\n        if (isTapeOn) {\n          const tensorsToSave = this.getTensorsForGradient(kernelName, inputs, outTensors);\n          saved = this.saveTensorsForBackwardMode(tensorsToSave);\n        }\n\n        return outTensors;\n      };\n    } else {\n      const {\n        forwardFunc\n      } = kernelParams; // Running a customGrad op.\n\n      const saveFunc = tensors => {\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n        if (!isTapeOn) {\n          return;\n        }\n\n        saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n      };\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n        const outs = Array.isArray(out) ? out : [out];\n\n        if (this.shouldCheckForMemLeaks()) {\n          // Scope name is used to print a more helpful error message if needed.\n          this.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);\n        }\n\n        return outs;\n      };\n    } //\n    // Run the kernelFunc. Optionally profiling it.\n    //\n\n\n    const {\n      inputs,\n      attrs\n    } = kernelParams;\n    const backwardsFunc = isRegisteredKernelInvocation(kernelParams) ? null : kernelParams.backwardsFunc;\n    let kernelProfile;\n    this.scopedRun( // Stop recording to a tape when running a kernel.\n    () => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n      if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n        outputs = kernelFunc();\n      } else {\n        kernelProfile = this.profiler.profileKernel(kernelOrScopeName, inputs, () => kernelFunc());\n\n        if (this.ENV.getBool('DEBUG')) {\n          this.profiler.logKernelProfile(kernelProfile);\n        }\n\n        outputs = kernelProfile.outputs;\n      }\n    });\n\n    if (isTapeOn) {\n      this.addTapeNode(kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);\n    }\n\n    if (this.state.profiling) {\n      this.state.activeProfile.kernels.push({\n        name: kernelOrScopeName,\n        bytesAdded: this.state.numBytes - startingBytecount,\n        totalBytesSnapshot: this.state.numBytes,\n        tensorsAdded: this.state.numTensors - startingNumTensors,\n        totalTensorsSnapshot: this.state.numTensors,\n        inputShapes: Object.keys(inputs).map(key => inputs[key] != null ? inputs[key].shape : null),\n        outputShapes: outputs.map(item => item.shape),\n        kernelTimeMs: kernelProfile.timeMs,\n        extraInfo: kernelProfile.extraInfo\n      });\n    }\n\n    return Array.isArray(out) ? outputs : outputs[0];\n  }\n  /**\n   * Saves tensors used in forward mode for use in backward mode.\n   *\n   * @param tensors the list of tensors to save.\n   */\n\n\n  saveTensorsForBackwardMode(tensors) {\n    const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n    return saved;\n  }\n  /**\n   * Returns a list of tensors to save for a given gradient calculation.\n   *\n   * @param kernelName name of kernel to look up gradient for.\n   * @param inputs a map of input tensors.\n   * @param outputs an array of output tensors from forward mode of kernel.\n   */\n\n\n  getTensorsForGradient(kernelName, inputs, outputs) {\n    const gradConfig = getGradient(kernelName);\n\n    if (gradConfig != null) {\n      const inputsToSave = gradConfig.inputsToSave || [];\n      const outputsToSave = gradConfig.outputsToSave || []; // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n      // specified in inputsToSave will be saved.\n\n      let inputTensorsToSave;\n\n      if (gradConfig.saveAllInputs) {\n        util.assert(Array.isArray(inputs), () => 'saveAllInputs is true, expected inputs to be an array.');\n        inputTensorsToSave = Object.keys(inputs).map(key => inputs[key]);\n      } else {\n        inputTensorsToSave = inputsToSave.map(inputName => inputs[inputName]);\n      }\n\n      const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);\n      return inputTensorsToSave.concat(outputTensorsToSave);\n    } // We return an empty list rather than throw an error because the kernel we\n    // are looking up may not actually be relevant to backproping through the\n    // overall function\n    //\n    // See 'does not error if irrelevant (pruned) ops are missing grads' test\n    // in gradients_test.ts for an example.\n\n\n    return [];\n  }\n  /**\n   * Internal method used by public APIs for tensor creation. Makes a new\n   * tensor with the provided shape, dtype and values. It always\n   * creates a new data id and writes the values to the underlying backend.\n   */\n\n\n  makeTensor(values, shape, dtype, backend) {\n    if (values == null) {\n      throw new Error('Values passed to engine.makeTensor() are null');\n    }\n\n    dtype = dtype || 'float32';\n    backend = backend || this.backend;\n    let backendVals = values;\n\n    if (dtype === 'string' && util.isString(values[0])) {\n      backendVals = values.map(d => util.encodeString(d));\n    }\n\n    const dataId = backend.write(backendVals, shape, dtype);\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend); // Count bytes for string tensors.\n\n    if (dtype === 'string') {\n      const info = this.state.tensorInfo.get(dataId);\n      const newBytes = bytesFromStringArray(backendVals);\n      this.state.numBytes += newBytes - info.bytes;\n      info.bytes = newBytes;\n    }\n\n    return t;\n  }\n  /**\n   * Internal method used by backends. Makes a new tensor\n   * that is a wrapper around an existing data id. It doesn't create\n   * a new data id, only increments the ref count used in memory tracking.\n   */\n\n\n  makeTensorFromDataId(dataId, shape, dtype, backend) {\n    dtype = dtype || 'float32';\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend);\n    return t;\n  }\n\n  makeVariable(initialValue, trainable = true, name, dtype) {\n    name = name || this.nextVariableId().toString();\n\n    if (dtype != null && dtype !== initialValue.dtype) {\n      initialValue = initialValue.cast(dtype);\n    }\n\n    const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n\n    if (this.state.registeredVariables[v.name] != null) {\n      throw new Error(`Variable with name ${v.name} was already registered`);\n    }\n\n    this.state.registeredVariables[v.name] = v;\n    this.incRef(v, this.backend);\n    return v;\n  }\n\n  trackTensor(a, backend) {\n    this.state.numTensors++;\n\n    if (a.dtype === 'string') {\n      this.state.numStringTensors++;\n    } // Bytes for complex numbers are counted by their components. Bytes for\n    // string tensors are counted when writing values.\n\n\n    let bytes = 0;\n\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      bytes = a.size * util.bytesPerElement(a.dtype);\n    }\n\n    this.state.numBytes += bytes;\n\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      this.state.numDataBuffers++;\n      this.state.tensorInfo.set(a.dataId, {\n        backend: backend || this.backend,\n        dtype: a.dtype,\n        shape: a.shape,\n        bytes\n      });\n    }\n\n    if (!(a instanceof Variable)) {\n      this.track(a);\n    }\n  } // Track the tensor by dataId and increase the refCount for the dataId in the\n  // backend.\n  // TODO(pyu10055): This is currently used by makeVariable method, to increase\n  // refCount on the backend for the dataId. It can potentially be replaced with\n  // Identity op indead of calling backend directly.\n\n\n  incRef(a, backend) {\n    this.trackTensor(a, backend);\n    this.backend.incRef(a.dataId);\n  }\n\n  removeDataId(dataId, backend) {\n    if (this.state.tensorInfo.has(dataId) && this.state.tensorInfo.get(dataId).backend === backend) {\n      this.state.tensorInfo.delete(dataId);\n      this.state.numDataBuffers--;\n    }\n  }\n\n  disposeTensor(a) {\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      return;\n    }\n\n    const info = this.state.tensorInfo.get(a.dataId);\n    this.state.numTensors--;\n\n    if (a.dtype === 'string') {\n      this.state.numStringTensors--;\n      this.state.numBytes -= info.bytes;\n    } // Don't count bytes for complex numbers as they are counted by their\n    // components.\n\n\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      const bytes = a.size * util.bytesPerElement(a.dtype);\n      this.state.numBytes -= bytes;\n    } // Remove the reference to dataId if backend dispose the data successfully\n\n\n    if (info.backend.disposeData(a.dataId)) {\n      this.removeDataId(a.dataId, info.backend);\n    } // TODO(nsthorat): Construct an error and save the stack trace for\n    // debugging when in debug mode. Creating a stack trace is too expensive\n    // to do unconditionally.\n\n  }\n\n  disposeVariables() {\n    for (const varName in this.state.registeredVariables) {\n      const v = this.state.registeredVariables[varName];\n      this.disposeVariable(v);\n    }\n  }\n\n  disposeVariable(v) {\n    this.disposeTensor(v);\n\n    if (this.state.registeredVariables[v.name] != null) {\n      delete this.state.registeredVariables[v.name];\n    }\n  }\n\n  memory() {\n    const info = this.backend.memory();\n    info.numTensors = this.state.numTensors;\n    info.numDataBuffers = this.state.numDataBuffers;\n    info.numBytes = this.state.numBytes;\n\n    if (this.state.numStringTensors > 0) {\n      info.unreliable = true;\n\n      if (info.reasons == null) {\n        info.reasons = [];\n      }\n\n      info.reasons.push('Memory usage by string tensors is approximate ' + '(2 bytes per character)');\n    }\n\n    return info;\n  }\n\n  profile(query) {\n    var _this3 = this;\n\n    return _asyncToGenerator(function* () {\n      _this3.state.profiling = true;\n      const startBytes = _this3.state.numBytes;\n      const startNumTensors = _this3.state.numTensors;\n      _this3.state.activeProfile.kernels = [];\n      _this3.state.activeProfile.result = yield query();\n      _this3.state.profiling = false;\n      _this3.state.activeProfile.peakBytes = Math.max(..._this3.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n      _this3.state.activeProfile.newBytes = _this3.state.numBytes - startBytes;\n      _this3.state.activeProfile.newTensors = _this3.state.numTensors - startNumTensors;\n\n      for (const kernel of _this3.state.activeProfile.kernels) {\n        kernel.kernelTimeMs = yield kernel.kernelTimeMs;\n        kernel.extraInfo = yield kernel.extraInfo;\n      }\n\n      return _this3.state.activeProfile;\n    })();\n  }\n\n  isTapeOn() {\n    return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n  }\n\n  addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\n    const tapeNode = {\n      id: this.state.nextTapeNodeId++,\n      kernelName,\n      inputs,\n      outputs,\n      saved\n    };\n    const gradConfig = getGradient(kernelName);\n\n    if (gradConfig != null) {\n      gradientsFunc = gradConfig.gradFunc;\n    }\n\n    if (gradientsFunc != null) {\n      tapeNode.gradient = dys => {\n        // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n        // the backprop graph to the user as null instead of zeros\n        dys = dys.map((dy, i) => {\n          if (dy == null) {\n            const output = outputs[i];\n            const vals = util.makeZerosTypedArray(output.size, output.dtype);\n            return this.makeTensor(vals, output.shape, output.dtype);\n          }\n\n          return dy;\n        }); // Grad functions of ops with single outputs expect a dy, while ops\n        // with multiple outputs expect dys (array of dy).\n\n        return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n      };\n    }\n\n    this.state.activeTape.push(tapeNode);\n  }\n\n  keep(result) {\n    result.kept = true;\n    return result;\n  }\n\n  startTape() {\n    if (this.state.gradientDepth === 0) {\n      this.state.activeTape = [];\n    }\n\n    this.state.gradientDepth++;\n  }\n\n  endTape() {\n    this.state.gradientDepth--;\n  }\n  /**\n   * Start a scope. Use this with endScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n\n\n  startScope(name) {\n    const scopeInfo = {\n      track: [],\n      name: 'unnamed scope',\n      id: this.state.nextScopeId++\n    };\n\n    if (name) {\n      scopeInfo.name = name;\n    }\n\n    this.state.scopeStack.push(scopeInfo);\n    this.state.activeScope = scopeInfo;\n  }\n  /**\n   * End a scope. Use this with startScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n\n\n  endScope(result) {\n    const tensorsToTrackInParent = getTensorsInContainer(result);\n    const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(t => t.id)); // Dispose the arrays tracked in this scope.\n\n    for (let i = 0; i < this.state.activeScope.track.length; i++) {\n      const tensor = this.state.activeScope.track[i];\n\n      if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n        tensor.dispose();\n      }\n    }\n\n    const oldScope = this.state.scopeStack.pop();\n    this.state.activeScope = this.state.scopeStack.length === 0 ? null : this.state.scopeStack[this.state.scopeStack.length - 1]; // Track the current result in the parent scope.\n\n    tensorsToTrackInParent.forEach(tensor => {\n      // Only track the tensor if was allocated in the inner scope and is not\n      // globally kept.\n      if (!tensor.kept && tensor.scopeId === oldScope.id) {\n        this.track(tensor);\n      }\n    });\n  }\n  /**\n   * Returns gradients of `f` with respect to each of the `xs`. The gradients\n   * returned are of the same length as `xs`, but some might be null if `f`\n   * was not a function of that `x`. It also takes optional dy to multiply the\n   * gradient, which defaults to `1`.\n   */\n\n\n  gradients(f, xs, dy, allowNoGradients = false) {\n    util.assert(xs.length > 0, () => 'gradients() received an empty list of xs.');\n\n    if (dy != null && dy.dtype !== 'float32') {\n      throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n    }\n\n    const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy('forward', f));\n    util.assert(y instanceof Tensor, () => 'The result y returned by f() must be a tensor.'); // Filter out the nodes that don't connect x => y.\n\n    const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n\n    if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n      throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' + 'that the f you passed encloses all operations that lead from x ' + 'to y.');\n    }\n\n    return this.tidy('backward', () => {\n      const accumulatedGradientMap = {};\n      accumulatedGradientMap[y.id] = dy == null ? ones(y.shape) : dy; // Backprop gradients through the filtered nodes.\n\n      backpropagateGradients(accumulatedGradientMap, filteredTape, // Pass the tidy function to avoid circular dep with `tape.ts`.\n      f => this.tidy(f), // Pass an add function to avoide a circular dep with `tape.ts`.\n      add);\n      const grads = xs.map(x => accumulatedGradientMap[x.id]);\n\n      if (this.state.gradientDepth === 0) {\n        // This means that we are not computing higher-order gradients\n        // and can clean up the tape.\n        this.state.activeTape.forEach(node => {\n          for (const tensor of node.saved) {\n            tensor.dispose();\n          }\n        });\n        this.state.activeTape = null;\n      }\n\n      return {\n        value: y,\n        grads\n      };\n    });\n  }\n\n  customGrad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in customGrad(f) must be a function.');\n    return (...inputs) => {\n      util.assert(inputs.every(t => t instanceof Tensor), () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' + 'tensors');\n      let res;\n      const inputMap = {};\n      inputs.forEach((input, i) => {\n        inputMap[i] = input;\n      });\n\n      const forwardFunc = (_, save) => {\n        res = f(...[...inputs, save]);\n        util.assert(res.value instanceof Tensor, () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.value` is a tensor');\n        util.assert(util.isFunction(res.gradFunc), () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function.');\n        return res.value;\n      };\n\n      const backwardsFunc = (dy, saved) => {\n        const gradRes = res.gradFunc(dy, saved);\n        const grads = Array.isArray(gradRes) ? gradRes : [gradRes];\n        util.assert(grads.length === inputs.length, () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'the same number of tensors as inputs passed to f(...).');\n        util.assert(grads.every(t => t instanceof Tensor), () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'a list of only tensors.');\n        const gradMap = {};\n        grads.forEach((grad, i) => {\n          gradMap[i] = () => grad;\n        });\n        return gradMap;\n      };\n\n      return this.runKernelFunc({\n        forwardFunc,\n        backwardsFunc,\n        inputs: inputMap\n      });\n    };\n  }\n\n  readSync(dataId) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readSync(dataId);\n  }\n\n  read(dataId) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.read(dataId);\n  }\n\n  time(query) {\n    var _this4 = this;\n\n    return _asyncToGenerator(function* () {\n      const start = now();\n      const timingInfo = yield _this4.backend.time(query);\n      timingInfo.wallMs = now() - start;\n      return timingInfo;\n    })();\n  }\n  /**\n   * Tracks a Tensor in the current scope to be automatically cleaned up\n   * when the current scope ends, and returns the value.\n   *\n   * @param result The Tensor to track in the current scope.\n   */\n\n\n  track(result) {\n    if (this.state.activeScope != null) {\n      result.scopeId = this.state.activeScope.id;\n      this.state.activeScope.track.push(result);\n    }\n\n    return result;\n  }\n\n  get registeredVariables() {\n    return this.state.registeredVariables;\n  }\n  /**\n   * Resets the engine state. Removes all backends but does not remove\n   * registered backend factories.\n   */\n\n\n  reset() {\n    // Make any pending promise obsolete.\n    this.pendingBackendInitId++;\n    this.state.dispose();\n    this.ENV.reset();\n    this.state = new EngineState();\n\n    for (const backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    this.backendName = null;\n    this.backendInstance = null;\n    this.pendingBackendInit = null;\n  }\n\n}\nEngine.nextTensorId = 0;\nEngine.nextVariableId = 0;\n\nfunction ones(shape) {\n  const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\n\nexport function getOrMakeEngine() {\n  const ns = getGlobalNamespace();\n\n  if (ns._tfengine == null) {\n    const environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n\n  setEnvironmentGlobal(ns._tfengine.ENV); // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n\n  setTensorTracker(() => ns._tfengine);\n  return ns._tfengine;\n}\nexport const ENGINE = getOrMakeEngine();\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\n\nexport function add(a, b) {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  const inputs = {\n    a,\n    b\n  };\n  return ENGINE.runKernel(Add, inputs);\n}","map":{"version":3,"sources":["/Users/ryanliang/Downloads/main_movir_picker/node_modules/@tensorflow/tfjs-core/dist/engine.js"],"names":["KernelBackend","Environment","setEnvironmentGlobal","getGlobalNamespace","Add","Cast","Identity","getGradient","getKernel","getKernelsForBackend","Profiler","backpropagateGradients","getFilteredNodesXToY","setTensorTracker","Tensor","Variable","getTensorsInContainer","util","bytesFromStringArray","makeOnesTypedArray","now","sizeFromShape","isRegisteredKernelInvocation","kernelInvocation","kernelName","EngineState","constructor","registeredVariables","nextTapeNodeId","numBytes","numTensors","numStringTensors","numDataBuffers","gradientDepth","kernelDepth","scopeStack","numDataMovesStack","nextScopeId","tensorInfo","WeakMap","profiling","activeProfile","newBytes","newTensors","peakBytes","kernels","result","kernelNames","Array","from","Set","map","k","name","dispose","variableName","Engine","ENV","registry","registryFactory","pendingBackendInitId","state","ready","pendingBackendInit","then","backendInstance","sortedBackends","getSortedBackends","i","length","backendName","success","initializeBackend","setBackend","Error","backend","asyncInit","initializeBackendsAndReturnBest","backendNames","Object","keys","findBackend","findBackendFactory","factory","registerBackend","priority","console","warn","setupRegisteredKernels","profiler","forEach","kernel","setupFunc","disposeRegisteredKernels","disposeFunc","registryFactoryEntry","promiseId","catch","err","stack","message","removeBackend","sort","a","b","moveData","dataId","info","get","srcBackend","values","readSync","refCount","disposeData","move","shape","dtype","shouldCheckForMemLeaks","tidy","nameOrFn","fn","String","scopedRun","startScope","endScope","Promise","error","start","end","f","res","ex","nextTensorId","nextVariableId","clone","x","y","ENGINE","runKernel","inputs","grad","dy","gradInputs","attrs","saved","addTapeNode","activeScope","hasKernel","runKernelFunc","getBool","checkKernelForMemLeak","numDataIdsBefore","outInfos","numDataIdsAfter","numDataIds","numOutputDataIds","numMoves","dataIdsLeaked","kernelParams","outputs","isTapeOn","startingBytecount","startingNumTensors","push","kernelFunc","out","kernelOrScopeName","assert","isArray","outTensors","outInfo","rank","makeTensorFromDataId","tensorsToSave","getTensorsForGradient","saveTensorsForBackwardMode","forwardFunc","saveFunc","tensors","tensor","keep","outs","backwardsFunc","kernelProfile","profileKernel","logKernelProfile","bytesAdded","totalBytesSnapshot","tensorsAdded","totalTensorsSnapshot","inputShapes","key","outputShapes","item","kernelTimeMs","timeMs","extraInfo","gradConfig","inputsToSave","outputsToSave","inputTensorsToSave","saveAllInputs","inputName","outputTensorsToSave","filter","_","concat","makeTensor","backendVals","isString","d","encodeString","write","t","trackTensor","bytes","makeVariable","initialValue","trainable","toString","cast","v","incRef","size","bytesPerElement","has","set","track","removeDataId","delete","disposeTensor","disposeVariables","varName","disposeVariable","memory","unreliable","reasons","profile","query","startBytes","startNumTensors","Math","max","gradientsFunc","tapeNode","id","gradFunc","gradient","dys","output","vals","makeZerosTypedArray","activeTape","kept","startTape","endTape","scopeInfo","tensorsToTrackInParent","tensorsToTrackInParentSet","oldScope","pop","scopeId","gradients","xs","allowNoGradients","filteredTape","accumulatedGradientMap","ones","add","grads","node","value","customGrad","isFunction","every","inputMap","input","save","gradRes","gradMap","read","time","timingInfo","wallMs","reset","getOrMakeEngine","ns","_tfengine","environment"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,aAAT,QAA8B,oBAA9B;AACA,SAASC,WAAT,EAAsBC,oBAAtB,QAAkD,eAAlD;AACA,SAASC,kBAAT,QAAmC,eAAnC;AACA,SAASC,GAAT,EAAcC,IAAd,EAAoBC,QAApB,QAAoC,gBAApC;AACA,SAASC,WAAT,EAAsBC,SAAtB,EAAiCC,oBAAjC,QAA6D,mBAA7D;AACA,SAASC,QAAT,QAAyB,YAAzB;AACA,SAASC,sBAAT,EAAiCC,oBAAjC,QAA6D,QAA7D;AACA,SAASC,gBAAT,EAA2BC,MAA3B,EAAmCC,QAAnC,QAAmD,UAAnD;AACA,SAASC,qBAAT,QAAsC,eAAtC;AACA,OAAO,KAAKC,IAAZ,MAAsB,QAAtB;AACA,SAASC,oBAAT,EAA+BC,kBAA/B,EAAmDC,GAAnD,EAAwDC,aAAxD,QAA6E,QAA7E;;AACA,SAASC,4BAAT,CAAsCC,gBAAtC,EAAwD;AACpD,SAAOA,gBAAgB,CAACC,UAAjB,IAA+B,IAAtC;AACH;;AACD,MAAMC,WAAN,CAAkB;AACdC,EAAAA,WAAW,GAAG;AACV;AACA,SAAKC,mBAAL,GAA2B,EAA3B;AACA,SAAKC,cAAL,GAAsB,CAAtB;AACA,SAAKC,QAAL,GAAgB,CAAhB;AACA,SAAKC,UAAL,GAAkB,CAAlB;AACA,SAAKC,gBAAL,GAAwB,CAAxB;AACA,SAAKC,cAAL,GAAsB,CAAtB,CAPU,CAQV;AACA;AACA;;AACA,SAAKC,aAAL,GAAqB,CAArB,CAXU,CAYV;AACA;;AACA,SAAKC,WAAL,GAAmB,CAAnB;AACA,SAAKC,UAAL,GAAkB,EAAlB;AACA;AACR;AACA;AACA;;AACQ,SAAKC,iBAAL,GAAyB,EAAzB;AACA,SAAKC,WAAL,GAAmB,CAAnB;AACA,SAAKC,UAAL,GAAkB,IAAIC,OAAJ,EAAlB;AACA,SAAKC,SAAL,GAAiB,KAAjB;AACA,SAAKC,aAAL,GAAqB;AACjBC,MAAAA,QAAQ,EAAE,CADO;AAEjBC,MAAAA,UAAU,EAAE,CAFK;AAGjBC,MAAAA,SAAS,EAAE,CAHM;AAIjBC,MAAAA,OAAO,EAAE,EAJQ;AAKjBC,MAAAA,MAAM,EAAE,IALS;;AAMjB,UAAIC,WAAJ,GAAkB;AACd,eAAOC,KAAK,CAACC,IAAN,CAAW,IAAIC,GAAJ,CAAQ,KAAKL,OAAL,CAAaM,GAAb,CAAiBC,CAAC,IAAIA,CAAC,CAACC,IAAxB,CAAR,CAAX,CAAP;AACH;;AARgB,KAArB;AAUH;;AACDC,EAAAA,OAAO,GAAG;AACN,SAAK,MAAMC,YAAX,IAA2B,KAAK5B,mBAAhC,EAAqD;AACjD,WAAKA,mBAAL,CAAyB4B,YAAzB,EAAuCD,OAAvC;AACH;AACJ;;AAxCa;;AA0ClB,OAAO,MAAME,MAAN,CAAa;AAChB9B,EAAAA,WAAW,CAAC+B,GAAD,EAAM;AACb,SAAKA,GAAL,GAAWA,GAAX;AACA,SAAKC,QAAL,GAAgB,EAAhB;AACA,SAAKC,eAAL,GAAuB,EAAvB;AACA,SAAKC,oBAAL,GAA4B,CAA5B;AACA,SAAKC,KAAL,GAAa,IAAIpC,WAAJ,EAAb;AACH;;AACKqC,EAAAA,KAAK,GAAG;AAAA;;AAAA;AACV,UAAI,KAAI,CAACC,kBAAL,IAA2B,IAA/B,EAAqC;AACjC,eAAO,KAAI,CAACA,kBAAL,CAAwBC,IAAxB,CAA6B,MAAM,CAAG,CAAtC,CAAP;AACH;;AACD,UAAI,KAAI,CAACC,eAAL,IAAwB,IAA5B,EAAkC;AAC9B;AACH;;AACD,YAAMC,cAAc,GAAG,KAAI,CAACC,iBAAL,EAAvB;;AACA,WAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGF,cAAc,CAACG,MAAnC,EAA2CD,CAAC,EAA5C,EAAgD;AAC5C,cAAME,WAAW,GAAGJ,cAAc,CAACE,CAAD,CAAlC;AACA,cAAMG,OAAO,SAAS,KAAI,CAACC,iBAAL,CAAuBF,WAAvB,EAAoCC,OAA1D;;AACA,YAAIA,OAAJ,EAAa;AACT,gBAAM,KAAI,CAACE,UAAL,CAAgBH,WAAhB,CAAN;AACA;AACH;AACJ;;AACD,YAAM,IAAII,KAAJ,CAAW,iEAAD,GACX,SADC,CAAN;AAhBU;AAkBb;;AACU,MAAPC,OAAO,GAAG;AACV,QAAI,KAAKZ,kBAAL,IAA2B,IAA/B,EAAqC;AACjC,YAAM,IAAIW,KAAJ,CAAW,YAAW,KAAKJ,WAAY,uCAA7B,GACX,mEADW,GAEX,eAFC,CAAN;AAGH;;AACD,QAAI,KAAKL,eAAL,IAAwB,IAA5B,EAAkC;AAC9B,YAAM;AAAEZ,QAAAA,IAAF;AAAQuB,QAAAA;AAAR,UAAsB,KAAKC,+BAAL,EAA5B;;AACA,UAAID,SAAJ,EAAe;AACX,cAAM,IAAIF,KAAJ,CAAW,iCAAgCrB,IAAK,qBAAtC,GACX,gDADW,GAEX,oDAFC,CAAN;AAGH;;AACD,WAAKoB,UAAL,CAAgBpB,IAAhB;AACH;;AACD,WAAO,KAAKY,eAAZ;AACH;;AACDa,EAAAA,YAAY,GAAG;AACX,WAAOC,MAAM,CAACC,IAAP,CAAY,KAAKrB,eAAjB,CAAP;AACH;;AACDsB,EAAAA,WAAW,CAACX,WAAD,EAAc;AACrB,QAAI,EAAEA,WAAW,IAAI,KAAKZ,QAAtB,CAAJ,EAAqC;AACjC;AACA;AACA,UAAIY,WAAW,IAAI,KAAKX,eAAxB,EAAyC;AACrC,cAAM;AAAEiB,UAAAA;AAAF,YAAgB,KAAKJ,iBAAL,CAAuBF,WAAvB,CAAtB;;AACA,YAAIM,SAAJ,EAAe;AACX;AACA,iBAAO,IAAP;AACH;AACJ,OAND,MAOK;AACD,eAAO,IAAP;AACH;AACJ;;AACD,WAAO,KAAKlB,QAAL,CAAcY,WAAd,CAAP;AACH;;AACDY,EAAAA,kBAAkB,CAACZ,WAAD,EAAc;AAC5B,QAAI,EAAEA,WAAW,IAAI,KAAKX,eAAtB,CAAJ,EAA4C;AACxC,aAAO,IAAP;AACH;;AACD,WAAO,KAAKA,eAAL,CAAqBW,WAArB,EAAkCa,OAAzC;AACH;;AACDC,EAAAA,eAAe,CAACd,WAAD,EAAca,OAAd,EAAuBE,QAAQ,GAAG,CAAlC,EAAqC;AAChD,QAAIf,WAAW,IAAI,KAAKX,eAAxB,EAAyC;AACrC2B,MAAAA,OAAO,CAACC,IAAR,CAAc,GAAEjB,WAAY,mCAAf,GACR,mCADL;AAEA,aAAO,KAAP;AACH;;AACD,SAAKX,eAAL,CAAqBW,WAArB,IAAoC;AAAEa,MAAAA,OAAF;AAAWE,MAAAA;AAAX,KAApC;AACA,WAAO,IAAP;AACH;;AACKZ,EAAAA,UAAU,CAACH,WAAD,EAAc;AAAA;;AAAA;AAC1B,UAAI,MAAI,CAACX,eAAL,CAAqBW,WAArB,KAAqC,IAAzC,EAA+C;AAC3C,cAAM,IAAII,KAAJ,CAAW,iBAAgBJ,WAAY,yBAAvC,CAAN;AACH;;AACD,MAAA,MAAI,CAACA,WAAL,GAAmBA,WAAnB;;AACA,UAAI,MAAI,CAACZ,QAAL,CAAcY,WAAd,KAA8B,IAAlC,EAAwC;AACpC,QAAA,MAAI,CAACL,eAAL,GAAuB,IAAvB;;AACA,cAAM;AAAEM,UAAAA,OAAF;AAAWK,UAAAA;AAAX,YAAyB,MAAI,CAACJ,iBAAL,CAAuBF,WAAvB,CAA/B;;AACA,cAAMxB,MAAM,GAAG8B,SAAS,SAASL,OAAT,GAAmBA,OAA3C;;AACA,YAAI,CAACzB,MAAL,EAAa;AACT,iBAAO,KAAP;AACH;AACJ;;AACD,MAAA,MAAI,CAACmB,eAAL,GAAuB,MAAI,CAACP,QAAL,CAAcY,WAAd,CAAvB;;AACA,MAAA,MAAI,CAACkB,sBAAL,GAd0B,CAe1B;;;AACA,MAAA,MAAI,CAACC,QAAL,GAAgB,IAAI/E,QAAJ,CAAa,MAAI,CAACuD,eAAlB,CAAhB;AACA,aAAO,IAAP;AAjB0B;AAkB7B;;AACDuB,EAAAA,sBAAsB,GAAG;AACrB,UAAM3C,OAAO,GAAGpC,oBAAoB,CAAC,KAAK6D,WAAN,CAApC;AACAzB,IAAAA,OAAO,CAAC6C,OAAR,CAAgBC,MAAM,IAAI;AACtB,UAAIA,MAAM,CAACC,SAAP,IAAoB,IAAxB,EAA8B;AAC1BD,QAAAA,MAAM,CAACC,SAAP,CAAiB,KAAK3B,eAAtB;AACH;AACJ,KAJD;AAKH;;AACD4B,EAAAA,wBAAwB,CAACvB,WAAD,EAAc;AAClC,UAAMzB,OAAO,GAAGpC,oBAAoB,CAAC6D,WAAD,CAApC;AACAzB,IAAAA,OAAO,CAAC6C,OAAR,CAAgBC,MAAM,IAAI;AACtB,UAAIA,MAAM,CAACG,WAAP,IAAsB,IAA1B,EAAgC;AAC5BH,QAAAA,MAAM,CAACG,WAAP,CAAmB,KAAKpC,QAAL,CAAcY,WAAd,CAAnB;AACH;AACJ,KAJD;AAKH;AACD;AACJ;AACA;AACA;AACA;AACA;;;AACIE,EAAAA,iBAAiB,CAACF,WAAD,EAAc;AAC3B,UAAMyB,oBAAoB,GAAG,KAAKpC,eAAL,CAAqBW,WAArB,CAA7B;;AACA,QAAIyB,oBAAoB,IAAI,IAA5B,EAAkC;AAC9B,YAAM,IAAIrB,KAAJ,CAAW,6BAA4BJ,WAAY,0BAAnD,CAAN;AACH;;AACD,QAAI;AACA,YAAMK,OAAO,GAAGoB,oBAAoB,CAACZ,OAArB,EAAhB;AACA;AACZ;AACA;AACA;AACA;;AACY,UAAIR,OAAO,IAAI,EAAEA,OAAO,YAAY3E,aAArB,CAAX,IACA,OAAO2E,OAAO,CAACX,IAAf,KAAwB,UAD5B,EACwC;AACpC,cAAMgC,SAAS,GAAG,EAAE,KAAKpC,oBAAzB;AACA,cAAMW,OAAO,GAAGI,OAAO,CAClBX,IADW,CACNC,eAAe,IAAI;AACzB;AACA,cAAI+B,SAAS,GAAG,KAAKpC,oBAArB,EAA2C;AACvC,mBAAO,KAAP;AACH;;AACD,eAAKF,QAAL,CAAcY,WAAd,IAA6BL,eAA7B;AACA,eAAKF,kBAAL,GAA0B,IAA1B;AACA,iBAAO,IAAP;AACH,SATe,EAUXkC,KAVW,CAULC,GAAG,IAAI;AACd;AACA,cAAIF,SAAS,GAAG,KAAKpC,oBAArB,EAA2C;AACvC,mBAAO,KAAP;AACH;;AACD,eAAKG,kBAAL,GAA0B,IAA1B;AACAuB,UAAAA,OAAO,CAACC,IAAR,CAAc,6BAA4BjB,WAAY,SAAtD;AACAgB,UAAAA,OAAO,CAACC,IAAR,CAAaW,GAAG,CAACC,KAAJ,IAAaD,GAAG,CAACE,OAA9B;AACA,iBAAO,KAAP;AACH,SAnBe,CAAhB;AAoBA,aAAKrC,kBAAL,GAA0BQ,OAA1B;AACA,eAAO;AAAEA,UAAAA,OAAF;AAAWK,UAAAA,SAAS,EAAE;AAAtB,SAAP;AACH,OAzBD,MA0BK;AACD,aAAKlB,QAAL,CAAcY,WAAd,IAA6BK,OAA7B;AACA,eAAO;AAAEJ,UAAAA,OAAO,EAAE,IAAX;AAAiBK,UAAAA,SAAS,EAAE;AAA5B,SAAP;AACH;AACJ,KArCD,CAsCA,OAAOsB,GAAP,EAAY;AACRZ,MAAAA,OAAO,CAACC,IAAR,CAAc,6BAA4BjB,WAAY,SAAtD;AACAgB,MAAAA,OAAO,CAACC,IAAR,CAAaW,GAAG,CAACC,KAAJ,IAAaD,GAAG,CAACE,OAA9B;AACA,aAAO;AAAE7B,QAAAA,OAAO,EAAE,KAAX;AAAkBK,QAAAA,SAAS,EAAE;AAA7B,OAAP;AACH;AACJ;;AACDyB,EAAAA,aAAa,CAAC/B,WAAD,EAAc;AACvB,QAAI,EAAEA,WAAW,IAAI,KAAKX,eAAtB,CAAJ,EAA4C;AACxC,YAAM,IAAIe,KAAJ,CAAW,GAAEJ,WAAY,gCAAzB,CAAN;AACH;;AACD,QAAI,KAAKA,WAAL,KAAqBA,WAArB,IAAoC,KAAKP,kBAAL,IAA2B,IAAnE,EAAyE;AACrE;AACA;AACA,WAAKH,oBAAL;AACH;;AACD,QAAIU,WAAW,IAAI,KAAKZ,QAAxB,EAAkC;AAC9B,WAAKmC,wBAAL,CAA8BvB,WAA9B;AACA,WAAKZ,QAAL,CAAcY,WAAd,EAA2BhB,OAA3B;AACA,aAAO,KAAKI,QAAL,CAAcY,WAAd,CAAP;AACH;;AACD,WAAO,KAAKX,eAAL,CAAqBW,WAArB,CAAP,CAduB,CAevB;;AACA,QAAI,KAAKA,WAAL,KAAqBA,WAAzB,EAAsC;AAClC,WAAKP,kBAAL,GAA0B,IAA1B;AACA,WAAKO,WAAL,GAAmB,IAAnB;AACA,WAAKL,eAAL,GAAuB,IAAvB;AACH;AACJ;;AACDE,EAAAA,iBAAiB,GAAG;AAChB,QAAIY,MAAM,CAACC,IAAP,CAAY,KAAKrB,eAAjB,EAAkCU,MAAlC,KAA6C,CAAjD,EAAoD;AAChD,YAAM,IAAIK,KAAJ,CAAU,+BAAV,CAAN;AACH;;AACD,WAAOK,MAAM,CAACC,IAAP,CAAY,KAAKrB,eAAjB,EAAkC2C,IAAlC,CAAuC,CAACC,CAAD,EAAIC,CAAJ,KAAU;AACpD;AACA,aAAO,KAAK7C,eAAL,CAAqB6C,CAArB,EAAwBnB,QAAxB,GACH,KAAK1B,eAAL,CAAqB4C,CAArB,EAAwBlB,QAD5B;AAEH,KAJM,CAAP;AAKH;;AACDR,EAAAA,+BAA+B,GAAG;AAC9B,UAAMX,cAAc,GAAG,KAAKC,iBAAL,EAAvB;;AACA,SAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGF,cAAc,CAACG,MAAnC,EAA2CD,CAAC,EAA5C,EAAgD;AAC5C,YAAME,WAAW,GAAGJ,cAAc,CAACE,CAAD,CAAlC;AACA,YAAM;AAAEG,QAAAA,OAAF;AAAWK,QAAAA;AAAX,UAAyB,KAAKJ,iBAAL,CAAuBF,WAAvB,CAA/B;;AACA,UAAIM,SAAS,IAAIL,OAAjB,EAA0B;AACtB,eAAO;AAAElB,UAAAA,IAAI,EAAEiB,WAAR;AAAqBM,UAAAA;AAArB,SAAP;AACH;AACJ;;AACD,UAAM,IAAIF,KAAJ,CAAW,iEAAD,GACX,SADC,CAAN;AAEH;;AACD+B,EAAAA,QAAQ,CAAC9B,OAAD,EAAU+B,MAAV,EAAkB;AACtB,UAAMC,IAAI,GAAG,KAAK9C,KAAL,CAAWvB,UAAX,CAAsBsE,GAAtB,CAA0BF,MAA1B,CAAb;AACA,UAAMG,UAAU,GAAGF,IAAI,CAAChC,OAAxB;AACA,UAAMmC,MAAM,GAAG,KAAKC,QAAL,CAAcL,MAAd,CAAf;AACA,UAAMM,QAAQ,GAAGH,UAAU,CAACG,QAAX,CAAoBN,MAApB,CAAjB,CAJsB,CAKtB;AACA;;AACAG,IAAAA,UAAU,CAACI,WAAX,CAAuBP,MAAvB,EAA+B,IAA/B;AACAC,IAAAA,IAAI,CAAChC,OAAL,GAAeA,OAAf;AACAA,IAAAA,OAAO,CAACuC,IAAR,CAAaR,MAAb,EAAqBI,MAArB,EAA6BH,IAAI,CAACQ,KAAlC,EAAyCR,IAAI,CAACS,KAA9C,EAAqDJ,QAArD;;AACA,QAAI,KAAKK,sBAAL,EAAJ,EAAmC;AAC/B;AACA;AACA,WAAKxD,KAAL,CAAWzB,iBAAX,CAA6B,KAAKyB,KAAL,CAAWzB,iBAAX,CAA6BiC,MAA7B,GAAsC,CAAnE;AACH;AACJ;;AACDiD,EAAAA,IAAI,CAACC,QAAD,EAAWC,EAAX,EAAe;AACf,QAAInE,IAAI,GAAG,IAAX;;AACA,QAAImE,EAAE,IAAI,IAAV,EAAgB;AACZ;AACA,UAAI,OAAOD,QAAP,KAAoB,UAAxB,EAAoC;AAChC,cAAM,IAAI7C,KAAJ,CAAU,qCAAV,CAAN;AACH;;AACD8C,MAAAA,EAAE,GAAGD,QAAL;AACH,KAND,MAOK;AACD;AACA,UAAI,OAAOA,QAAP,KAAoB,QAApB,IAAgC,EAAEA,QAAQ,YAAYE,MAAtB,CAApC,EAAmE;AAC/D,cAAM,IAAI/C,KAAJ,CAAU,yDACZ,4BADE,CAAN;AAEH;;AACD,UAAI,OAAO8C,EAAP,KAAc,UAAlB,EAA8B;AAC1B,cAAM,IAAI9C,KAAJ,CAAU,uDACZ,8BADE,CAAN;AAEH;;AACDrB,MAAAA,IAAI,GAAGkE,QAAP,CAVC,CAWD;AACA;AACH;;AACD,QAAIzE,MAAJ;AACA,WAAO,KAAK4E,SAAL,CAAe,MAAM,KAAKC,UAAL,CAAgBtE,IAAhB,CAArB,EAA4C,MAAM,KAAKuE,QAAL,CAAc9E,MAAd,CAAlD,EAAyE,MAAM;AAClFA,MAAAA,MAAM,GAAG0E,EAAE,EAAX;;AACA,UAAI1E,MAAM,YAAY+E,OAAtB,EAA+B;AAC3BvC,QAAAA,OAAO,CAACwC,KAAR,CAAc,yCAAd;AACH;;AACD,aAAOhF,MAAP;AACH,KANM,CAAP;AAOH;;AACD4E,EAAAA,SAAS,CAACK,KAAD,EAAQC,GAAR,EAAaC,CAAb,EAAgB;AACrBF,IAAAA,KAAK;;AACL,QAAI;AACA,YAAMG,GAAG,GAAGD,CAAC,EAAb;AACAD,MAAAA,GAAG;AACH,aAAOE,GAAP;AACH,KAJD,CAKA,OAAOC,EAAP,EAAW;AACPH,MAAAA,GAAG;AACH,YAAMG,EAAN;AACH;AACJ;;AACDC,EAAAA,YAAY,GAAG;AACX,WAAO5E,MAAM,CAAC4E,YAAP,EAAP;AACH;;AACDC,EAAAA,cAAc,GAAG;AACb,WAAO7E,MAAM,CAAC6E,cAAP,EAAP;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;;;AACIC,EAAAA,KAAK,CAACC,CAAD,EAAI;AACL,UAAMC,CAAC,GAAGC,MAAM,CAACC,SAAP,CAAiBpI,QAAjB,EAA2B;AAAEiI,MAAAA;AAAF,KAA3B,CAAV;AACA,UAAMI,MAAM,GAAG;AAAEJ,MAAAA;AAAF,KAAf;;AACA,UAAMK,IAAI,GAAIC,EAAD,KAAS;AAClBN,MAAAA,CAAC,EAAE,MAAM;AACL,cAAMnB,KAAK,GAAG,SAAd;AACA,cAAM0B,UAAU,GAAG;AAAEP,UAAAA,CAAC,EAAEM;AAAL,SAAnB;AACA,cAAME,KAAK,GAAG;AAAE3B,UAAAA;AAAF,SAAd;AACA,eAAOqB,MAAM,CAACC,SAAP,CAAiBrI,IAAjB,EAAuByI,UAAvB,EACP;AACAC,QAAAA,KAFO,CAAP;AAGH;AARiB,KAAT,CAAb;;AAUA,UAAMC,KAAK,GAAG,EAAd;AACA,SAAKC,WAAL,CAAiB,KAAKpF,KAAL,CAAWqF,WAAX,CAAuB7F,IAAxC,EAA8CsF,MAA9C,EAAsD,CAACH,CAAD,CAAtD,EAA2DI,IAA3D,EAAiEI,KAAjE,EAAwE,EAAxE;AACA,WAAOR,CAAP;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIE,EAAAA,SAAS,CAAClH,UAAD,EAAamH,MAAb,EAAqBI,KAArB,EAA4B;AACjC,UAAMI,SAAS,GAAG3I,SAAS,CAACgB,UAAD,EAAa,KAAK8C,WAAlB,CAAT,IAA2C,IAA7D;;AACA,QAAI,CAAC6E,SAAL,EAAgB;AACZ,YAAM,IAAIzE,KAAJ,CAAW,WAAUlD,UAAW,iCAAgC,KAAK8C,WAAY,GAAjF,CAAN;AACH;;AACD,WAAO,KAAK8E,aAAL,CAAmB;AAAE5H,MAAAA,UAAF;AAAcmH,MAAAA,MAAd;AAAsBI,MAAAA;AAAtB,KAAnB,CAAP;AACH;;AACD1B,EAAAA,sBAAsB,GAAG;AACrB,WAAO,KAAK5D,GAAL,CAAS4F,OAAT,CAAiB,SAAjB,CAAP;AACH;;AACDC,EAAAA,qBAAqB,CAAC9H,UAAD,EAAa+H,gBAAb,EAA+BC,QAA/B,EAAyC;AAC1D,UAAMC,eAAe,GAAG,KAAK9E,OAAL,CAAa+E,UAAb,EAAxB,CAD0D,CAE1D;;AACA,QAAIC,gBAAgB,GAAG,CAAvB;AACAH,IAAAA,QAAQ,CAAC9D,OAAT,CAAiBiB,IAAI,IAAI;AACrB;AACA;AACAgD,MAAAA,gBAAgB,IAAKhD,IAAI,CAACS,KAAL,KAAe,WAAf,GAA6B,CAA7B,GAAiC,CAAtD;AACH,KAJD,EAJ0D,CAS1D;AACA;AACA;AACA;AACA;;AACA,UAAMwC,QAAQ,GAAG,KAAK/F,KAAL,CAAWzB,iBAAX,CAA6B,KAAKyB,KAAL,CAAWzB,iBAAX,CAA6BiC,MAA7B,GAAsC,CAAnE,CAAjB;AACA,UAAMwF,aAAa,GAAGJ,eAAe,GAAGF,gBAAlB,GAAqCI,gBAArC,GAAwDC,QAA9E;;AACA,QAAIC,aAAa,GAAG,CAApB,EAAuB;AACnB,YAAM,IAAInF,KAAJ,CAAW,YAAW,KAAKJ,WAAY,gCAA7B,GACX,IAAGuF,aAAc,6BAA4BrI,UAAW,GADvD,CAAN;AAEH;AACJ;AACD;AACJ;AACA;AACA;AACA;;;AACI4H,EAAAA,aAAa,CAACU,YAAD,EAAe;AACxB,QAAIC,OAAJ;AACA,QAAIf,KAAK,GAAG,EAAZ;AACA,UAAMgB,QAAQ,GAAG,KAAKA,QAAL,EAAjB;AACA,UAAMC,iBAAiB,GAAG,KAAKpG,KAAL,CAAWhC,QAArC;AACA,UAAMqI,kBAAkB,GAAG,KAAKrG,KAAL,CAAW/B,UAAtC;;AACA,QAAI,KAAKuF,sBAAL,EAAJ,EAAmC;AAC/B,WAAKxD,KAAL,CAAWzB,iBAAX,CAA6B+H,IAA7B,CAAkC,CAAlC;AACH;;AACD,QAAIC,UAAJ;;AACA,QAAI,KAAK9F,WAAL,IAAoB,IAAxB,EAA8B;AAC1B;AACA;AACA;AACA;AACA;AACA,WAAKK,OAAL;AACH;;AACD,QAAI0F,GAAJ;AACA,UAAMC,iBAAiB,GAAGhJ,4BAA4B,CAACwI,YAAD,CAA5B,GACtBA,YAAY,CAACtI,UADS,GAEtB,KAAKqC,KAAL,CAAWqF,WAAX,IAA0B,IAA1B,GAAiC,KAAKrF,KAAL,CAAWqF,WAAX,CAAuB7F,IAAxD,GAA+D,EAFnE,CAnBwB,CAsBxB;AACA;AACA;;AACA,QAAI/B,4BAA4B,CAACwI,YAAD,CAAhC,EAAgD;AAC5C,YAAM;AAAEtI,QAAAA,UAAF;AAAcmH,QAAAA,MAAd;AAAsBI,QAAAA;AAAtB,UAAgCe,YAAtC;;AACA,UAAI,KAAKxF,WAAL,IAAoB,IAAxB,EAA8B;AAC1B;AACA;AACA;AACA;AACA;AACA,aAAKK,OAAL;AACH;;AACD,YAAMgB,MAAM,GAAGnF,SAAS,CAACgB,UAAD,EAAa,KAAK8C,WAAlB,CAAxB;AACArD,MAAAA,IAAI,CAACsJ,MAAL,CAAY5E,MAAM,IAAI,IAAtB,EAA4B,MAAO,kCAAiCnE,UAAW,kBAAiB,KAAK8C,WAAY,GAAjH;;AACA8F,MAAAA,UAAU,GAAG,MAAM;AACf,cAAMb,gBAAgB,GAAG,KAAK5E,OAAL,CAAa+E,UAAb,EAAzB;AACAW,QAAAA,GAAG,GAAG1E,MAAM,CAACyE,UAAP,CAAkB;AAAEzB,UAAAA,MAAF;AAAUI,UAAAA,KAAV;AAAiBpE,UAAAA,OAAO,EAAE,KAAKA;AAA/B,SAAlB,CAAN;AACA,cAAM6E,QAAQ,GAAGxG,KAAK,CAACwH,OAAN,CAAcH,GAAd,IAAqBA,GAArB,GAA2B,CAACA,GAAD,CAA5C;;AACA,YAAI,KAAKhD,sBAAL,EAAJ,EAAmC;AAC/B,eAAKiC,qBAAL,CAA2B9H,UAA3B,EAAuC+H,gBAAvC,EAAyDC,QAAzD;AACH;;AACD,cAAMiB,UAAU,GAAGjB,QAAQ,CAACrG,GAAT,CAAcuH,OAAD,IAAa;AACzC;AACA;AACA;AACA,cAAIA,OAAO,CAACC,IAAR,IAAgB,IAApB,EAA0B;AACtB,mBAAOD,OAAP;AACH;;AACD,gBAAM;AAAEhE,YAAAA,MAAF;AAAUS,YAAAA,KAAV;AAAiBC,YAAAA;AAAjB,cAA2BsD,OAAjC;AACA,iBAAO,KAAKE,oBAAL,CAA0BlE,MAA1B,EAAkCS,KAAlC,EAAyCC,KAAzC,CAAP;AACH,SATkB,CAAnB,CAPe,CAiBf;AACA;AACA;AACA;;AACA,YAAI4C,QAAJ,EAAc;AACV,gBAAMa,aAAa,GAAG,KAAKC,qBAAL,CAA2BtJ,UAA3B,EAAuCmH,MAAvC,EAA+C8B,UAA/C,CAAtB;AACAzB,UAAAA,KAAK,GAAG,KAAK+B,0BAAL,CAAgCF,aAAhC,CAAR;AACH;;AACD,eAAOJ,UAAP;AACH,OA1BD;AA2BH,KAvCD,MAwCK;AACD,YAAM;AAAEO,QAAAA;AAAF,UAAkBlB,YAAxB,CADC,CAED;;AACA,YAAMmB,QAAQ,GAAIC,OAAD,IAAa;AAC1B;AACA;AACA;AACA,YAAI,CAAClB,QAAL,EAAe;AACX;AACH;;AACDhB,QAAAA,KAAK,GAAGkC,OAAO,CAAC/H,GAAR,CAAYgI,MAAM,IAAI,KAAKC,IAAL,CAAU,KAAK9C,KAAL,CAAW6C,MAAX,CAAV,CAAtB,CAAR;AACH,OARD;;AASAf,MAAAA,UAAU,GAAG,MAAM;AACf,cAAMb,gBAAgB,GAAG,KAAK5E,OAAL,CAAa+E,UAAb,EAAzB;AACAW,QAAAA,GAAG,GAAG,KAAK/C,IAAL,CAAU,MAAM0D,WAAW,CAAC,KAAKrG,OAAN,EAAesG,QAAf,CAA3B,CAAN;AACA,cAAMI,IAAI,GAAIrI,KAAK,CAACwH,OAAN,CAAcH,GAAd,IAAqBA,GAArB,GAA2B,CAACA,GAAD,CAAzC;;AACA,YAAI,KAAKhD,sBAAL,EAAJ,EAAmC;AAC/B;AACA,eAAKiC,qBAAL,CAA2BgB,iBAA3B,EAA8Cf,gBAA9C,EAAgE8B,IAAhE;AACH;;AACD,eAAOA,IAAP;AACH,OATD;AAUH,KAvFuB,CAwFxB;AACA;AACA;;;AACA,UAAM;AAAE1C,MAAAA,MAAF;AAAUI,MAAAA;AAAV,QAAoBe,YAA1B;AACA,UAAMwB,aAAa,GAAGhK,4BAA4B,CAACwI,YAAD,CAA5B,GAClB,IADkB,GAElBA,YAAY,CAACwB,aAFjB;AAGA,QAAIC,aAAJ;AACA,SAAK7D,SAAL,EACA;AACA,UAAM,KAAK7D,KAAL,CAAW3B,WAAX,EAFN,EAEgC,MAAM,KAAK2B,KAAL,CAAW3B,WAAX,EAFtC,EAEgE,MAAM;AAClE,UAAI,CAAC,KAAKuB,GAAL,CAAS4F,OAAT,CAAiB,OAAjB,CAAD,IAA8B,CAAC,KAAKxF,KAAL,CAAWrB,SAA9C,EAAyD;AACrDuH,QAAAA,OAAO,GAAGK,UAAU,EAApB;AACH,OAFD,MAGK;AACDmB,QAAAA,aAAa,GAAG,KAAK9F,QAAL,CAAc+F,aAAd,CAA4BlB,iBAA5B,EAA+C3B,MAA/C,EAAuD,MAAMyB,UAAU,EAAvE,CAAhB;;AACA,YAAI,KAAK3G,GAAL,CAAS4F,OAAT,CAAiB,OAAjB,CAAJ,EAA+B;AAC3B,eAAK5D,QAAL,CAAcgG,gBAAd,CAA+BF,aAA/B;AACH;;AACDxB,QAAAA,OAAO,GAAGwB,aAAa,CAACxB,OAAxB;AACH;AACJ,KAbD;;AAcA,QAAIC,QAAJ,EAAc;AACV,WAAKf,WAAL,CAAiBqB,iBAAjB,EAAoC3B,MAApC,EAA4CoB,OAA5C,EAAqDuB,aAArD,EAAoEtC,KAApE,EAA2ED,KAA3E;AACH;;AACD,QAAI,KAAKlF,KAAL,CAAWrB,SAAf,EAA0B;AACtB,WAAKqB,KAAL,CAAWpB,aAAX,CAAyBI,OAAzB,CAAiCsH,IAAjC,CAAsC;AAClC9G,QAAAA,IAAI,EAAEiH,iBAD4B;AAElCoB,QAAAA,UAAU,EAAE,KAAK7H,KAAL,CAAWhC,QAAX,GAAsBoI,iBAFA;AAGlC0B,QAAAA,kBAAkB,EAAE,KAAK9H,KAAL,CAAWhC,QAHG;AAIlC+J,QAAAA,YAAY,EAAE,KAAK/H,KAAL,CAAW/B,UAAX,GAAwBoI,kBAJJ;AAKlC2B,QAAAA,oBAAoB,EAAE,KAAKhI,KAAL,CAAW/B,UALC;AAMlCgK,QAAAA,WAAW,EAAE/G,MAAM,CAACC,IAAP,CAAY2D,MAAZ,EAAoBxF,GAApB,CAAwB4I,GAAG,IAAIpD,MAAM,CAACoD,GAAD,CAAN,IAAe,IAAf,GAAsBpD,MAAM,CAACoD,GAAD,CAAN,CAAY5E,KAAlC,GAA0C,IAAzE,CANqB;AAOlC6E,QAAAA,YAAY,EAAEjC,OAAO,CAAC5G,GAAR,CAAY8I,IAAI,IAAIA,IAAI,CAAC9E,KAAzB,CAPoB;AAQlC+E,QAAAA,YAAY,EAAEX,aAAa,CAACY,MARM;AASlCC,QAAAA,SAAS,EAAEb,aAAa,CAACa;AATS,OAAtC;AAWH;;AACD,WAAQpJ,KAAK,CAACwH,OAAN,CAAcH,GAAd,IAAqBN,OAArB,GAA+BA,OAAO,CAAC,CAAD,CAA9C;AACH;AACD;AACJ;AACA;AACA;AACA;;;AACIgB,EAAAA,0BAA0B,CAACG,OAAD,EAAU;AAChC,UAAMlC,KAAK,GAAGkC,OAAO,CAAC/H,GAAR,CAAYgI,MAAM,IAAI,KAAKC,IAAL,CAAU,KAAK9C,KAAL,CAAW6C,MAAX,CAAV,CAAtB,CAAd;AACA,WAAOnC,KAAP;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;;;AACI8B,EAAAA,qBAAqB,CAACtJ,UAAD,EAAamH,MAAb,EAAqBoB,OAArB,EAA8B;AAC/C,UAAMsC,UAAU,GAAG9L,WAAW,CAACiB,UAAD,CAA9B;;AACA,QAAI6K,UAAU,IAAI,IAAlB,EAAwB;AACpB,YAAMC,YAAY,GAAGD,UAAU,CAACC,YAAX,IAA2B,EAAhD;AACA,YAAMC,aAAa,GAAGF,UAAU,CAACE,aAAX,IAA4B,EAAlD,CAFoB,CAGpB;AACA;;AACA,UAAIC,kBAAJ;;AACA,UAAIH,UAAU,CAACI,aAAf,EAA8B;AAC1BxL,QAAAA,IAAI,CAACsJ,MAAL,CAAYvH,KAAK,CAACwH,OAAN,CAAc7B,MAAd,CAAZ,EAAmC,MAAM,wDAAzC;AACA6D,QAAAA,kBAAkB,GAAGzH,MAAM,CAACC,IAAP,CAAY2D,MAAZ,EAAoBxF,GAApB,CAAyB4I,GAAD,IAASpD,MAAM,CAACoD,GAAD,CAAvC,CAArB;AACH,OAHD,MAIK;AACDS,QAAAA,kBAAkB,GAAGF,YAAY,CAACnJ,GAAb,CAAkBuJ,SAAD,IAAe/D,MAAM,CAAC+D,SAAD,CAAtC,CAArB;AACH;;AACD,YAAMC,mBAAmB,GAAG5C,OAAO,CAAC6C,MAAR,CAAe,CAACC,CAAD,EAAIzI,CAAJ,KAAUmI,aAAa,CAACnI,CAAD,CAAtC,CAA5B;AACA,aAAOoI,kBAAkB,CAACM,MAAnB,CAA0BH,mBAA1B,CAAP;AACH,KAjB8C,CAkB/C;AACA;AACA;AACA;AACA;AACA;;;AACA,WAAO,EAAP;AACH;AACD;AACJ;AACA;AACA;AACA;;;AACII,EAAAA,UAAU,CAACjG,MAAD,EAASK,KAAT,EAAgBC,KAAhB,EAAuBzC,OAAvB,EAAgC;AACtC,QAAImC,MAAM,IAAI,IAAd,EAAoB;AAChB,YAAM,IAAIpC,KAAJ,CAAU,+CAAV,CAAN;AACH;;AACD0C,IAAAA,KAAK,GAAGA,KAAK,IAAI,SAAjB;AACAzC,IAAAA,OAAO,GAAGA,OAAO,IAAI,KAAKA,OAA1B;AACA,QAAIqI,WAAW,GAAGlG,MAAlB;;AACA,QAAIM,KAAK,KAAK,QAAV,IAAsBnG,IAAI,CAACgM,QAAL,CAAcnG,MAAM,CAAC,CAAD,CAApB,CAA1B,EAAoD;AAChDkG,MAAAA,WAAW,GAAGlG,MAAM,CAAC3D,GAAP,CAAW+J,CAAC,IAAIjM,IAAI,CAACkM,YAAL,CAAkBD,CAAlB,CAAhB,CAAd;AACH;;AACD,UAAMxG,MAAM,GAAG/B,OAAO,CAACyI,KAAR,CAAcJ,WAAd,EAA2B7F,KAA3B,EAAkCC,KAAlC,CAAf;AACA,UAAMiG,CAAC,GAAG,IAAIvM,MAAJ,CAAWqG,KAAX,EAAkBC,KAAlB,EAAyBV,MAAzB,EAAiC,KAAK0B,YAAL,EAAjC,CAAV;AACA,SAAKkF,WAAL,CAAiBD,CAAjB,EAAoB1I,OAApB,EAZsC,CAatC;;AACA,QAAIyC,KAAK,KAAK,QAAd,EAAwB;AACpB,YAAMT,IAAI,GAAG,KAAK9C,KAAL,CAAWvB,UAAX,CAAsBsE,GAAtB,CAA0BF,MAA1B,CAAb;AACA,YAAMhE,QAAQ,GAAGxB,oBAAoB,CAAC8L,WAAD,CAArC;AACA,WAAKnJ,KAAL,CAAWhC,QAAX,IAAuBa,QAAQ,GAAGiE,IAAI,CAAC4G,KAAvC;AACA5G,MAAAA,IAAI,CAAC4G,KAAL,GAAa7K,QAAb;AACH;;AACD,WAAO2K,CAAP;AACH;AACD;AACJ;AACA;AACA;AACA;;;AACIzC,EAAAA,oBAAoB,CAAClE,MAAD,EAASS,KAAT,EAAgBC,KAAhB,EAAuBzC,OAAvB,EAAgC;AAChDyC,IAAAA,KAAK,GAAGA,KAAK,IAAI,SAAjB;AACA,UAAMiG,CAAC,GAAG,IAAIvM,MAAJ,CAAWqG,KAAX,EAAkBC,KAAlB,EAAyBV,MAAzB,EAAiC,KAAK0B,YAAL,EAAjC,CAAV;AACA,SAAKkF,WAAL,CAAiBD,CAAjB,EAAoB1I,OAApB;AACA,WAAO0I,CAAP;AACH;;AACDG,EAAAA,YAAY,CAACC,YAAD,EAAeC,SAAS,GAAG,IAA3B,EAAiCrK,IAAjC,EAAuC+D,KAAvC,EAA8C;AACtD/D,IAAAA,IAAI,GAAGA,IAAI,IAAI,KAAKgF,cAAL,GAAsBsF,QAAtB,EAAf;;AACA,QAAIvG,KAAK,IAAI,IAAT,IAAiBA,KAAK,KAAKqG,YAAY,CAACrG,KAA5C,EAAmD;AAC/CqG,MAAAA,YAAY,GAAGA,YAAY,CAACG,IAAb,CAAkBxG,KAAlB,CAAf;AACH;;AACD,UAAMyG,CAAC,GAAG,IAAI9M,QAAJ,CAAa0M,YAAb,EAA2BC,SAA3B,EAAsCrK,IAAtC,EAA4C,KAAK+E,YAAL,EAA5C,CAAV;;AACA,QAAI,KAAKvE,KAAL,CAAWlC,mBAAX,CAA+BkM,CAAC,CAACxK,IAAjC,KAA0C,IAA9C,EAAoD;AAChD,YAAM,IAAIqB,KAAJ,CAAW,sBAAqBmJ,CAAC,CAACxK,IAAK,yBAAvC,CAAN;AACH;;AACD,SAAKQ,KAAL,CAAWlC,mBAAX,CAA+BkM,CAAC,CAACxK,IAAjC,IAAyCwK,CAAzC;AACA,SAAKC,MAAL,CAAYD,CAAZ,EAAe,KAAKlJ,OAApB;AACA,WAAOkJ,CAAP;AACH;;AACDP,EAAAA,WAAW,CAAC/G,CAAD,EAAI5B,OAAJ,EAAa;AACpB,SAAKd,KAAL,CAAW/B,UAAX;;AACA,QAAIyE,CAAC,CAACa,KAAF,KAAY,QAAhB,EAA0B;AACtB,WAAKvD,KAAL,CAAW9B,gBAAX;AACH,KAJmB,CAKpB;AACA;;;AACA,QAAIwL,KAAK,GAAG,CAAZ;;AACA,QAAIhH,CAAC,CAACa,KAAF,KAAY,WAAZ,IAA2Bb,CAAC,CAACa,KAAF,KAAY,QAA3C,EAAqD;AACjDmG,MAAAA,KAAK,GAAGhH,CAAC,CAACwH,IAAF,GAAS9M,IAAI,CAAC+M,eAAL,CAAqBzH,CAAC,CAACa,KAAvB,CAAjB;AACH;;AACD,SAAKvD,KAAL,CAAWhC,QAAX,IAAuB0L,KAAvB;;AACA,QAAI,CAAC,KAAK1J,KAAL,CAAWvB,UAAX,CAAsB2L,GAAtB,CAA0B1H,CAAC,CAACG,MAA5B,CAAL,EAA0C;AACtC,WAAK7C,KAAL,CAAW7B,cAAX;AACA,WAAK6B,KAAL,CAAWvB,UAAX,CAAsB4L,GAAtB,CAA0B3H,CAAC,CAACG,MAA5B,EAAoC;AAChC/B,QAAAA,OAAO,EAAEA,OAAO,IAAI,KAAKA,OADO;AAEhCyC,QAAAA,KAAK,EAAEb,CAAC,CAACa,KAFuB;AAGhCD,QAAAA,KAAK,EAAEZ,CAAC,CAACY,KAHuB;AAIhCoG,QAAAA;AAJgC,OAApC;AAMH;;AACD,QAAI,EAAEhH,CAAC,YAAYxF,QAAf,CAAJ,EAA8B;AAC1B,WAAKoN,KAAL,CAAW5H,CAAX;AACH;AACJ,GAplBe,CAqlBhB;AACA;AACA;AACA;AACA;;;AACAuH,EAAAA,MAAM,CAACvH,CAAD,EAAI5B,OAAJ,EAAa;AACf,SAAK2I,WAAL,CAAiB/G,CAAjB,EAAoB5B,OAApB;AACA,SAAKA,OAAL,CAAamJ,MAAb,CAAoBvH,CAAC,CAACG,MAAtB;AACH;;AACD0H,EAAAA,YAAY,CAAC1H,MAAD,EAAS/B,OAAT,EAAkB;AAC1B,QAAI,KAAKd,KAAL,CAAWvB,UAAX,CAAsB2L,GAAtB,CAA0BvH,MAA1B,KACA,KAAK7C,KAAL,CAAWvB,UAAX,CAAsBsE,GAAtB,CAA0BF,MAA1B,EAAkC/B,OAAlC,KAA8CA,OADlD,EAC2D;AACvD,WAAKd,KAAL,CAAWvB,UAAX,CAAsB+L,MAAtB,CAA6B3H,MAA7B;AACA,WAAK7C,KAAL,CAAW7B,cAAX;AACH;AACJ;;AACDsM,EAAAA,aAAa,CAAC/H,CAAD,EAAI;AACb,QAAI,CAAC,KAAK1C,KAAL,CAAWvB,UAAX,CAAsB2L,GAAtB,CAA0B1H,CAAC,CAACG,MAA5B,CAAL,EAA0C;AACtC;AACH;;AACD,UAAMC,IAAI,GAAG,KAAK9C,KAAL,CAAWvB,UAAX,CAAsBsE,GAAtB,CAA0BL,CAAC,CAACG,MAA5B,CAAb;AACA,SAAK7C,KAAL,CAAW/B,UAAX;;AACA,QAAIyE,CAAC,CAACa,KAAF,KAAY,QAAhB,EAA0B;AACtB,WAAKvD,KAAL,CAAW9B,gBAAX;AACA,WAAK8B,KAAL,CAAWhC,QAAX,IAAuB8E,IAAI,CAAC4G,KAA5B;AACH,KATY,CAUb;AACA;;;AACA,QAAIhH,CAAC,CAACa,KAAF,KAAY,WAAZ,IAA2Bb,CAAC,CAACa,KAAF,KAAY,QAA3C,EAAqD;AACjD,YAAMmG,KAAK,GAAGhH,CAAC,CAACwH,IAAF,GAAS9M,IAAI,CAAC+M,eAAL,CAAqBzH,CAAC,CAACa,KAAvB,CAAvB;AACA,WAAKvD,KAAL,CAAWhC,QAAX,IAAuB0L,KAAvB;AACH,KAfY,CAgBb;;;AACA,QAAI5G,IAAI,CAAChC,OAAL,CAAasC,WAAb,CAAyBV,CAAC,CAACG,MAA3B,CAAJ,EAAwC;AACpC,WAAK0H,YAAL,CAAkB7H,CAAC,CAACG,MAApB,EAA4BC,IAAI,CAAChC,OAAjC;AACH,KAnBY,CAoBb;AACA;AACA;;AACH;;AACD4J,EAAAA,gBAAgB,GAAG;AACf,SAAK,MAAMC,OAAX,IAAsB,KAAK3K,KAAL,CAAWlC,mBAAjC,EAAsD;AAClD,YAAMkM,CAAC,GAAG,KAAKhK,KAAL,CAAWlC,mBAAX,CAA+B6M,OAA/B,CAAV;AACA,WAAKC,eAAL,CAAqBZ,CAArB;AACH;AACJ;;AACDY,EAAAA,eAAe,CAACZ,CAAD,EAAI;AACf,SAAKS,aAAL,CAAmBT,CAAnB;;AACA,QAAI,KAAKhK,KAAL,CAAWlC,mBAAX,CAA+BkM,CAAC,CAACxK,IAAjC,KAA0C,IAA9C,EAAoD;AAChD,aAAO,KAAKQ,KAAL,CAAWlC,mBAAX,CAA+BkM,CAAC,CAACxK,IAAjC,CAAP;AACH;AACJ;;AACDqL,EAAAA,MAAM,GAAG;AACL,UAAM/H,IAAI,GAAG,KAAKhC,OAAL,CAAa+J,MAAb,EAAb;AACA/H,IAAAA,IAAI,CAAC7E,UAAL,GAAkB,KAAK+B,KAAL,CAAW/B,UAA7B;AACA6E,IAAAA,IAAI,CAAC3E,cAAL,GAAsB,KAAK6B,KAAL,CAAW7B,cAAjC;AACA2E,IAAAA,IAAI,CAAC9E,QAAL,GAAgB,KAAKgC,KAAL,CAAWhC,QAA3B;;AACA,QAAI,KAAKgC,KAAL,CAAW9B,gBAAX,GAA8B,CAAlC,EAAqC;AACjC4E,MAAAA,IAAI,CAACgI,UAAL,GAAkB,IAAlB;;AACA,UAAIhI,IAAI,CAACiI,OAAL,IAAgB,IAApB,EAA0B;AACtBjI,QAAAA,IAAI,CAACiI,OAAL,GAAe,EAAf;AACH;;AACDjI,MAAAA,IAAI,CAACiI,OAAL,CAAazE,IAAb,CAAkB,mDACd,yBADJ;AAEH;;AACD,WAAOxD,IAAP;AACH;;AACKkI,EAAAA,OAAO,CAACC,KAAD,EAAQ;AAAA;;AAAA;AACjB,MAAA,MAAI,CAACjL,KAAL,CAAWrB,SAAX,GAAuB,IAAvB;AACA,YAAMuM,UAAU,GAAG,MAAI,CAAClL,KAAL,CAAWhC,QAA9B;AACA,YAAMmN,eAAe,GAAG,MAAI,CAACnL,KAAL,CAAW/B,UAAnC;AACA,MAAA,MAAI,CAAC+B,KAAL,CAAWpB,aAAX,CAAyBI,OAAzB,GAAmC,EAAnC;AACA,MAAA,MAAI,CAACgB,KAAL,CAAWpB,aAAX,CAAyBK,MAAzB,SAAwCgM,KAAK,EAA7C;AACA,MAAA,MAAI,CAACjL,KAAL,CAAWrB,SAAX,GAAuB,KAAvB;AACA,MAAA,MAAI,CAACqB,KAAL,CAAWpB,aAAX,CAAyBG,SAAzB,GAAqCqM,IAAI,CAACC,GAAL,CAAS,GAAG,MAAI,CAACrL,KAAL,CAAWpB,aAAX,CAAyBI,OAAzB,CAAiCM,GAAjC,CAAqC+J,CAAC,IAAIA,CAAC,CAACvB,kBAA5C,CAAZ,CAArC;AACA,MAAA,MAAI,CAAC9H,KAAL,CAAWpB,aAAX,CAAyBC,QAAzB,GAAoC,MAAI,CAACmB,KAAL,CAAWhC,QAAX,GAAsBkN,UAA1D;AACA,MAAA,MAAI,CAAClL,KAAL,CAAWpB,aAAX,CAAyBE,UAAzB,GACI,MAAI,CAACkB,KAAL,CAAW/B,UAAX,GAAwBkN,eAD5B;;AAEA,WAAK,MAAMrJ,MAAX,IAAqB,MAAI,CAAC9B,KAAL,CAAWpB,aAAX,CAAyBI,OAA9C,EAAuD;AACnD8C,QAAAA,MAAM,CAACuG,YAAP,SAA4BvG,MAAM,CAACuG,YAAnC;AACAvG,QAAAA,MAAM,CAACyG,SAAP,SAAyBzG,MAAM,CAACyG,SAAhC;AACH;;AACD,aAAO,MAAI,CAACvI,KAAL,CAAWpB,aAAlB;AAfiB;AAgBpB;;AACDuH,EAAAA,QAAQ,GAAG;AACP,WAAO,KAAKnG,KAAL,CAAW5B,aAAX,GAA2B,CAA3B,IAAgC,KAAK4B,KAAL,CAAW3B,WAAX,KAA2B,CAAlE;AACH;;AACD+G,EAAAA,WAAW,CAACzH,UAAD,EAAamH,MAAb,EAAqBoB,OAArB,EAA8BoF,aAA9B,EAA6CnG,KAA7C,EAAoDD,KAApD,EAA2D;AAClE,UAAMqG,QAAQ,GAAG;AAAEC,MAAAA,EAAE,EAAE,KAAKxL,KAAL,CAAWjC,cAAX,EAAN;AAAmCJ,MAAAA,UAAnC;AAA+CmH,MAAAA,MAA/C;AAAuDoB,MAAAA,OAAvD;AAAgEf,MAAAA;AAAhE,KAAjB;AACA,UAAMqD,UAAU,GAAG9L,WAAW,CAACiB,UAAD,CAA9B;;AACA,QAAI6K,UAAU,IAAI,IAAlB,EAAwB;AACpB8C,MAAAA,aAAa,GAAG9C,UAAU,CAACiD,QAA3B;AACH;;AACD,QAAIH,aAAa,IAAI,IAArB,EAA2B;AACvBC,MAAAA,QAAQ,CAACG,QAAT,GAAqBC,GAAD,IAAS;AACzB;AACA;AACAA,QAAAA,GAAG,GAAGA,GAAG,CAACrM,GAAJ,CAAQ,CAAC0F,EAAD,EAAKzE,CAAL,KAAW;AACrB,cAAIyE,EAAE,IAAI,IAAV,EAAgB;AACZ,kBAAM4G,MAAM,GAAG1F,OAAO,CAAC3F,CAAD,CAAtB;AACA,kBAAMsL,IAAI,GAAGzO,IAAI,CAAC0O,mBAAL,CAAyBF,MAAM,CAAC1B,IAAhC,EAAsC0B,MAAM,CAACrI,KAA7C,CAAb;AACA,mBAAO,KAAK2F,UAAL,CAAgB2C,IAAhB,EAAsBD,MAAM,CAACtI,KAA7B,EAAoCsI,MAAM,CAACrI,KAA3C,CAAP;AACH;;AACD,iBAAOyB,EAAP;AACH,SAPK,CAAN,CAHyB,CAWzB;AACA;;AACA,eAAOsG,aAAa,CAACK,GAAG,CAACnL,MAAJ,GAAa,CAAb,GAAiBmL,GAAjB,GAAuBA,GAAG,CAAC,CAAD,CAA3B,EAAgCxG,KAAhC,EAAuCD,KAAvC,CAApB;AACH,OAdD;AAeH;;AACD,SAAKlF,KAAL,CAAW+L,UAAX,CAAsBzF,IAAtB,CAA2BiF,QAA3B;AACH;;AACDhE,EAAAA,IAAI,CAACtI,MAAD,EAAS;AACTA,IAAAA,MAAM,CAAC+M,IAAP,GAAc,IAAd;AACA,WAAO/M,MAAP;AACH;;AACDgN,EAAAA,SAAS,GAAG;AACR,QAAI,KAAKjM,KAAL,CAAW5B,aAAX,KAA6B,CAAjC,EAAoC;AAChC,WAAK4B,KAAL,CAAW+L,UAAX,GAAwB,EAAxB;AACH;;AACD,SAAK/L,KAAL,CAAW5B,aAAX;AACH;;AACD8N,EAAAA,OAAO,GAAG;AACN,SAAKlM,KAAL,CAAW5B,aAAX;AACH;AACD;AACJ;AACA;AACA;;;AACI0F,EAAAA,UAAU,CAACtE,IAAD,EAAO;AACb,UAAM2M,SAAS,GAAG;AACd7B,MAAAA,KAAK,EAAE,EADO;AAEd9K,MAAAA,IAAI,EAAE,eAFQ;AAGdgM,MAAAA,EAAE,EAAE,KAAKxL,KAAL,CAAWxB,WAAX;AAHU,KAAlB;;AAKA,QAAIgB,IAAJ,EAAU;AACN2M,MAAAA,SAAS,CAAC3M,IAAV,GAAiBA,IAAjB;AACH;;AACD,SAAKQ,KAAL,CAAW1B,UAAX,CAAsBgI,IAAtB,CAA2B6F,SAA3B;AACA,SAAKnM,KAAL,CAAWqF,WAAX,GAAyB8G,SAAzB;AACH;AACD;AACJ;AACA;AACA;;;AACIpI,EAAAA,QAAQ,CAAC9E,MAAD,EAAS;AACb,UAAMmN,sBAAsB,GAAGjP,qBAAqB,CAAC8B,MAAD,CAApD;AACA,UAAMoN,yBAAyB,GAAG,IAAIhN,GAAJ,CAAQ+M,sBAAsB,CAAC9M,GAAvB,CAA2BkK,CAAC,IAAIA,CAAC,CAACgC,EAAlC,CAAR,CAAlC,CAFa,CAGb;;AACA,SAAK,IAAIjL,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAAKP,KAAL,CAAWqF,WAAX,CAAuBiF,KAAvB,CAA6B9J,MAAjD,EAAyDD,CAAC,EAA1D,EAA8D;AAC1D,YAAM+G,MAAM,GAAG,KAAKtH,KAAL,CAAWqF,WAAX,CAAuBiF,KAAvB,CAA6B/J,CAA7B,CAAf;;AACA,UAAI,CAAC+G,MAAM,CAAC0E,IAAR,IAAgB,CAACK,yBAAyB,CAACjC,GAA1B,CAA8B9C,MAAM,CAACkE,EAArC,CAArB,EAA+D;AAC3DlE,QAAAA,MAAM,CAAC7H,OAAP;AACH;AACJ;;AACD,UAAM6M,QAAQ,GAAG,KAAKtM,KAAL,CAAW1B,UAAX,CAAsBiO,GAAtB,EAAjB;AACA,SAAKvM,KAAL,CAAWqF,WAAX,GAAyB,KAAKrF,KAAL,CAAW1B,UAAX,CAAsBkC,MAAtB,KAAiC,CAAjC,GACrB,IADqB,GAErB,KAAKR,KAAL,CAAW1B,UAAX,CAAsB,KAAK0B,KAAL,CAAW1B,UAAX,CAAsBkC,MAAtB,GAA+B,CAArD,CAFJ,CAXa,CAcb;;AACA4L,IAAAA,sBAAsB,CAACvK,OAAvB,CAA+ByF,MAAM,IAAI;AACrC;AACA;AACA,UAAI,CAACA,MAAM,CAAC0E,IAAR,IAAgB1E,MAAM,CAACkF,OAAP,KAAmBF,QAAQ,CAACd,EAAhD,EAAoD;AAChD,aAAKlB,KAAL,CAAWhD,MAAX;AACH;AACJ,KAND;AAOH;AACD;AACJ;AACA;AACA;AACA;AACA;;;AACImF,EAAAA,SAAS,CAACrI,CAAD,EAAIsI,EAAJ,EAAQ1H,EAAR,EAAY2H,gBAAgB,GAAG,KAA/B,EAAsC;AAC3CvP,IAAAA,IAAI,CAACsJ,MAAL,CAAYgG,EAAE,CAAClM,MAAH,GAAY,CAAxB,EAA2B,MAAM,2CAAjC;;AACA,QAAIwE,EAAE,IAAI,IAAN,IAAcA,EAAE,CAACzB,KAAH,KAAa,SAA/B,EAA0C;AACtC,YAAM,IAAI1C,KAAJ,CAAW,0CAAyCmE,EAAE,CAACzB,KAAM,GAA7D,CAAN;AACH;;AACD,UAAMoB,CAAC,GAAG,KAAKd,SAAL,CAAe,MAAM,KAAKoI,SAAL,EAArB,EAAuC,MAAM,KAAKC,OAAL,EAA7C,EAA6D,MAAM,KAAKzI,IAAL,CAAU,SAAV,EAAqBW,CAArB,CAAnE,CAAV;AACAhH,IAAAA,IAAI,CAACsJ,MAAL,CAAY/B,CAAC,YAAY1H,MAAzB,EAAiC,MAAM,gDAAvC,EAN2C,CAO3C;;AACA,UAAM2P,YAAY,GAAG7P,oBAAoB,CAAC,KAAKiD,KAAL,CAAW+L,UAAZ,EAAwBW,EAAxB,EAA4B/H,CAA5B,CAAzC;;AACA,QAAI,CAACgI,gBAAD,IAAqBC,YAAY,CAACpM,MAAb,KAAwB,CAA7C,IAAkDkM,EAAE,CAAClM,MAAH,GAAY,CAAlE,EAAqE;AACjE,YAAM,IAAIK,KAAJ,CAAU,oEACZ,iEADY,GAEZ,OAFE,CAAN;AAGH;;AACD,WAAO,KAAK4C,IAAL,CAAU,UAAV,EAAsB,MAAM;AAC/B,YAAMoJ,sBAAsB,GAAG,EAA/B;AACAA,MAAAA,sBAAsB,CAAClI,CAAC,CAAC6G,EAAH,CAAtB,GAAgCxG,EAAE,IAAI,IAAP,GAAe8H,IAAI,CAACnI,CAAC,CAACrB,KAAH,CAAnB,GAA+B0B,EAA9D,CAF+B,CAG/B;;AACAlI,MAAAA,sBAAsB,CAAC+P,sBAAD,EAAyBD,YAAzB,EACtB;AACAxI,MAAAA,CAAC,IAAI,KAAKX,IAAL,CAAUW,CAAV,CAFiB,EAGtB;AACA2I,MAAAA,GAJsB,CAAtB;AAKA,YAAMC,KAAK,GAAGN,EAAE,CAACpN,GAAH,CAAOoF,CAAC,IAAImI,sBAAsB,CAACnI,CAAC,CAAC8G,EAAH,CAAlC,CAAd;;AACA,UAAI,KAAKxL,KAAL,CAAW5B,aAAX,KAA6B,CAAjC,EAAoC;AAChC;AACA;AACA,aAAK4B,KAAL,CAAW+L,UAAX,CAAsBlK,OAAtB,CAA8BoL,IAAI,IAAI;AAClC,eAAK,MAAM3F,MAAX,IAAqB2F,IAAI,CAAC9H,KAA1B,EAAiC;AAC7BmC,YAAAA,MAAM,CAAC7H,OAAP;AACH;AACJ,SAJD;AAKA,aAAKO,KAAL,CAAW+L,UAAX,GAAwB,IAAxB;AACH;;AACD,aAAO;AAAEmB,QAAAA,KAAK,EAAEvI,CAAT;AAAYqI,QAAAA;AAAZ,OAAP;AACH,KArBM,CAAP;AAsBH;;AACDG,EAAAA,UAAU,CAAC/I,CAAD,EAAI;AACVhH,IAAAA,IAAI,CAACsJ,MAAL,CAAYtJ,IAAI,CAACgQ,UAAL,CAAgBhJ,CAAhB,CAAZ,EAAgC,MAAM,mDAAtC;AACA,WAAO,CAAC,GAAGU,MAAJ,KAAe;AAClB1H,MAAAA,IAAI,CAACsJ,MAAL,CAAY5B,MAAM,CAACuI,KAAP,CAAa7D,CAAC,IAAIA,CAAC,YAAYvM,MAA/B,CAAZ,EAAoD,MAAM,8DACtD,SADJ;AAEA,UAAIoH,GAAJ;AACA,YAAMiJ,QAAQ,GAAG,EAAjB;AACAxI,MAAAA,MAAM,CAACjD,OAAP,CAAe,CAAC0L,KAAD,EAAQhN,CAAR,KAAc;AACzB+M,QAAAA,QAAQ,CAAC/M,CAAD,CAAR,GAAcgN,KAAd;AACH,OAFD;;AAGA,YAAMpG,WAAW,GAAG,CAAC6B,CAAD,EAAIwE,IAAJ,KAAa;AAC7BnJ,QAAAA,GAAG,GAAGD,CAAC,CAAC,GAAG,CAAC,GAAGU,MAAJ,EAAY0I,IAAZ,CAAJ,CAAP;AACApQ,QAAAA,IAAI,CAACsJ,MAAL,CAAYrC,GAAG,CAAC6I,KAAJ,YAAqBjQ,MAAjC,EAAyC,MAAM,2DAC3C,sCADJ;AAEAG,QAAAA,IAAI,CAACsJ,MAAL,CAAYtJ,IAAI,CAACgQ,UAAL,CAAgB/I,GAAG,CAACoH,QAApB,CAAZ,EAA2C,MAAM,2DAC7C,4CADJ;AAEA,eAAOpH,GAAG,CAAC6I,KAAX;AACH,OAPD;;AAQA,YAAMzF,aAAa,GAAG,CAACzC,EAAD,EAAKG,KAAL,KAAe;AACjC,cAAMsI,OAAO,GAAGpJ,GAAG,CAACoH,QAAJ,CAAazG,EAAb,EAAiBG,KAAjB,CAAhB;AACA,cAAM6H,KAAK,GAAG7N,KAAK,CAACwH,OAAN,CAAc8G,OAAd,IAAyBA,OAAzB,GAAmC,CAACA,OAAD,CAAjD;AACArQ,QAAAA,IAAI,CAACsJ,MAAL,CAAYsG,KAAK,CAACxM,MAAN,KAAiBsE,MAAM,CAACtE,MAApC,EAA4C,MAAM,2DAC9C,yDAD8C,GAE9C,wDAFJ;AAGApD,QAAAA,IAAI,CAACsJ,MAAL,CAAYsG,KAAK,CAACK,KAAN,CAAY7D,CAAC,IAAIA,CAAC,YAAYvM,MAA9B,CAAZ,EAAmD,MAAM,2DACrD,yDADqD,GAErD,yBAFJ;AAGA,cAAMyQ,OAAO,GAAG,EAAhB;AACAV,QAAAA,KAAK,CAACnL,OAAN,CAAc,CAACkD,IAAD,EAAOxE,CAAP,KAAa;AACvBmN,UAAAA,OAAO,CAACnN,CAAD,CAAP,GAAa,MAAMwE,IAAnB;AACH,SAFD;AAGA,eAAO2I,OAAP;AACH,OAdD;;AAeA,aAAO,KAAKnI,aAAL,CAAmB;AACtB4B,QAAAA,WADsB;AAEtBM,QAAAA,aAFsB;AAGtB3C,QAAAA,MAAM,EAAEwI;AAHc,OAAnB,CAAP;AAKH,KApCD;AAqCH;;AACDpK,EAAAA,QAAQ,CAACL,MAAD,EAAS;AACb;AACA,UAAMC,IAAI,GAAG,KAAK9C,KAAL,CAAWvB,UAAX,CAAsBsE,GAAtB,CAA0BF,MAA1B,CAAb;AACA,WAAOC,IAAI,CAAChC,OAAL,CAAaoC,QAAb,CAAsBL,MAAtB,CAAP;AACH;;AACD8K,EAAAA,IAAI,CAAC9K,MAAD,EAAS;AACT;AACA,UAAMC,IAAI,GAAG,KAAK9C,KAAL,CAAWvB,UAAX,CAAsBsE,GAAtB,CAA0BF,MAA1B,CAAb;AACA,WAAOC,IAAI,CAAChC,OAAL,CAAa6M,IAAb,CAAkB9K,MAAlB,CAAP;AACH;;AACK+K,EAAAA,IAAI,CAAC3C,KAAD,EAAQ;AAAA;;AAAA;AACd,YAAM/G,KAAK,GAAG3G,GAAG,EAAjB;AACA,YAAMsQ,UAAU,SAAS,MAAI,CAAC/M,OAAL,CAAa8M,IAAb,CAAkB3C,KAAlB,CAAzB;AACA4C,MAAAA,UAAU,CAACC,MAAX,GAAoBvQ,GAAG,KAAK2G,KAA5B;AACA,aAAO2J,UAAP;AAJc;AAKjB;AACD;AACJ;AACA;AACA;AACA;AACA;;;AACIvD,EAAAA,KAAK,CAACrL,MAAD,EAAS;AACV,QAAI,KAAKe,KAAL,CAAWqF,WAAX,IAA0B,IAA9B,EAAoC;AAChCpG,MAAAA,MAAM,CAACuN,OAAP,GAAiB,KAAKxM,KAAL,CAAWqF,WAAX,CAAuBmG,EAAxC;AACA,WAAKxL,KAAL,CAAWqF,WAAX,CAAuBiF,KAAvB,CAA6BhE,IAA7B,CAAkCrH,MAAlC;AACH;;AACD,WAAOA,MAAP;AACH;;AACsB,MAAnBnB,mBAAmB,GAAG;AACtB,WAAO,KAAKkC,KAAL,CAAWlC,mBAAlB;AACH;AACD;AACJ;AACA;AACA;;;AACIiQ,EAAAA,KAAK,GAAG;AACJ;AACA,SAAKhO,oBAAL;AACA,SAAKC,KAAL,CAAWP,OAAX;AACA,SAAKG,GAAL,CAASmO,KAAT;AACA,SAAK/N,KAAL,GAAa,IAAIpC,WAAJ,EAAb;;AACA,SAAK,MAAM6C,WAAX,IAA0B,KAAKZ,QAA/B,EAAyC;AACrC,WAAKmC,wBAAL,CAA8BvB,WAA9B;AACA,WAAKZ,QAAL,CAAcY,WAAd,EAA2BhB,OAA3B;AACA,aAAO,KAAKI,QAAL,CAAcY,WAAd,CAAP;AACH;;AACD,SAAKA,WAAL,GAAmB,IAAnB;AACA,SAAKL,eAAL,GAAuB,IAAvB;AACA,SAAKF,kBAAL,GAA0B,IAA1B;AACH;;AAl4Be;AAo4BpBP,MAAM,CAAC4E,YAAP,GAAsB,CAAtB;AACA5E,MAAM,CAAC6E,cAAP,GAAwB,CAAxB;;AACA,SAASsI,IAAT,CAAcxJ,KAAd,EAAqB;AACjB,QAAML,MAAM,GAAG3F,kBAAkB,CAACE,aAAa,CAAC8F,KAAD,CAAd,EAAuB,SAAvB,CAAjC;AACA,SAAOsB,MAAM,CAACsE,UAAP,CAAkBjG,MAAlB,EAA0BK,KAA1B,EAAiC,SAAjC,CAAP;AACH;;AACD,OAAO,SAAS0K,eAAT,GAA2B;AAC9B,QAAMC,EAAE,GAAG3R,kBAAkB,EAA7B;;AACA,MAAI2R,EAAE,CAACC,SAAH,IAAgB,IAApB,EAA0B;AACtB,UAAMC,WAAW,GAAG,IAAI/R,WAAJ,CAAgB6R,EAAhB,CAApB;AACAA,IAAAA,EAAE,CAACC,SAAH,GAAe,IAAIvO,MAAJ,CAAWwO,WAAX,CAAf;AACH;;AACD9R,EAAAA,oBAAoB,CAAC4R,EAAE,CAACC,SAAH,CAAatO,GAAd,CAApB,CAN8B,CAO9B;AACA;;AACA5C,EAAAA,gBAAgB,CAAC,MAAMiR,EAAE,CAACC,SAAV,CAAhB;AACA,SAAOD,EAAE,CAACC,SAAV;AACH;AACD,OAAO,MAAMtJ,MAAM,GAAGoJ,eAAe,EAA9B;AACP;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASjB,GAAT,CAAarK,CAAb,EAAgBC,CAAhB,EAAmB;AACtB;AACA,QAAMmC,MAAM,GAAG;AAAEpC,IAAAA,CAAF;AAAKC,IAAAA;AAAL,GAAf;AACA,SAAOiC,MAAM,CAACC,SAAP,CAAiBtI,GAAjB,EAAsBuI,MAAtB,CAAP;AACH","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { KernelBackend } from './backends/backend';\nimport { Environment, setEnvironmentGlobal } from './environment';\nimport { getGlobalNamespace } from './global_util';\nimport { Add, Cast, Identity } from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\nimport { Profiler } from './profiler';\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\nimport { setTensorTracker, Tensor, Variable } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\nimport * as util from './util';\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\nfunction isRegisteredKernelInvocation(kernelInvocation) {\n    return kernelInvocation.kernelName != null;\n}\nclass EngineState {\n    constructor() {\n        // Public since optimizers will use it.\n        this.registeredVariables = {};\n        this.nextTapeNodeId = 0;\n        this.numBytes = 0;\n        this.numTensors = 0;\n        this.numStringTensors = 0;\n        this.numDataBuffers = 0;\n        // Number of nested tf.grad() statements when computing higher-order\n        // gradients. E.g. `1` for first-order gradients and `2` for second-order\n        // gradients. Used to track if the tape should be removed after a backprop.\n        this.gradientDepth = 0;\n        // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n        // off the tape.\n        this.kernelDepth = 0;\n        this.scopeStack = [];\n        /**\n         * Keeps track of the number of data moves during a kernel execution. We\n         * maintain a stack since kernels can call other kernels, recursively.\n         */\n        this.numDataMovesStack = [];\n        this.nextScopeId = 0;\n        this.tensorInfo = new WeakMap();\n        this.profiling = false;\n        this.activeProfile = {\n            newBytes: 0,\n            newTensors: 0,\n            peakBytes: 0,\n            kernels: [],\n            result: null,\n            get kernelNames() {\n                return Array.from(new Set(this.kernels.map(k => k.name)));\n            }\n        };\n    }\n    dispose() {\n        for (const variableName in this.registeredVariables) {\n            this.registeredVariables[variableName].dispose();\n        }\n    }\n}\nexport class Engine {\n    constructor(ENV) {\n        this.ENV = ENV;\n        this.registry = {};\n        this.registryFactory = {};\n        this.pendingBackendInitId = 0;\n        this.state = new EngineState();\n    }\n    async ready() {\n        if (this.pendingBackendInit != null) {\n            return this.pendingBackendInit.then(() => { });\n        }\n        if (this.backendInstance != null) {\n            return;\n        }\n        const sortedBackends = this.getSortedBackends();\n        for (let i = 0; i < sortedBackends.length; i++) {\n            const backendName = sortedBackends[i];\n            const success = await this.initializeBackend(backendName).success;\n            if (success) {\n                await this.setBackend(backendName);\n                return;\n            }\n        }\n        throw new Error(`Could not initialize any backends, all backend initializations ` +\n            `failed.`);\n    }\n    get backend() {\n        if (this.pendingBackendInit != null) {\n            throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make ` +\n                `sure to await tf.ready() or await tf.setBackend() before calling ` +\n                `other methods`);\n        }\n        if (this.backendInstance == null) {\n            const { name, asyncInit } = this.initializeBackendsAndReturnBest();\n            if (asyncInit) {\n                throw new Error(`The highest priority backend '${name}' has not yet been ` +\n                    `initialized. Make sure to await tf.ready() or ` +\n                    `await tf.setBackend() before calling other methods`);\n            }\n            this.setBackend(name);\n        }\n        return this.backendInstance;\n    }\n    backendNames() {\n        return Object.keys(this.registryFactory);\n    }\n    findBackend(backendName) {\n        if (!(backendName in this.registry)) {\n            // If the backend hasn't been initialized but we have a registry entry for\n            // it, initialize it and return it.\n            if (backendName in this.registryFactory) {\n                const { asyncInit } = this.initializeBackend(backendName);\n                if (asyncInit) {\n                    // Backend is not ready yet.\n                    return null;\n                }\n            }\n            else {\n                return null;\n            }\n        }\n        return this.registry[backendName];\n    }\n    findBackendFactory(backendName) {\n        if (!(backendName in this.registryFactory)) {\n            return null;\n        }\n        return this.registryFactory[backendName].factory;\n    }\n    registerBackend(backendName, factory, priority = 1) {\n        if (backendName in this.registryFactory) {\n            console.warn(`${backendName} backend was already registered. ` +\n                `Reusing existing backend factory.`);\n            return false;\n        }\n        this.registryFactory[backendName] = { factory, priority };\n        return true;\n    }\n    async setBackend(backendName) {\n        if (this.registryFactory[backendName] == null) {\n            throw new Error(`Backend name '${backendName}' not found in registry`);\n        }\n        this.backendName = backendName;\n        if (this.registry[backendName] == null) {\n            this.backendInstance = null;\n            const { success, asyncInit } = this.initializeBackend(backendName);\n            const result = asyncInit ? await success : success;\n            if (!result) {\n                return false;\n            }\n        }\n        this.backendInstance = this.registry[backendName];\n        this.setupRegisteredKernels();\n        // Reset the profiler.\n        this.profiler = new Profiler(this.backendInstance);\n        return true;\n    }\n    setupRegisteredKernels() {\n        const kernels = getKernelsForBackend(this.backendName);\n        kernels.forEach(kernel => {\n            if (kernel.setupFunc != null) {\n                kernel.setupFunc(this.backendInstance);\n            }\n        });\n    }\n    disposeRegisteredKernels(backendName) {\n        const kernels = getKernelsForBackend(backendName);\n        kernels.forEach(kernel => {\n            if (kernel.disposeFunc != null) {\n                kernel.disposeFunc(this.registry[backendName]);\n            }\n        });\n    }\n    /**\n     * Initializes a backend by looking up the backend name in the factory\n     * registry and calling the factory method. Returns a boolean representing\n     * whether the initialization of the backend suceeded. Throws an error if\n     * there is no backend in the factory registry.\n     */\n    initializeBackend(backendName) {\n        const registryFactoryEntry = this.registryFactory[backendName];\n        if (registryFactoryEntry == null) {\n            throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);\n        }\n        try {\n            const backend = registryFactoryEntry.factory();\n            /* Test if the factory returns a promise.\n            Done in a more liberal way than\n            previous 'Promise.resolve(backend)===backend'\n            as we needed to account for custom Promise\n            implementations (e.g. Angular) */\n            if (backend && !(backend instanceof KernelBackend) &&\n                typeof backend.then === 'function') {\n                const promiseId = ++this.pendingBackendInitId;\n                const success = backend\n                    .then(backendInstance => {\n                    // Outdated promise. Another backend was set in the meantime.\n                    if (promiseId < this.pendingBackendInitId) {\n                        return false;\n                    }\n                    this.registry[backendName] = backendInstance;\n                    this.pendingBackendInit = null;\n                    return true;\n                })\n                    .catch(err => {\n                    // Outdated promise. Another backend was set in the meantime.\n                    if (promiseId < this.pendingBackendInitId) {\n                        return false;\n                    }\n                    this.pendingBackendInit = null;\n                    console.warn(`Initialization of backend ${backendName} failed`);\n                    console.warn(err.stack || err.message);\n                    return false;\n                });\n                this.pendingBackendInit = success;\n                return { success, asyncInit: true };\n            }\n            else {\n                this.registry[backendName] = backend;\n                return { success: true, asyncInit: false };\n            }\n        }\n        catch (err) {\n            console.warn(`Initialization of backend ${backendName} failed`);\n            console.warn(err.stack || err.message);\n            return { success: false, asyncInit: false };\n        }\n    }\n    removeBackend(backendName) {\n        if (!(backendName in this.registryFactory)) {\n            throw new Error(`${backendName} backend not found in registry`);\n        }\n        if (this.backendName === backendName && this.pendingBackendInit != null) {\n            // There is a pending promise of the backend we want to remove. Make it\n            // obsolete.\n            this.pendingBackendInitId++;\n        }\n        if (backendName in this.registry) {\n            this.disposeRegisteredKernels(backendName);\n            this.registry[backendName].dispose();\n            delete this.registry[backendName];\n        }\n        delete this.registryFactory[backendName];\n        // Unset the backend if it is active.\n        if (this.backendName === backendName) {\n            this.pendingBackendInit = null;\n            this.backendName = null;\n            this.backendInstance = null;\n        }\n    }\n    getSortedBackends() {\n        if (Object.keys(this.registryFactory).length === 0) {\n            throw new Error('No backend found in registry.');\n        }\n        return Object.keys(this.registryFactory).sort((a, b) => {\n            // Highest priority comes first.\n            return this.registryFactory[b].priority -\n                this.registryFactory[a].priority;\n        });\n    }\n    initializeBackendsAndReturnBest() {\n        const sortedBackends = this.getSortedBackends();\n        for (let i = 0; i < sortedBackends.length; i++) {\n            const backendName = sortedBackends[i];\n            const { success, asyncInit } = this.initializeBackend(backendName);\n            if (asyncInit || success) {\n                return { name: backendName, asyncInit };\n            }\n        }\n        throw new Error(`Could not initialize any backends, all backend initializations ` +\n            `failed.`);\n    }\n    moveData(backend, dataId) {\n        const info = this.state.tensorInfo.get(dataId);\n        const srcBackend = info.backend;\n        const values = this.readSync(dataId);\n        const refCount = srcBackend.refCount(dataId);\n        // Delete the tensor from the old backend and move it to the new\n        // backend.\n        srcBackend.disposeData(dataId, true);\n        info.backend = backend;\n        backend.move(dataId, values, info.shape, info.dtype, refCount);\n        if (this.shouldCheckForMemLeaks()) {\n            // Track the number of moves during a kernel execution to correctly\n            // detect memory leaks.\n            this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n        }\n    }\n    tidy(nameOrFn, fn) {\n        let name = null;\n        if (fn == null) {\n            // Called with only 1 argument.\n            if (typeof nameOrFn !== 'function') {\n                throw new Error('Please provide a function to tidy()');\n            }\n            fn = nameOrFn;\n        }\n        else {\n            // Called with 2 arguments.\n            if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n                throw new Error('When calling with two arguments, the first argument ' +\n                    'to tidy() must be a string');\n            }\n            if (typeof fn !== 'function') {\n                throw new Error('When calling with two arguments, the 2nd argument ' +\n                    'to tidy() must be a function');\n            }\n            name = nameOrFn;\n            // TODO(nsthorat,smilkov): Do operation logging and performance\n            // profiling.\n        }\n        let result;\n        return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {\n            result = fn();\n            if (result instanceof Promise) {\n                console.error('Cannot return a Promise inside of tidy.');\n            }\n            return result;\n        });\n    }\n    scopedRun(start, end, f) {\n        start();\n        try {\n            const res = f();\n            end();\n            return res;\n        }\n        catch (ex) {\n            end();\n            throw ex;\n        }\n    }\n    nextTensorId() {\n        return Engine.nextTensorId++;\n    }\n    nextVariableId() {\n        return Engine.nextVariableId++;\n    }\n    /**\n     * This method is called instead of the public-facing tensor.clone() when\n     * saving a tensor for backwards pass. It makes sure to add the clone\n     * operation to the tape regardless of being called inside a kernel\n     * execution.\n     */\n    clone(x) {\n        const y = ENGINE.runKernel(Identity, { x });\n        const inputs = { x };\n        const grad = (dy) => ({\n            x: () => {\n                const dtype = 'float32';\n                const gradInputs = { x: dy };\n                const attrs = { dtype };\n                return ENGINE.runKernel(Cast, gradInputs, \n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                attrs);\n            }\n        });\n        const saved = [];\n        this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n        return y;\n    }\n    /**\n     * Execute a kernel with the given name and return the output tensor.\n     *\n     * @param kernelName The name of the kernel to execute.\n     * @param inputs A map of input names to tensors.\n     * @param attrs A map of attribute names to their values. An attribute is a\n     *     primitive (non-tensor) input to the kernel.\n     * @param inputsToSave A list of tensors, inputs to save for the backprop\n     *     computation.\n     * @param outputsToSave A list of booleans, specifying which output to save\n     *     for the backprop computation. These are booleans since the output\n     * tensors are not visible to the user.\n     */\n    runKernel(kernelName, inputs, attrs) {\n        const hasKernel = getKernel(kernelName, this.backendName) != null;\n        if (!hasKernel) {\n            throw new Error(`Kernel '${kernelName}' not registered for backend '${this.backendName}'`);\n        }\n        return this.runKernelFunc({ kernelName, inputs, attrs });\n    }\n    shouldCheckForMemLeaks() {\n        return this.ENV.getBool('IS_TEST');\n    }\n    checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\n        const numDataIdsAfter = this.backend.numDataIds();\n        // Count the number of data ids associated with the result of the kernel.\n        let numOutputDataIds = 0;\n        outInfos.forEach(info => {\n            // Complex numbers allocate 3 data ids, one for 'real', one for\n            // 'imaginary', and one for the container that holds the former two.\n            numOutputDataIds += (info.dtype === 'complex64' ? 3 : 1);\n        });\n        // Account for the number of moves during kernel execution. A \"data move\"\n        // can happen in the middle of a kernel execution, placing a new (key,value)\n        // pair in the data storage. Since data moves have net zero effect (we\n        // always remove the data from the old backend), we have to cancel them out\n        // when detecting memory leaks.\n        const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n        const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n        if (dataIdsLeaked > 0) {\n            throw new Error(`Backend '${this.backendName}' has an internal memory leak ` +\n                `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n        }\n    }\n    /**\n     * Internal helper method to execute a kernel Func\n     *\n     * Use `runKernel` to execute kernels from outside of engine.\n     */\n    runKernelFunc(kernelParams) {\n        let outputs;\n        let saved = [];\n        const isTapeOn = this.isTapeOn();\n        const startingBytecount = this.state.numBytes;\n        const startingNumTensors = this.state.numTensors;\n        if (this.shouldCheckForMemLeaks()) {\n            this.state.numDataMovesStack.push(0);\n        }\n        let kernelFunc;\n        if (this.backendName == null) {\n            // backend has not been initialized yet (backend initialization is lazy\n            // can be deferred until an op/ kernel is run).\n            // The below getter has side effects that will try to initialize the\n            // backend and set properties like this.backendName\n            // tslint:disable-next-line: no-unused-expression\n            this.backend;\n        }\n        let out;\n        const kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ?\n            kernelParams.kernelName :\n            this.state.activeScope != null ? this.state.activeScope.name : '';\n        // Create the kernelFunc from either a registered kernel OR passed in\n        // forward/backward functions (used by custom grad). In this context a\n        // kernelFunc wraps a kernel implementation with some bookkeeping.\n        if (isRegisteredKernelInvocation(kernelParams)) {\n            const { kernelName, inputs, attrs } = kernelParams;\n            if (this.backendName == null) {\n                // backend has not been initialized yet (backend initialization is lazy\n                // can be deferred until an op/ kernel is run).\n                // The below getter has side effects that will try to initialize the\n                // backend and set properties like this.backendName\n                // tslint:disable-next-line: no-unused-expression\n                this.backend;\n            }\n            const kernel = getKernel(kernelName, this.backendName);\n            util.assert(kernel != null, () => `Cannot find registered kernel '${kernelName}' for backend '${this.backendName}'`);\n            kernelFunc = () => {\n                const numDataIdsBefore = this.backend.numDataIds();\n                out = kernel.kernelFunc({ inputs, attrs, backend: this.backend });\n                const outInfos = Array.isArray(out) ? out : [out];\n                if (this.shouldCheckForMemLeaks()) {\n                    this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n                }\n                const outTensors = outInfos.map((outInfo) => {\n                    // todo (yassogba) remove this option (Tensor) when node backend\n                    // methods have been modularized and they all return tensorInfo.\n                    // TensorInfos do not have a rank attribute.\n                    if (outInfo.rank != null) {\n                        return outInfo;\n                    }\n                    const { dataId, shape, dtype } = outInfo;\n                    return this.makeTensorFromDataId(dataId, shape, dtype);\n                });\n                // Save any required inputs and outputs.\n                // Do not save unless we are recording to the tape. Otherwise it would\n                // cause a mem leak since there would be no backprop for these tensors\n                // (which would otherwise dispose them).\n                if (isTapeOn) {\n                    const tensorsToSave = this.getTensorsForGradient(kernelName, inputs, outTensors);\n                    saved = this.saveTensorsForBackwardMode(tensorsToSave);\n                }\n                return outTensors;\n            };\n        }\n        else {\n            const { forwardFunc } = kernelParams;\n            // Running a customGrad op.\n            const saveFunc = (tensors) => {\n                // Do not save unless we are recording to the tape. Otherwise it would\n                // cause a mem leak since we would never run backprop, which disposes\n                // the kept tensors.\n                if (!isTapeOn) {\n                    return;\n                }\n                saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n            };\n            kernelFunc = () => {\n                const numDataIdsBefore = this.backend.numDataIds();\n                out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n                const outs = (Array.isArray(out) ? out : [out]);\n                if (this.shouldCheckForMemLeaks()) {\n                    // Scope name is used to print a more helpful error message if needed.\n                    this.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);\n                }\n                return outs;\n            };\n        }\n        //\n        // Run the kernelFunc. Optionally profiling it.\n        //\n        const { inputs, attrs } = kernelParams;\n        const backwardsFunc = isRegisteredKernelInvocation(kernelParams) ?\n            null :\n            kernelParams.backwardsFunc;\n        let kernelProfile;\n        this.scopedRun(\n        // Stop recording to a tape when running a kernel.\n        () => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n            if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n                outputs = kernelFunc();\n            }\n            else {\n                kernelProfile = this.profiler.profileKernel(kernelOrScopeName, inputs, () => kernelFunc());\n                if (this.ENV.getBool('DEBUG')) {\n                    this.profiler.logKernelProfile(kernelProfile);\n                }\n                outputs = kernelProfile.outputs;\n            }\n        });\n        if (isTapeOn) {\n            this.addTapeNode(kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);\n        }\n        if (this.state.profiling) {\n            this.state.activeProfile.kernels.push({\n                name: kernelOrScopeName,\n                bytesAdded: this.state.numBytes - startingBytecount,\n                totalBytesSnapshot: this.state.numBytes,\n                tensorsAdded: this.state.numTensors - startingNumTensors,\n                totalTensorsSnapshot: this.state.numTensors,\n                inputShapes: Object.keys(inputs).map(key => inputs[key] != null ? inputs[key].shape : null),\n                outputShapes: outputs.map(item => item.shape),\n                kernelTimeMs: kernelProfile.timeMs,\n                extraInfo: kernelProfile.extraInfo\n            });\n        }\n        return (Array.isArray(out) ? outputs : outputs[0]);\n    }\n    /**\n     * Saves tensors used in forward mode for use in backward mode.\n     *\n     * @param tensors the list of tensors to save.\n     */\n    saveTensorsForBackwardMode(tensors) {\n        const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n        return saved;\n    }\n    /**\n     * Returns a list of tensors to save for a given gradient calculation.\n     *\n     * @param kernelName name of kernel to look up gradient for.\n     * @param inputs a map of input tensors.\n     * @param outputs an array of output tensors from forward mode of kernel.\n     */\n    getTensorsForGradient(kernelName, inputs, outputs) {\n        const gradConfig = getGradient(kernelName);\n        if (gradConfig != null) {\n            const inputsToSave = gradConfig.inputsToSave || [];\n            const outputsToSave = gradConfig.outputsToSave || [];\n            // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n            // specified in inputsToSave will be saved.\n            let inputTensorsToSave;\n            if (gradConfig.saveAllInputs) {\n                util.assert(Array.isArray(inputs), () => 'saveAllInputs is true, expected inputs to be an array.');\n                inputTensorsToSave = Object.keys(inputs).map((key) => inputs[key]);\n            }\n            else {\n                inputTensorsToSave = inputsToSave.map((inputName) => inputs[inputName]);\n            }\n            const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);\n            return inputTensorsToSave.concat(outputTensorsToSave);\n        }\n        // We return an empty list rather than throw an error because the kernel we\n        // are looking up may not actually be relevant to backproping through the\n        // overall function\n        //\n        // See 'does not error if irrelevant (pruned) ops are missing grads' test\n        // in gradients_test.ts for an example.\n        return [];\n    }\n    /**\n     * Internal method used by public APIs for tensor creation. Makes a new\n     * tensor with the provided shape, dtype and values. It always\n     * creates a new data id and writes the values to the underlying backend.\n     */\n    makeTensor(values, shape, dtype, backend) {\n        if (values == null) {\n            throw new Error('Values passed to engine.makeTensor() are null');\n        }\n        dtype = dtype || 'float32';\n        backend = backend || this.backend;\n        let backendVals = values;\n        if (dtype === 'string' && util.isString(values[0])) {\n            backendVals = values.map(d => util.encodeString(d));\n        }\n        const dataId = backend.write(backendVals, shape, dtype);\n        const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n        this.trackTensor(t, backend);\n        // Count bytes for string tensors.\n        if (dtype === 'string') {\n            const info = this.state.tensorInfo.get(dataId);\n            const newBytes = bytesFromStringArray(backendVals);\n            this.state.numBytes += newBytes - info.bytes;\n            info.bytes = newBytes;\n        }\n        return t;\n    }\n    /**\n     * Internal method used by backends. Makes a new tensor\n     * that is a wrapper around an existing data id. It doesn't create\n     * a new data id, only increments the ref count used in memory tracking.\n     */\n    makeTensorFromDataId(dataId, shape, dtype, backend) {\n        dtype = dtype || 'float32';\n        const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n        this.trackTensor(t, backend);\n        return t;\n    }\n    makeVariable(initialValue, trainable = true, name, dtype) {\n        name = name || this.nextVariableId().toString();\n        if (dtype != null && dtype !== initialValue.dtype) {\n            initialValue = initialValue.cast(dtype);\n        }\n        const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n        if (this.state.registeredVariables[v.name] != null) {\n            throw new Error(`Variable with name ${v.name} was already registered`);\n        }\n        this.state.registeredVariables[v.name] = v;\n        this.incRef(v, this.backend);\n        return v;\n    }\n    trackTensor(a, backend) {\n        this.state.numTensors++;\n        if (a.dtype === 'string') {\n            this.state.numStringTensors++;\n        }\n        // Bytes for complex numbers are counted by their components. Bytes for\n        // string tensors are counted when writing values.\n        let bytes = 0;\n        if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n            bytes = a.size * util.bytesPerElement(a.dtype);\n        }\n        this.state.numBytes += bytes;\n        if (!this.state.tensorInfo.has(a.dataId)) {\n            this.state.numDataBuffers++;\n            this.state.tensorInfo.set(a.dataId, {\n                backend: backend || this.backend,\n                dtype: a.dtype,\n                shape: a.shape,\n                bytes\n            });\n        }\n        if (!(a instanceof Variable)) {\n            this.track(a);\n        }\n    }\n    // Track the tensor by dataId and increase the refCount for the dataId in the\n    // backend.\n    // TODO(pyu10055): This is currently used by makeVariable method, to increase\n    // refCount on the backend for the dataId. It can potentially be replaced with\n    // Identity op indead of calling backend directly.\n    incRef(a, backend) {\n        this.trackTensor(a, backend);\n        this.backend.incRef(a.dataId);\n    }\n    removeDataId(dataId, backend) {\n        if (this.state.tensorInfo.has(dataId) &&\n            this.state.tensorInfo.get(dataId).backend === backend) {\n            this.state.tensorInfo.delete(dataId);\n            this.state.numDataBuffers--;\n        }\n    }\n    disposeTensor(a) {\n        if (!this.state.tensorInfo.has(a.dataId)) {\n            return;\n        }\n        const info = this.state.tensorInfo.get(a.dataId);\n        this.state.numTensors--;\n        if (a.dtype === 'string') {\n            this.state.numStringTensors--;\n            this.state.numBytes -= info.bytes;\n        }\n        // Don't count bytes for complex numbers as they are counted by their\n        // components.\n        if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n            const bytes = a.size * util.bytesPerElement(a.dtype);\n            this.state.numBytes -= bytes;\n        }\n        // Remove the reference to dataId if backend dispose the data successfully\n        if (info.backend.disposeData(a.dataId)) {\n            this.removeDataId(a.dataId, info.backend);\n        }\n        // TODO(nsthorat): Construct an error and save the stack trace for\n        // debugging when in debug mode. Creating a stack trace is too expensive\n        // to do unconditionally.\n    }\n    disposeVariables() {\n        for (const varName in this.state.registeredVariables) {\n            const v = this.state.registeredVariables[varName];\n            this.disposeVariable(v);\n        }\n    }\n    disposeVariable(v) {\n        this.disposeTensor(v);\n        if (this.state.registeredVariables[v.name] != null) {\n            delete this.state.registeredVariables[v.name];\n        }\n    }\n    memory() {\n        const info = this.backend.memory();\n        info.numTensors = this.state.numTensors;\n        info.numDataBuffers = this.state.numDataBuffers;\n        info.numBytes = this.state.numBytes;\n        if (this.state.numStringTensors > 0) {\n            info.unreliable = true;\n            if (info.reasons == null) {\n                info.reasons = [];\n            }\n            info.reasons.push('Memory usage by string tensors is approximate ' +\n                '(2 bytes per character)');\n        }\n        return info;\n    }\n    async profile(query) {\n        this.state.profiling = true;\n        const startBytes = this.state.numBytes;\n        const startNumTensors = this.state.numTensors;\n        this.state.activeProfile.kernels = [];\n        this.state.activeProfile.result = await query();\n        this.state.profiling = false;\n        this.state.activeProfile.peakBytes = Math.max(...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n        this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n        this.state.activeProfile.newTensors =\n            this.state.numTensors - startNumTensors;\n        for (const kernel of this.state.activeProfile.kernels) {\n            kernel.kernelTimeMs = await kernel.kernelTimeMs;\n            kernel.extraInfo = await kernel.extraInfo;\n        }\n        return this.state.activeProfile;\n    }\n    isTapeOn() {\n        return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n    }\n    addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\n        const tapeNode = { id: this.state.nextTapeNodeId++, kernelName, inputs, outputs, saved };\n        const gradConfig = getGradient(kernelName);\n        if (gradConfig != null) {\n            gradientsFunc = gradConfig.gradFunc;\n        }\n        if (gradientsFunc != null) {\n            tapeNode.gradient = (dys) => {\n                // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n                // the backprop graph to the user as null instead of zeros\n                dys = dys.map((dy, i) => {\n                    if (dy == null) {\n                        const output = outputs[i];\n                        const vals = util.makeZerosTypedArray(output.size, output.dtype);\n                        return this.makeTensor(vals, output.shape, output.dtype);\n                    }\n                    return dy;\n                });\n                // Grad functions of ops with single outputs expect a dy, while ops\n                // with multiple outputs expect dys (array of dy).\n                return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n            };\n        }\n        this.state.activeTape.push(tapeNode);\n    }\n    keep(result) {\n        result.kept = true;\n        return result;\n    }\n    startTape() {\n        if (this.state.gradientDepth === 0) {\n            this.state.activeTape = [];\n        }\n        this.state.gradientDepth++;\n    }\n    endTape() {\n        this.state.gradientDepth--;\n    }\n    /**\n     * Start a scope. Use this with endScope() to achieve the same functionality\n     * as scope() without the need for a function closure.\n     */\n    startScope(name) {\n        const scopeInfo = {\n            track: [],\n            name: 'unnamed scope',\n            id: this.state.nextScopeId++\n        };\n        if (name) {\n            scopeInfo.name = name;\n        }\n        this.state.scopeStack.push(scopeInfo);\n        this.state.activeScope = scopeInfo;\n    }\n    /**\n     * End a scope. Use this with startScope() to achieve the same functionality\n     * as scope() without the need for a function closure.\n     */\n    endScope(result) {\n        const tensorsToTrackInParent = getTensorsInContainer(result);\n        const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(t => t.id));\n        // Dispose the arrays tracked in this scope.\n        for (let i = 0; i < this.state.activeScope.track.length; i++) {\n            const tensor = this.state.activeScope.track[i];\n            if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n                tensor.dispose();\n            }\n        }\n        const oldScope = this.state.scopeStack.pop();\n        this.state.activeScope = this.state.scopeStack.length === 0 ?\n            null :\n            this.state.scopeStack[this.state.scopeStack.length - 1];\n        // Track the current result in the parent scope.\n        tensorsToTrackInParent.forEach(tensor => {\n            // Only track the tensor if was allocated in the inner scope and is not\n            // globally kept.\n            if (!tensor.kept && tensor.scopeId === oldScope.id) {\n                this.track(tensor);\n            }\n        });\n    }\n    /**\n     * Returns gradients of `f` with respect to each of the `xs`. The gradients\n     * returned are of the same length as `xs`, but some might be null if `f`\n     * was not a function of that `x`. It also takes optional dy to multiply the\n     * gradient, which defaults to `1`.\n     */\n    gradients(f, xs, dy, allowNoGradients = false) {\n        util.assert(xs.length > 0, () => 'gradients() received an empty list of xs.');\n        if (dy != null && dy.dtype !== 'float32') {\n            throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n        }\n        const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy('forward', f));\n        util.assert(y instanceof Tensor, () => 'The result y returned by f() must be a tensor.');\n        // Filter out the nodes that don't connect x => y.\n        const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n        if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n            throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' +\n                'that the f you passed encloses all operations that lead from x ' +\n                'to y.');\n        }\n        return this.tidy('backward', () => {\n            const accumulatedGradientMap = {};\n            accumulatedGradientMap[y.id] = (dy == null) ? ones(y.shape) : dy;\n            // Backprop gradients through the filtered nodes.\n            backpropagateGradients(accumulatedGradientMap, filteredTape, \n            // Pass the tidy function to avoid circular dep with `tape.ts`.\n            f => this.tidy(f), \n            // Pass an add function to avoide a circular dep with `tape.ts`.\n            add);\n            const grads = xs.map(x => accumulatedGradientMap[x.id]);\n            if (this.state.gradientDepth === 0) {\n                // This means that we are not computing higher-order gradients\n                // and can clean up the tape.\n                this.state.activeTape.forEach(node => {\n                    for (const tensor of node.saved) {\n                        tensor.dispose();\n                    }\n                });\n                this.state.activeTape = null;\n            }\n            return { value: y, grads };\n        });\n    }\n    customGrad(f) {\n        util.assert(util.isFunction(f), () => 'The f passed in customGrad(f) must be a function.');\n        return (...inputs) => {\n            util.assert(inputs.every(t => t instanceof Tensor), () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' +\n                'tensors');\n            let res;\n            const inputMap = {};\n            inputs.forEach((input, i) => {\n                inputMap[i] = input;\n            });\n            const forwardFunc = (_, save) => {\n                res = f(...[...inputs, save]);\n                util.assert(res.value instanceof Tensor, () => 'The function f passed in customGrad(f) must return an ' +\n                    'object where `obj.value` is a tensor');\n                util.assert(util.isFunction(res.gradFunc), () => 'The function f passed in customGrad(f) must return an ' +\n                    'object where `obj.gradFunc` is a function.');\n                return res.value;\n            };\n            const backwardsFunc = (dy, saved) => {\n                const gradRes = res.gradFunc(dy, saved);\n                const grads = Array.isArray(gradRes) ? gradRes : [gradRes];\n                util.assert(grads.length === inputs.length, () => 'The function f passed in customGrad(f) must return an ' +\n                    'object where `obj.gradFunc` is a function that returns ' +\n                    'the same number of tensors as inputs passed to f(...).');\n                util.assert(grads.every(t => t instanceof Tensor), () => 'The function f passed in customGrad(f) must return an ' +\n                    'object where `obj.gradFunc` is a function that returns ' +\n                    'a list of only tensors.');\n                const gradMap = {};\n                grads.forEach((grad, i) => {\n                    gradMap[i] = () => grad;\n                });\n                return gradMap;\n            };\n            return this.runKernelFunc({\n                forwardFunc,\n                backwardsFunc,\n                inputs: inputMap,\n            });\n        };\n    }\n    readSync(dataId) {\n        // Route the read to the correct backend.\n        const info = this.state.tensorInfo.get(dataId);\n        return info.backend.readSync(dataId);\n    }\n    read(dataId) {\n        // Route the read to the correct backend.\n        const info = this.state.tensorInfo.get(dataId);\n        return info.backend.read(dataId);\n    }\n    async time(query) {\n        const start = now();\n        const timingInfo = await this.backend.time(query);\n        timingInfo.wallMs = now() - start;\n        return timingInfo;\n    }\n    /**\n     * Tracks a Tensor in the current scope to be automatically cleaned up\n     * when the current scope ends, and returns the value.\n     *\n     * @param result The Tensor to track in the current scope.\n     */\n    track(result) {\n        if (this.state.activeScope != null) {\n            result.scopeId = this.state.activeScope.id;\n            this.state.activeScope.track.push(result);\n        }\n        return result;\n    }\n    get registeredVariables() {\n        return this.state.registeredVariables;\n    }\n    /**\n     * Resets the engine state. Removes all backends but does not remove\n     * registered backend factories.\n     */\n    reset() {\n        // Make any pending promise obsolete.\n        this.pendingBackendInitId++;\n        this.state.dispose();\n        this.ENV.reset();\n        this.state = new EngineState();\n        for (const backendName in this.registry) {\n            this.disposeRegisteredKernels(backendName);\n            this.registry[backendName].dispose();\n            delete this.registry[backendName];\n        }\n        this.backendName = null;\n        this.backendInstance = null;\n        this.pendingBackendInit = null;\n    }\n}\nEngine.nextTensorId = 0;\nEngine.nextVariableId = 0;\nfunction ones(shape) {\n    const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n    return ENGINE.makeTensor(values, shape, 'float32');\n}\nexport function getOrMakeEngine() {\n    const ns = getGlobalNamespace();\n    if (ns._tfengine == null) {\n        const environment = new Environment(ns);\n        ns._tfengine = new Engine(environment);\n    }\n    setEnvironmentGlobal(ns._tfengine.ENV);\n    // Tell the current tensor interface that the global engine is responsible\n    // for tracking.\n    setTensorTracker(() => ns._tfengine);\n    return ns._tfengine;\n}\nexport const ENGINE = getOrMakeEngine();\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\nexport function add(a, b) {\n    // We duplicate Add here to avoid a circular dependency with add.ts.\n    const inputs = { a, b };\n    return ENGINE.runKernel(Add, inputs);\n}\n"]},"metadata":{},"sourceType":"module"}