{"ast":null,"code":"import _asyncToGenerator from \"/Users/ryanliang/Downloads/main_movir_picker/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport { iteratorFromConcatenated, iteratorFromFunction, iteratorFromItems, iteratorFromZipped, ZipMismatchMode } from './iterators/lazy_iterator';\nimport { canTensorify, deepMapAndAwaitAll, isIterable } from './util/deep_map'; // TODO(soergel): consider vectorized operations within the pipeline.\n\n/**\n * Represents a potentially large list of independent data elements (typically\n * 'samples' or 'examples').\n *\n * A 'data example' may be a primitive, an array, a map from string keys to\n * values, or any nested structure of these.\n *\n * A `Dataset` represents an ordered collection of elements, together with a\n * chain of transformations to be performed on those elements. Each\n * transformation is a method of `Dataset` that returns another `Dataset`, so\n * these may be chained, e.g.\n * `const processedDataset = rawDataset.filter(...).map(...).batch(...)`.\n *\n * Data loading and transformation is done in a lazy, streaming fashion.  The\n * dataset may be iterated over multiple times; each iteration starts the data\n * loading anew and recapitulates the transformations.\n *\n * A `Dataset` is typically processed as a stream of unbatched examples --i.e.,\n * its transformations are applied one example at a time. Batching produces a\n * new `Dataset` where each element is a batch. Batching should usually come\n * last in a pipeline, because data transformations are easier to express on a\n * per-example basis than on a per-batch basis.\n *\n * The following code examples are calling `await dataset.forEachAsync(...)` to\n * iterate once over the entire dataset in order to print out the data.\n *\n * @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'}\n */\n\nexport class Dataset {\n  constructor() {\n    this.size = null;\n  } // TODO(soergel): Make Datasets report whether repeated iterator() calls\n  // produce the same result (e.g., reading from a file) or different results\n  // (e.g., from the webcam).  Currently we don't make this distinction but it\n  // could be important for the user to know.\n  // abstract isDeterministic(): boolean;\n\n  /**\n   * Groups elements into batches.\n   *\n   * It is assumed that each of the incoming dataset elements has the same\n   * structure-- i.e. the same set of keys at each location in an object\n   * hierarchy.  For each key, the resulting `Dataset` provides a batched\n   * element collecting all of the incoming values for that key.\n   *\n   *  * Incoming primitives are grouped into a 1-D Tensor.\n   *  * Incoming Tensors are grouped into a new Tensor where the 0'th axis is\n   *    the batch dimension.\n   *  * Incoming arrays are converted to Tensor and then batched.\n   *  * A nested array is interpreted as an n-D Tensor, so the batched result\n   *    has n+1 dimensions.\n   *  * An array that cannot be converted to Tensor produces an error.\n   *\n   * If an array should not be batched as a unit, it should first be converted\n   * to an object with integer keys.\n   *\n   * Here are a few examples:\n   *\n   * Batch a dataset of numbers:\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);\n   * await a.forEachAsync(e => e.print());\n   * ```\n   *\n   * Batch a dataset of arrays:\n   * ```js\n   * const b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);\n   * await b.forEachAsync(e => e.print());\n   * ```\n   *\n   * Batch a dataset of objects:\n   * ```js\n   * const c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},\n   *   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},\n   *   {a: 8, b: 18}]).batch(4);\n   * await c.forEachAsync(e => {\n   *   console.log('{');\n   *   for(var key in e) {\n   *     console.log(key+':');\n   *     e[key].print();\n   *   }\n   *   console.log('}');\n   * })\n   * ```\n   *\n   * @param batchSize The number of elements desired per batch.\n   * @param smallLastBatch Whether to emit the final batch when it has fewer\n   *   than batchSize elements. Default true.\n   * @returns A `Dataset`, from which a stream of batches can be obtained.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  batch(batchSize, smallLastBatch = true) {\n    const base = this;\n    tf.util.assert(batchSize > 0, () => `batchSize needs to be positive, but it is\n      ${batchSize}`);\n    let size;\n\n    if (this.size === Infinity || this.size == null) {\n      // If the size of this dataset is infinity or null, the new size keeps the\n      // same.\n      size = this.size;\n    } else if (smallLastBatch) {\n      // If the size of this dataset is known and include small last batch, the\n      // new size is full batch count plus last batch.\n      size = Math.ceil(this.size / batchSize);\n    } else {\n      // If the size of this dataset is known and not include small last batch,\n      // the new size is full batch count.\n      size = Math.floor(this.size / batchSize);\n    }\n\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield base.iterator()).columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat);\n    }), size);\n  }\n  /**\n   * Concatenates this `Dataset` with another.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]);\n   * const b = tf.data.array([4, 5, 6]);\n   * const c = a.concatenate(b);\n   * await c.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param dataset A `Dataset` to be concatenated onto this one.\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  concatenate(dataset) {\n    const base = this;\n    let size;\n\n    if (this.size === Infinity || dataset.size === Infinity) {\n      // If the size of any of these two dataset is infinity, new size is\n      // infinity.\n      size = Infinity;\n    } else if (this.size != null && dataset.size != null) {\n      // If the size of both datasets are known and not infinity, new size is\n      // sum the size of these two datasets.\n      size = this.size + dataset.size;\n    } else {\n      // If neither of these two datasets has infinite size and any of these two\n      // datasets' size is null, the new size is null.\n      size = null;\n    }\n\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield base.iterator()).concatenate(yield dataset.iterator());\n    }), size);\n  }\n  /**\n   * Filters this dataset according to `predicate`.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n   *   .filter(x => x%2 === 0);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param predicate A function mapping a dataset element to a boolean or a\n   * `Promise` for one.\n   *\n   * @returns A `Dataset` of elements for which the predicate was true.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  filter(predicate) {\n    const base = this;\n    let size;\n\n    if (this.size === Infinity) {\n      // If the size of this dataset is infinity, new size is infinity\n      size = Infinity;\n    } else {\n      // If this dataset has limited elements, new size is null because it might\n      // exhausted randomly.\n      size = null;\n    }\n\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield base.iterator()).filter(x => tf.tidy(() => predicate(x)));\n    }), size);\n  }\n  /**\n   * Apply a function to every element of the dataset.\n   *\n   * After the function is applied to a dataset element, any Tensors contained\n   * within that element are disposed.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param f A function to apply to each dataset element.\n   * @returns A `Promise` that resolves after all elements have been processed.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  forEachAsync(f) {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      return (yield _this.iterator()).forEachAsync(f);\n    })();\n  }\n  /**\n   * Maps this dataset through a 1-to-1 transform.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]).map(x => x*x);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param transform A function mapping a dataset element to a transformed\n   *   dataset element.\n   *\n   * @returns A `Dataset` of transformed elements.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  map(transform) {\n    const base = this;\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield base.iterator()).map(x => tf.tidy(() => transform(x)));\n    }), this.size);\n  }\n  /**\n   * Maps this dataset through an async 1-to-1 transform.\n   *\n   * ```js\n   * const a =\n   *  tf.data.array([1, 2, 3]).mapAsync(x => new Promise(function(resolve){\n   *    setTimeout(() => {\n   *      resolve(x * x);\n   *    }, Math.random()*1000 + 500);\n   *  }));\n   * console.log(await a.toArray());\n   * ```\n   *\n   * @param transform A function mapping a dataset element to a `Promise` for a\n   *   transformed dataset element.  This transform is responsible for disposing\n   *   any intermediate `Tensor`s, i.e. by wrapping its computation in\n   *   `tf.tidy()`; that cannot be automated here (as it is in the synchronous\n   *   `map()` case).\n   *\n   * @returns A `Dataset` of transformed elements.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  mapAsync(transform) {\n    const base = this;\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield base.iterator()).mapAsync(transform);\n    }), this.size);\n  }\n  /**\n   *  Creates a `Dataset` that prefetches elements from this dataset.\n   *\n   * @param bufferSize: An integer specifying the number of elements to be\n   *   prefetched.\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  prefetch(bufferSize) {\n    if (bufferSize == null) {\n      throw new RangeError('`Dataset.prefetch()` requires bufferSize to be specified.');\n    }\n\n    const base = this;\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield base.iterator()).prefetch(bufferSize);\n    }), this.size);\n  }\n  /**\n   * Repeats this dataset `count` times.\n   *\n   * NOTE: If this dataset is a function of global state (e.g. a random number\n   * generator), then different repetitions may produce different elements.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]).repeat(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: (Optional) An integer, representing the number of times\n   *   the dataset should be repeated. The default behavior (if `count` is\n   *   `undefined` or negative) is for the dataset be repeated indefinitely.\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  repeat(count) {\n    const base = this;\n    let size;\n\n    if (this.size != null && count > 0) {\n      // If this dataset has size and count is positive, new size is current\n      // size multiply count. This also covers the case that current size is\n      // infinity.\n      size = this.size * count;\n    } else if (count === 0) {\n      // If count is 0, new size is 0.\n      size = 0;\n    } else if (this.size != null && (count === undefined || count < 0)) {\n      // If this dataset has size and count is undefined or negative, the\n      // dataset will be repeated indefinitely and new size is infinity.\n      size = Infinity;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      const iteratorIterator = iteratorFromFunction( /*#__PURE__*/_asyncToGenerator(function* () {\n        return {\n          value: yield base.iterator(),\n          done: false\n        };\n      }));\n      return iteratorFromConcatenated(iteratorIterator.take(count));\n    }), size);\n  }\n  /**\n   * Creates a `Dataset` that skips `count` initial elements from this dataset.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: The number of elements of this dataset that should be skipped\n   *   to form the new dataset.  If `count` is greater than the size of this\n   *   dataset, the new dataset will contain no elements.  If `count`\n   *   is `undefined` or negative, skips the entire dataset.\n   *\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  skip(count) {\n    const base = this;\n    let size;\n\n    if (this.size != null && count >= 0 && this.size >= count) {\n      // If the size of this dataset is greater than count, the new dataset's\n      // size is current size minus skipped size.This also covers the case that\n      // current size is infinity.\n      size = this.size - count;\n    } else if (this.size != null && (this.size < count || count === undefined || count < 0)) {\n      // If the size of this dataset is smaller than count, or count is\n      // undefined or negative, skips the entire dataset and the new size is 0.\n      size = 0;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield base.iterator()).skip(count);\n    }), size);\n  }\n  /**\n   * Pseudorandomly shuffles the elements of this dataset. This is done in a\n   * streaming manner, by sampling from a given number of prefetched elements.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param bufferSize: An integer specifying the number of elements from this\n   *   dataset from which the new dataset will sample.\n   * @param seed: (Optional) An integer specifying the random seed that will\n   *   be used to create the distribution.\n   * @param reshuffleEachIteration: (Optional) A boolean, which if true\n   *   indicates that the dataset should be pseudorandomly reshuffled each time\n   *   it is iterated over. If false, elements will be returned in the same\n   *   shuffled order on each iteration. (Defaults to `true`.)\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  shuffle(bufferSize, seed, reshuffleEachIteration = true) {\n    if (bufferSize == null || bufferSize < 0) {\n      if (this.size == null) {\n        throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.');\n      } else {\n        throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.  ' + 'If your data fits in main memory (for regular JS objects), ' + 'and/or GPU memory (for `tf.Tensor`s), consider setting ' + `bufferSize to the dataset size (${this.size} elements)`);\n      }\n    }\n\n    const base = this;\n    const random = seedrandom.alea(seed || tf.util.now().toString());\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      let seed2 = random.int32();\n\n      if (reshuffleEachIteration) {\n        seed2 += random.int32();\n      }\n\n      return (yield base.iterator()).shuffle(bufferSize, seed2.toString());\n    }), this.size);\n  }\n  /**\n   * Creates a `Dataset` with at most `count` initial elements from this\n   * dataset.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: The number of elements of this dataset that should be taken\n   *   to form the new dataset.  If `count` is `undefined` or negative, or if\n   *   `count` is greater than the size of this dataset, the new dataset will\n   *   contain all elements of this dataset.\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  take(count) {\n    const base = this;\n    let size;\n\n    if (this.size != null && this.size > count) {\n      // If the size of this dataset is greater than count, the new dataset's\n      // size is count.\n      size = count;\n    } else if (this.size != null && this.size <= count) {\n      // If the size of this dataset is equal or smaller than count, the new\n      // dataset's size is the size of this dataset.\n      size = this.size;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield base.iterator()).take(count);\n    }), size);\n  }\n  /**\n   * Collect all elements of this dataset into an array.\n   *\n   * Obviously this will succeed only for small datasets that fit in memory.\n   * Useful for testing and generally should be avoided if possible.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]);\n   * console.log(await a.toArray());\n   * ```\n   *\n   * @returns A Promise for an array of elements, which will resolve\n   *   when a new stream has been obtained and fully consumed.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n\n\n  toArray() {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this2.size === Infinity) {\n        throw new Error('Can not convert infinite data stream to array.');\n      }\n\n      return (yield _this2.iterator()).toArray();\n    })();\n  }\n  /**\n   * Collect all elements of this dataset into an array with prefetching 100\n   * elements. This is useful for testing, because the prefetch changes the\n   * order in which the Promises are resolved along the processing pipeline.\n   * This may help expose bugs where results are dependent on the order of\n   * Promise resolution rather than on the logical order of the stream (i.e.,\n   * due to hidden mutable state).\n   *\n   * @returns A Promise for an array of elements, which will resolve\n   *   when a new stream has been obtained and fully consumed.\n   */\n\n\n  toArrayForTest() {\n    var _this3 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this3.size === Infinity) {\n        throw new Error('Can not convert infinite data stream to array.');\n      }\n\n      return (yield _this3.iterator()).toArrayForTest();\n    })();\n  }\n\n} // TODO(soergel): deep sharded shuffle, where supported\n\nDataset.MAX_BUFFER_SIZE = 10000;\n/**\n * Create a `Dataset` defined by a provided iterator() function.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const iter = tf.data.iteratorFromFunction(func);\n * const ds = tf.data.datasetFromIteratorFn(iter);\n * await ds.forEachAsync(e => console.log(e));\n * ```\n */\n\nexport function datasetFromIteratorFn(iteratorFn, size = null) {\n  return new class extends Dataset {\n    constructor() {\n      super(...arguments);\n      this.size = size;\n    }\n    /*\n     * Provide a new stream of elements.  Note this will also start new streams\n     * from any underlying `Dataset`s.\n     */\n\n\n    iterator() {\n      return _asyncToGenerator(function* () {\n        return iteratorFn();\n      })();\n    }\n\n  }();\n}\n/**\n * Create a `Dataset` from an array of elements.\n *\n * Create a Dataset from an array of objects:\n * ```js\n * const a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n *\n * Create a Dataset from an array of numbers:\n * ```js\n * const a = tf.data.array([4, 5, 6]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n * @param items An array of elements that will be parsed as items in a dataset.\n *\n * @doc {heading: 'Data', subheading: 'Creation', namespace: 'data'}\n */\n\nexport function array(items) {\n  return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n    return iteratorFromItems(items);\n  }), items.length);\n}\n/**\n * Create a `Dataset` by zipping together an array, dict, or nested\n * structure of `Dataset`s (and perhaps additional constants).\n * The underlying datasets must provide elements in a consistent order such that\n * they correspond.\n *\n * The number of elements in the resulting dataset is the same as the size of\n * the smallest dataset in datasets.\n *\n * The nested structure of the `datasets` argument determines the\n * structure of elements in the resulting iterator.\n *\n * Note this means that, given an array of two datasets that produce dict\n * elements, the result is a dataset that produces elements that are arrays\n * of two dicts:\n *\n * Zip an array of datasets:\n * ```js\n * console.log('Zip two datasets of objects:');\n * const ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const ds3 = tf.data.zip([ds1, ds2]);\n * await ds3.forEachAsync(e => console.log(JSON.stringify(e)));\n *\n * // If the goal is to merge the dicts in order to produce elements like\n * // {a: ..., b: ...}, this requires a second step such as:\n * console.log('Merge the objects:');\n * const ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\n * await ds4.forEachAsync(e => console.log(e));\n * ```\n *\n * Zip a dict of datasets:\n * ```js\n * const a = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const b = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const c = tf.data.zip({c: a, d: b});\n * await c.forEachAsync(e => console.log(JSON.stringify(e)));\n * ```\n *\n * @doc {heading: 'Data', subheading: 'Operations', namespace: 'data'}\n */\n\nexport function zip(datasets) {\n  // manually type-check the argument for JS users\n  if (!isIterable(datasets)) {\n    throw new Error('The argument to zip() must be an object or array.');\n  }\n\n  let size;\n\n  if (Array.isArray(datasets)) {\n    for (let i = 0; i < datasets.length; i++) {\n      size = size == null ? datasets[i].size : Math.min(size, datasets[i].size);\n    }\n  } else if (datasets instanceof Object) {\n    for (const ds in datasets) {\n      size = size == null ? datasets[ds].size : Math.min(size, datasets[ds].size);\n    }\n  }\n\n  return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n    const streams = yield deepMapAndAwaitAll(datasets, d => {\n      if (d instanceof Dataset) {\n        return {\n          value: d.iterator(),\n          recurse: false\n        };\n      } else if (isIterable(d)) {\n        return {\n          value: null,\n          recurse: true\n        };\n      } else {\n        throw new Error('Leaves of the structure passed to zip() must be Datasets, ' + 'not primitives.');\n      }\n    });\n    return iteratorFromZipped(streams, ZipMismatchMode.SHORTEST);\n  }), size);\n}\n/**\n * A zip function for use with deepZip, passed via the columnMajorBatch call.\n *\n * Accepts an array of identically-structured nested elements and either batches\n * them (if they are primitives, numeric arrays, or Tensors) or requests\n * recursion (if not).\n */\n// tslint:disable-next-line:no-any\n\nfunction deepBatchConcat(rows) {\n  if (rows === null) {\n    return null;\n  } // use the first item to decide whether to recurse or batch here.\n\n\n  const exampleRow = rows[0];\n\n  if (canTensorify(exampleRow)) {\n    // rows is an array of primitives, Tensors, or arrays.  Batch them.\n    const value = batchConcat(rows);\n    return {\n      value,\n      recurse: false\n    };\n  } // the example row is an object, so recurse into it.\n\n\n  return {\n    value: null,\n    recurse: true\n  };\n}\n/**\n * Assembles a list of same-shaped numbers, number arrays, or Tensors\n * into a single new Tensor where axis 0 is the batch dimension.\n */\n\n\nfunction batchConcat(arrays) {\n  if (arrays.length === 0) {\n    // We can't return an empty Tensor because we don't know the element shape.\n    throw new Error('Can\\'t make a batch of zero elements.');\n  }\n\n  if (arrays[0] instanceof tf.Tensor) {\n    // Input is an array of Tensors\n    return tf.stack(arrays);\n  } else {\n    // Input is a possibly-nested array of numbers.\n    return tf.tensor(arrays);\n  }\n}","map":{"version":3,"sources":["/Users/ryanliang/Downloads/main_movir_picker/node_modules/@tensorflow/tfjs-data/dist/dataset.js"],"names":["tf","seedrandom","iteratorFromConcatenated","iteratorFromFunction","iteratorFromItems","iteratorFromZipped","ZipMismatchMode","canTensorify","deepMapAndAwaitAll","isIterable","Dataset","constructor","size","batch","batchSize","smallLastBatch","base","util","assert","Infinity","Math","ceil","floor","datasetFromIteratorFn","iterator","columnMajorBatch","deepBatchConcat","concatenate","dataset","filter","predicate","x","tidy","forEachAsync","f","map","transform","mapAsync","prefetch","bufferSize","RangeError","repeat","count","undefined","iteratorIterator","value","done","take","skip","shuffle","seed","reshuffleEachIteration","random","alea","now","toString","seed2","int32","toArray","Error","toArrayForTest","MAX_BUFFER_SIZE","iteratorFn","arguments","array","items","length","zip","datasets","Array","isArray","i","min","Object","ds","streams","d","recurse","SHORTEST","rows","exampleRow","batchConcat","arrays","Tensor","stack","tensor"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,KAAKA,EAAZ,MAAoB,uBAApB;AACA,OAAO,KAAKC,UAAZ,MAA4B,YAA5B;AACA,SAASC,wBAAT,EAAmCC,oBAAnC,EAAyDC,iBAAzD,EAA4EC,kBAA5E,EAAgGC,eAAhG,QAAuH,2BAAvH;AACA,SAASC,YAAT,EAAuBC,kBAAvB,EAA2CC,UAA3C,QAA6D,iBAA7D,C,CACA;;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMC,OAAN,CAAc;AACjBC,EAAAA,WAAW,GAAG;AACV,SAAKC,IAAL,GAAY,IAAZ;AACH,GAHgB,CAIjB;AACA;AACA;AACA;AACA;;AACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIC,EAAAA,KAAK,CAACC,SAAD,EAAYC,cAAc,GAAG,IAA7B,EAAmC;AACpC,UAAMC,IAAI,GAAG,IAAb;AACAhB,IAAAA,EAAE,CAACiB,IAAH,CAAQC,MAAR,CAAeJ,SAAS,GAAG,CAA3B,EAA8B,MAAO;AAC7C,QAAQA,SAAU,EADV;AAEA,QAAIF,IAAJ;;AACA,QAAI,KAAKA,IAAL,KAAcO,QAAd,IAA0B,KAAKP,IAAL,IAAa,IAA3C,EAAiD;AAC7C;AACA;AACAA,MAAAA,IAAI,GAAG,KAAKA,IAAZ;AACH,KAJD,MAKK,IAAIG,cAAJ,EAAoB;AACrB;AACA;AACAH,MAAAA,IAAI,GAAGQ,IAAI,CAACC,IAAL,CAAU,KAAKT,IAAL,GAAYE,SAAtB,CAAP;AACH,KAJI,MAKA;AACD;AACA;AACAF,MAAAA,IAAI,GAAGQ,IAAI,CAACE,KAAL,CAAW,KAAKV,IAAL,GAAYE,SAAvB,CAAP;AACH;;AACD,WAAOS,qBAAqB,iCAAC,aAAY;AACrC,aAAO,OAAOP,IAAI,CAACQ,QAAL,EAAP,EACFC,gBADE,CACeX,SADf,EAC0BC,cAD1B,EAC0CW,eAD1C,CAAP;AAEH,KAH2B,GAGzBd,IAHyB,CAA5B;AAIH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIe,EAAAA,WAAW,CAACC,OAAD,EAAU;AACjB,UAAMZ,IAAI,GAAG,IAAb;AACA,QAAIJ,IAAJ;;AACA,QAAI,KAAKA,IAAL,KAAcO,QAAd,IAA0BS,OAAO,CAAChB,IAAR,KAAiBO,QAA/C,EAAyD;AACrD;AACA;AACAP,MAAAA,IAAI,GAAGO,QAAP;AACH,KAJD,MAKK,IAAI,KAAKP,IAAL,IAAa,IAAb,IAAqBgB,OAAO,CAAChB,IAAR,IAAgB,IAAzC,EAA+C;AAChD;AACA;AACAA,MAAAA,IAAI,GAAG,KAAKA,IAAL,GAAYgB,OAAO,CAAChB,IAA3B;AACH,KAJI,MAKA;AACD;AACA;AACAA,MAAAA,IAAI,GAAG,IAAP;AACH;;AACD,WAAOW,qBAAqB,iCAAC;AAAA,aAAY,OAAOP,IAAI,CAACQ,QAAL,EAAP,EAAwBG,WAAxB,OAA0CC,OAAO,CAACJ,QAAR,EAA1C,CAAZ;AAAA,KAAD,GAA4EZ,IAA5E,CAA5B;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIiB,EAAAA,MAAM,CAACC,SAAD,EAAY;AACd,UAAMd,IAAI,GAAG,IAAb;AACA,QAAIJ,IAAJ;;AACA,QAAI,KAAKA,IAAL,KAAcO,QAAlB,EAA4B;AACxB;AACAP,MAAAA,IAAI,GAAGO,QAAP;AACH,KAHD,MAIK;AACD;AACA;AACAP,MAAAA,IAAI,GAAG,IAAP;AACH;;AACD,WAAOW,qBAAqB,iCAAC,aAAY;AACrC,aAAO,OAAOP,IAAI,CAACQ,QAAL,EAAP,EAAwBK,MAAxB,CAA+BE,CAAC,IAAI/B,EAAE,CAACgC,IAAH,CAAQ,MAAMF,SAAS,CAACC,CAAD,CAAvB,CAApC,CAAP;AACH,KAF2B,GAEzBnB,IAFyB,CAA5B;AAGH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACUqB,EAAAA,YAAY,CAACC,CAAD,EAAI;AAAA;;AAAA;AAClB,aAAO,OAAO,KAAI,CAACV,QAAL,EAAP,EAAwBS,YAAxB,CAAqCC,CAArC,CAAP;AADkB;AAErB;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIC,EAAAA,GAAG,CAACC,SAAD,EAAY;AACX,UAAMpB,IAAI,GAAG,IAAb;AACA,WAAOO,qBAAqB,iCAAC,aAAY;AACrC,aAAO,OAAOP,IAAI,CAACQ,QAAL,EAAP,EAAwBW,GAAxB,CAA4BJ,CAAC,IAAI/B,EAAE,CAACgC,IAAH,CAAQ,MAAMI,SAAS,CAACL,CAAD,CAAvB,CAAjC,CAAP;AACH,KAF2B,GAEzB,KAAKnB,IAFoB,CAA5B;AAGH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIyB,EAAAA,QAAQ,CAACD,SAAD,EAAY;AAChB,UAAMpB,IAAI,GAAG,IAAb;AACA,WAAOO,qBAAqB,iCAAC,aAAY;AACrC,aAAO,OAAOP,IAAI,CAACQ,QAAL,EAAP,EAAwBa,QAAxB,CAAiCD,SAAjC,CAAP;AACH,KAF2B,GAEzB,KAAKxB,IAFoB,CAA5B;AAGH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACI0B,EAAAA,QAAQ,CAACC,UAAD,EAAa;AACjB,QAAIA,UAAU,IAAI,IAAlB,EAAwB;AACpB,YAAM,IAAIC,UAAJ,CAAe,2DAAf,CAAN;AACH;;AACD,UAAMxB,IAAI,GAAG,IAAb;AACA,WAAOO,qBAAqB,iCAAC;AAAA,aAAY,OAAOP,IAAI,CAACQ,QAAL,EAAP,EAAwBc,QAAxB,CAAiCC,UAAjC,CAAZ;AAAA,KAAD,GAA2D,KAAK3B,IAAhE,CAA5B;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACI6B,EAAAA,MAAM,CAACC,KAAD,EAAQ;AACV,UAAM1B,IAAI,GAAG,IAAb;AACA,QAAIJ,IAAJ;;AACA,QAAI,KAAKA,IAAL,IAAa,IAAb,IAAqB8B,KAAK,GAAG,CAAjC,EAAoC;AAChC;AACA;AACA;AACA9B,MAAAA,IAAI,GAAG,KAAKA,IAAL,GAAY8B,KAAnB;AACH,KALD,MAMK,IAAIA,KAAK,KAAK,CAAd,EAAiB;AAClB;AACA9B,MAAAA,IAAI,GAAG,CAAP;AACH,KAHI,MAIA,IAAI,KAAKA,IAAL,IAAa,IAAb,KAAsB8B,KAAK,KAAKC,SAAV,IAAuBD,KAAK,GAAG,CAArD,CAAJ,EAA6D;AAC9D;AACA;AACA9B,MAAAA,IAAI,GAAGO,QAAP;AACH,KAJI,MAKA;AACD;AACAP,MAAAA,IAAI,GAAG,IAAP;AACH;;AACD,WAAOW,qBAAqB,iCAAC,aAAY;AACrC,YAAMqB,gBAAgB,GAAGzC,oBAAoB,iCAAC;AAAA,eAAa;AAAE0C,UAAAA,KAAK,QAAQ7B,IAAI,CAACQ,QAAL,EAAf;AAAgCsB,UAAAA,IAAI,EAAE;AAAtC,SAAb;AAAA,OAAD,EAA7C;AACA,aAAO5C,wBAAwB,CAAC0C,gBAAgB,CAACG,IAAjB,CAAsBL,KAAtB,CAAD,CAA/B;AACH,KAH2B,GAGzB9B,IAHyB,CAA5B;AAIH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIoC,EAAAA,IAAI,CAACN,KAAD,EAAQ;AACR,UAAM1B,IAAI,GAAG,IAAb;AACA,QAAIJ,IAAJ;;AACA,QAAI,KAAKA,IAAL,IAAa,IAAb,IAAqB8B,KAAK,IAAI,CAA9B,IAAmC,KAAK9B,IAAL,IAAa8B,KAApD,EAA2D;AACvD;AACA;AACA;AACA9B,MAAAA,IAAI,GAAG,KAAKA,IAAL,GAAY8B,KAAnB;AACH,KALD,MAMK,IAAI,KAAK9B,IAAL,IAAa,IAAb,KACJ,KAAKA,IAAL,GAAY8B,KAAZ,IAAqBA,KAAK,KAAKC,SAA/B,IAA4CD,KAAK,GAAG,CADhD,CAAJ,EACwD;AACzD;AACA;AACA9B,MAAAA,IAAI,GAAG,CAAP;AACH,KALI,MAMA;AACD;AACAA,MAAAA,IAAI,GAAG,IAAP;AACH;;AACD,WAAOW,qBAAqB,iCAAC;AAAA,aAAY,OAAOP,IAAI,CAACQ,QAAL,EAAP,EAAwBwB,IAAxB,CAA6BN,KAA7B,CAAZ;AAAA,KAAD,GAAkD9B,IAAlD,CAA5B;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIqC,EAAAA,OAAO,CAACV,UAAD,EAAaW,IAAb,EAAmBC,sBAAsB,GAAG,IAA5C,EAAkD;AACrD,QAAIZ,UAAU,IAAI,IAAd,IAAsBA,UAAU,GAAG,CAAvC,EAA0C;AACtC,UAAI,KAAK3B,IAAL,IAAa,IAAjB,EAAuB;AACnB,cAAM,IAAI4B,UAAJ,CAAe,0DAAf,CAAN;AACH,OAFD,MAGK;AACD,cAAM,IAAIA,UAAJ,CAAe,+DACjB,6DADiB,GAEjB,yDAFiB,GAGhB,mCAAkC,KAAK5B,IAAK,YAH3C,CAAN;AAIH;AACJ;;AACD,UAAMI,IAAI,GAAG,IAAb;AACA,UAAMoC,MAAM,GAAGnD,UAAU,CAACoD,IAAX,CAAgBH,IAAI,IAAIlD,EAAE,CAACiB,IAAH,CAAQqC,GAAR,GAAcC,QAAd,EAAxB,CAAf;AACA,WAAOhC,qBAAqB,iCAAC,aAAY;AACrC,UAAIiC,KAAK,GAAGJ,MAAM,CAACK,KAAP,EAAZ;;AACA,UAAIN,sBAAJ,EAA4B;AACxBK,QAAAA,KAAK,IAAIJ,MAAM,CAACK,KAAP,EAAT;AACH;;AACD,aAAO,OAAOzC,IAAI,CAACQ,QAAL,EAAP,EAAwByB,OAAxB,CAAgCV,UAAhC,EAA4CiB,KAAK,CAACD,QAAN,EAA5C,CAAP;AACH,KAN2B,GAMzB,KAAK3C,IANoB,CAA5B;AAOH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACImC,EAAAA,IAAI,CAACL,KAAD,EAAQ;AACR,UAAM1B,IAAI,GAAG,IAAb;AACA,QAAIJ,IAAJ;;AACA,QAAI,KAAKA,IAAL,IAAa,IAAb,IAAqB,KAAKA,IAAL,GAAY8B,KAArC,EAA4C;AACxC;AACA;AACA9B,MAAAA,IAAI,GAAG8B,KAAP;AACH,KAJD,MAKK,IAAI,KAAK9B,IAAL,IAAa,IAAb,IAAqB,KAAKA,IAAL,IAAa8B,KAAtC,EAA6C;AAC9C;AACA;AACA9B,MAAAA,IAAI,GAAG,KAAKA,IAAZ;AACH,KAJI,MAKA;AACD;AACAA,MAAAA,IAAI,GAAG,IAAP;AACH;;AACD,WAAOW,qBAAqB,iCAAC;AAAA,aAAY,OAAOP,IAAI,CAACQ,QAAL,EAAP,EAAwBuB,IAAxB,CAA6BL,KAA7B,CAAZ;AAAA,KAAD,GAAkD9B,IAAlD,CAA5B;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACU8C,EAAAA,OAAO,GAAG;AAAA;;AAAA;AACZ,UAAI,MAAI,CAAC9C,IAAL,KAAcO,QAAlB,EAA4B;AACxB,cAAM,IAAIwC,KAAJ,CAAU,gDAAV,CAAN;AACH;;AACD,aAAO,OAAO,MAAI,CAACnC,QAAL,EAAP,EAAwBkC,OAAxB,EAAP;AAJY;AAKf;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACUE,EAAAA,cAAc,GAAG;AAAA;;AAAA;AACnB,UAAI,MAAI,CAAChD,IAAL,KAAcO,QAAlB,EAA4B;AACxB,cAAM,IAAIwC,KAAJ,CAAU,gDAAV,CAAN;AACH;;AACD,aAAO,OAAO,MAAI,CAACnC,QAAL,EAAP,EAAwBoC,cAAxB,EAAP;AAJmB;AAKtB;;AAzbgB,C,CA2brB;;AACAlD,OAAO,CAACmD,eAAR,GAA0B,KAA1B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAAStC,qBAAT,CAA+BuC,UAA/B,EAA2ClD,IAAI,GAAG,IAAlD,EAAwD;AAC3D,SAAO,IAAI,cAAcF,OAAd,CAAsB;AAC7BC,IAAAA,WAAW,GAAG;AACV,YAAM,GAAGoD,SAAT;AACA,WAAKnD,IAAL,GAAYA,IAAZ;AACH;AACD;AACR;AACA;AACA;;;AACcY,IAAAA,QAAQ,GAAG;AAAA;AACb,eAAOsC,UAAU,EAAjB;AADa;AAEhB;;AAX4B,GAA1B,EAAP;AAaH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASE,KAAT,CAAeC,KAAf,EAAsB;AACzB,SAAO1C,qBAAqB,iCAAC;AAAA,WAAYnB,iBAAiB,CAAC6D,KAAD,CAA7B;AAAA,GAAD,GAAuCA,KAAK,CAACC,MAA7C,CAA5B;AACH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASC,GAAT,CAAaC,QAAb,EAAuB;AAC1B;AACA,MAAI,CAAC3D,UAAU,CAAC2D,QAAD,CAAf,EAA2B;AACvB,UAAM,IAAIT,KAAJ,CAAU,mDAAV,CAAN;AACH;;AACD,MAAI/C,IAAJ;;AACA,MAAIyD,KAAK,CAACC,OAAN,CAAcF,QAAd,CAAJ,EAA6B;AACzB,SAAK,IAAIG,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGH,QAAQ,CAACF,MAA7B,EAAqCK,CAAC,EAAtC,EAA0C;AACtC3D,MAAAA,IAAI,GAAGA,IAAI,IAAI,IAAR,GAAewD,QAAQ,CAACG,CAAD,CAAR,CAAY3D,IAA3B,GACHQ,IAAI,CAACoD,GAAL,CAAS5D,IAAT,EAAewD,QAAQ,CAACG,CAAD,CAAR,CAAY3D,IAA3B,CADJ;AAEH;AACJ,GALD,MAMK,IAAIwD,QAAQ,YAAYK,MAAxB,EAAgC;AACjC,SAAK,MAAMC,EAAX,IAAiBN,QAAjB,EAA2B;AACvBxD,MAAAA,IAAI,GAAGA,IAAI,IAAI,IAAR,GAAewD,QAAQ,CAACM,EAAD,CAAR,CAAa9D,IAA5B,GACHQ,IAAI,CAACoD,GAAL,CAAS5D,IAAT,EAAewD,QAAQ,CAACM,EAAD,CAAR,CAAa9D,IAA5B,CADJ;AAEH;AACJ;;AACD,SAAOW,qBAAqB,iCAAC,aAAY;AACrC,UAAMoD,OAAO,SAASnE,kBAAkB,CAAC4D,QAAD,EAAWQ,CAAC,IAAI;AACpD,UAAIA,CAAC,YAAYlE,OAAjB,EAA0B;AACtB,eAAO;AAAEmC,UAAAA,KAAK,EAAE+B,CAAC,CAACpD,QAAF,EAAT;AAAuBqD,UAAAA,OAAO,EAAE;AAAhC,SAAP;AACH,OAFD,MAGK,IAAIpE,UAAU,CAACmE,CAAD,CAAd,EAAmB;AACpB,eAAO;AAAE/B,UAAAA,KAAK,EAAE,IAAT;AAAegC,UAAAA,OAAO,EAAE;AAAxB,SAAP;AACH,OAFI,MAGA;AACD,cAAM,IAAIlB,KAAJ,CAAU,+DACZ,iBADE,CAAN;AAEH;AACJ,KAXuC,CAAxC;AAYA,WAAOtD,kBAAkB,CAACsE,OAAD,EAAUrE,eAAe,CAACwE,QAA1B,CAAzB;AACH,GAd2B,GAczBlE,IAdyB,CAA5B;AAeH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASc,eAAT,CAAyBqD,IAAzB,EAA+B;AAC3B,MAAIA,IAAI,KAAK,IAAb,EAAmB;AACf,WAAO,IAAP;AACH,GAH0B,CAI3B;;;AACA,QAAMC,UAAU,GAAGD,IAAI,CAAC,CAAD,CAAvB;;AACA,MAAIxE,YAAY,CAACyE,UAAD,CAAhB,EAA8B;AAC1B;AACA,UAAMnC,KAAK,GAAGoC,WAAW,CAACF,IAAD,CAAzB;AACA,WAAO;AAAElC,MAAAA,KAAF;AAASgC,MAAAA,OAAO,EAAE;AAAlB,KAAP;AACH,GAV0B,CAW3B;;;AACA,SAAO;AAAEhC,IAAAA,KAAK,EAAE,IAAT;AAAegC,IAAAA,OAAO,EAAE;AAAxB,GAAP;AACH;AACD;AACA;AACA;AACA;;;AACA,SAASI,WAAT,CAAqBC,MAArB,EAA6B;AACzB,MAAIA,MAAM,CAAChB,MAAP,KAAkB,CAAtB,EAAyB;AACrB;AACA,UAAM,IAAIP,KAAJ,CAAU,uCAAV,CAAN;AACH;;AACD,MAAIuB,MAAM,CAAC,CAAD,CAAN,YAAqBlF,EAAE,CAACmF,MAA5B,EAAoC;AAChC;AACA,WAAOnF,EAAE,CAACoF,KAAH,CAASF,MAAT,CAAP;AACH,GAHD,MAIK;AACD;AACA,WAAOlF,EAAE,CAACqF,MAAH,CAAUH,MAAV,CAAP;AACH;AACJ","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport { iteratorFromConcatenated, iteratorFromFunction, iteratorFromItems, iteratorFromZipped, ZipMismatchMode } from './iterators/lazy_iterator';\nimport { canTensorify, deepMapAndAwaitAll, isIterable } from './util/deep_map';\n// TODO(soergel): consider vectorized operations within the pipeline.\n/**\n * Represents a potentially large list of independent data elements (typically\n * 'samples' or 'examples').\n *\n * A 'data example' may be a primitive, an array, a map from string keys to\n * values, or any nested structure of these.\n *\n * A `Dataset` represents an ordered collection of elements, together with a\n * chain of transformations to be performed on those elements. Each\n * transformation is a method of `Dataset` that returns another `Dataset`, so\n * these may be chained, e.g.\n * `const processedDataset = rawDataset.filter(...).map(...).batch(...)`.\n *\n * Data loading and transformation is done in a lazy, streaming fashion.  The\n * dataset may be iterated over multiple times; each iteration starts the data\n * loading anew and recapitulates the transformations.\n *\n * A `Dataset` is typically processed as a stream of unbatched examples --i.e.,\n * its transformations are applied one example at a time. Batching produces a\n * new `Dataset` where each element is a batch. Batching should usually come\n * last in a pipeline, because data transformations are easier to express on a\n * per-example basis than on a per-batch basis.\n *\n * The following code examples are calling `await dataset.forEachAsync(...)` to\n * iterate once over the entire dataset in order to print out the data.\n *\n * @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'}\n */\nexport class Dataset {\n    constructor() {\n        this.size = null;\n    }\n    // TODO(soergel): Make Datasets report whether repeated iterator() calls\n    // produce the same result (e.g., reading from a file) or different results\n    // (e.g., from the webcam).  Currently we don't make this distinction but it\n    // could be important for the user to know.\n    // abstract isDeterministic(): boolean;\n    /**\n     * Groups elements into batches.\n     *\n     * It is assumed that each of the incoming dataset elements has the same\n     * structure-- i.e. the same set of keys at each location in an object\n     * hierarchy.  For each key, the resulting `Dataset` provides a batched\n     * element collecting all of the incoming values for that key.\n     *\n     *  * Incoming primitives are grouped into a 1-D Tensor.\n     *  * Incoming Tensors are grouped into a new Tensor where the 0'th axis is\n     *    the batch dimension.\n     *  * Incoming arrays are converted to Tensor and then batched.\n     *  * A nested array is interpreted as an n-D Tensor, so the batched result\n     *    has n+1 dimensions.\n     *  * An array that cannot be converted to Tensor produces an error.\n     *\n     * If an array should not be batched as a unit, it should first be converted\n     * to an object with integer keys.\n     *\n     * Here are a few examples:\n     *\n     * Batch a dataset of numbers:\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);\n     * await a.forEachAsync(e => e.print());\n     * ```\n     *\n     * Batch a dataset of arrays:\n     * ```js\n     * const b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);\n     * await b.forEachAsync(e => e.print());\n     * ```\n     *\n     * Batch a dataset of objects:\n     * ```js\n     * const c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},\n     *   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},\n     *   {a: 8, b: 18}]).batch(4);\n     * await c.forEachAsync(e => {\n     *   console.log('{');\n     *   for(var key in e) {\n     *     console.log(key+':');\n     *     e[key].print();\n     *   }\n     *   console.log('}');\n     * })\n     * ```\n     *\n     * @param batchSize The number of elements desired per batch.\n     * @param smallLastBatch Whether to emit the final batch when it has fewer\n     *   than batchSize elements. Default true.\n     * @returns A `Dataset`, from which a stream of batches can be obtained.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    batch(batchSize, smallLastBatch = true) {\n        const base = this;\n        tf.util.assert(batchSize > 0, () => `batchSize needs to be positive, but it is\n      ${batchSize}`);\n        let size;\n        if (this.size === Infinity || this.size == null) {\n            // If the size of this dataset is infinity or null, the new size keeps the\n            // same.\n            size = this.size;\n        }\n        else if (smallLastBatch) {\n            // If the size of this dataset is known and include small last batch, the\n            // new size is full batch count plus last batch.\n            size = Math.ceil(this.size / batchSize);\n        }\n        else {\n            // If the size of this dataset is known and not include small last batch,\n            // the new size is full batch count.\n            size = Math.floor(this.size / batchSize);\n        }\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator())\n                .columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat);\n        }, size);\n    }\n    /**\n     * Concatenates this `Dataset` with another.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]);\n     * const b = tf.data.array([4, 5, 6]);\n     * const c = a.concatenate(b);\n     * await c.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param dataset A `Dataset` to be concatenated onto this one.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    concatenate(dataset) {\n        const base = this;\n        let size;\n        if (this.size === Infinity || dataset.size === Infinity) {\n            // If the size of any of these two dataset is infinity, new size is\n            // infinity.\n            size = Infinity;\n        }\n        else if (this.size != null && dataset.size != null) {\n            // If the size of both datasets are known and not infinity, new size is\n            // sum the size of these two datasets.\n            size = this.size + dataset.size;\n        }\n        else {\n            // If neither of these two datasets has infinite size and any of these two\n            // datasets' size is null, the new size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).concatenate(await dataset.iterator()), size);\n    }\n    /**\n     * Filters this dataset according to `predicate`.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n     *   .filter(x => x%2 === 0);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param predicate A function mapping a dataset element to a boolean or a\n     * `Promise` for one.\n     *\n     * @returns A `Dataset` of elements for which the predicate was true.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    filter(predicate) {\n        const base = this;\n        let size;\n        if (this.size === Infinity) {\n            // If the size of this dataset is infinity, new size is infinity\n            size = Infinity;\n        }\n        else {\n            // If this dataset has limited elements, new size is null because it might\n            // exhausted randomly.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).filter(x => tf.tidy(() => predicate(x)));\n        }, size);\n    }\n    /**\n     * Apply a function to every element of the dataset.\n     *\n     * After the function is applied to a dataset element, any Tensors contained\n     * within that element are disposed.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param f A function to apply to each dataset element.\n     * @returns A `Promise` that resolves after all elements have been processed.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    async forEachAsync(f) {\n        return (await this.iterator()).forEachAsync(f);\n    }\n    /**\n     * Maps this dataset through a 1-to-1 transform.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]).map(x => x*x);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param transform A function mapping a dataset element to a transformed\n     *   dataset element.\n     *\n     * @returns A `Dataset` of transformed elements.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    map(transform) {\n        const base = this;\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).map(x => tf.tidy(() => transform(x)));\n        }, this.size);\n    }\n    /**\n     * Maps this dataset through an async 1-to-1 transform.\n     *\n     * ```js\n     * const a =\n     *  tf.data.array([1, 2, 3]).mapAsync(x => new Promise(function(resolve){\n     *    setTimeout(() => {\n     *      resolve(x * x);\n     *    }, Math.random()*1000 + 500);\n     *  }));\n     * console.log(await a.toArray());\n     * ```\n     *\n     * @param transform A function mapping a dataset element to a `Promise` for a\n     *   transformed dataset element.  This transform is responsible for disposing\n     *   any intermediate `Tensor`s, i.e. by wrapping its computation in\n     *   `tf.tidy()`; that cannot be automated here (as it is in the synchronous\n     *   `map()` case).\n     *\n     * @returns A `Dataset` of transformed elements.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    mapAsync(transform) {\n        const base = this;\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).mapAsync(transform);\n        }, this.size);\n    }\n    /**\n     *  Creates a `Dataset` that prefetches elements from this dataset.\n     *\n     * @param bufferSize: An integer specifying the number of elements to be\n     *   prefetched.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    prefetch(bufferSize) {\n        if (bufferSize == null) {\n            throw new RangeError('`Dataset.prefetch()` requires bufferSize to be specified.');\n        }\n        const base = this;\n        return datasetFromIteratorFn(async () => (await base.iterator()).prefetch(bufferSize), this.size);\n    }\n    /**\n     * Repeats this dataset `count` times.\n     *\n     * NOTE: If this dataset is a function of global state (e.g. a random number\n     * generator), then different repetitions may produce different elements.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]).repeat(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: (Optional) An integer, representing the number of times\n     *   the dataset should be repeated. The default behavior (if `count` is\n     *   `undefined` or negative) is for the dataset be repeated indefinitely.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    repeat(count) {\n        const base = this;\n        let size;\n        if (this.size != null && count > 0) {\n            // If this dataset has size and count is positive, new size is current\n            // size multiply count. This also covers the case that current size is\n            // infinity.\n            size = this.size * count;\n        }\n        else if (count === 0) {\n            // If count is 0, new size is 0.\n            size = 0;\n        }\n        else if (this.size != null && (count === undefined || count < 0)) {\n            // If this dataset has size and count is undefined or negative, the\n            // dataset will be repeated indefinitely and new size is infinity.\n            size = Infinity;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => {\n            const iteratorIterator = iteratorFromFunction(async () => ({ value: await base.iterator(), done: false }));\n            return iteratorFromConcatenated(iteratorIterator.take(count));\n        }, size);\n    }\n    /**\n     * Creates a `Dataset` that skips `count` initial elements from this dataset.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: The number of elements of this dataset that should be skipped\n     *   to form the new dataset.  If `count` is greater than the size of this\n     *   dataset, the new dataset will contain no elements.  If `count`\n     *   is `undefined` or negative, skips the entire dataset.\n     *\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    skip(count) {\n        const base = this;\n        let size;\n        if (this.size != null && count >= 0 && this.size >= count) {\n            // If the size of this dataset is greater than count, the new dataset's\n            // size is current size minus skipped size.This also covers the case that\n            // current size is infinity.\n            size = this.size - count;\n        }\n        else if (this.size != null &&\n            (this.size < count || count === undefined || count < 0)) {\n            // If the size of this dataset is smaller than count, or count is\n            // undefined or negative, skips the entire dataset and the new size is 0.\n            size = 0;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).skip(count), size);\n    }\n    /**\n     * Pseudorandomly shuffles the elements of this dataset. This is done in a\n     * streaming manner, by sampling from a given number of prefetched elements.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param bufferSize: An integer specifying the number of elements from this\n     *   dataset from which the new dataset will sample.\n     * @param seed: (Optional) An integer specifying the random seed that will\n     *   be used to create the distribution.\n     * @param reshuffleEachIteration: (Optional) A boolean, which if true\n     *   indicates that the dataset should be pseudorandomly reshuffled each time\n     *   it is iterated over. If false, elements will be returned in the same\n     *   shuffled order on each iteration. (Defaults to `true`.)\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    shuffle(bufferSize, seed, reshuffleEachIteration = true) {\n        if (bufferSize == null || bufferSize < 0) {\n            if (this.size == null) {\n                throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.');\n            }\n            else {\n                throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.  ' +\n                    'If your data fits in main memory (for regular JS objects), ' +\n                    'and/or GPU memory (for `tf.Tensor`s), consider setting ' +\n                    `bufferSize to the dataset size (${this.size} elements)`);\n            }\n        }\n        const base = this;\n        const random = seedrandom.alea(seed || tf.util.now().toString());\n        return datasetFromIteratorFn(async () => {\n            let seed2 = random.int32();\n            if (reshuffleEachIteration) {\n                seed2 += random.int32();\n            }\n            return (await base.iterator()).shuffle(bufferSize, seed2.toString());\n        }, this.size);\n    }\n    /**\n     * Creates a `Dataset` with at most `count` initial elements from this\n     * dataset.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: The number of elements of this dataset that should be taken\n     *   to form the new dataset.  If `count` is `undefined` or negative, or if\n     *   `count` is greater than the size of this dataset, the new dataset will\n     *   contain all elements of this dataset.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    take(count) {\n        const base = this;\n        let size;\n        if (this.size != null && this.size > count) {\n            // If the size of this dataset is greater than count, the new dataset's\n            // size is count.\n            size = count;\n        }\n        else if (this.size != null && this.size <= count) {\n            // If the size of this dataset is equal or smaller than count, the new\n            // dataset's size is the size of this dataset.\n            size = this.size;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).take(count), size);\n    }\n    /**\n     * Collect all elements of this dataset into an array.\n     *\n     * Obviously this will succeed only for small datasets that fit in memory.\n     * Useful for testing and generally should be avoided if possible.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]);\n     * console.log(await a.toArray());\n     * ```\n     *\n     * @returns A Promise for an array of elements, which will resolve\n     *   when a new stream has been obtained and fully consumed.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    async toArray() {\n        if (this.size === Infinity) {\n            throw new Error('Can not convert infinite data stream to array.');\n        }\n        return (await this.iterator()).toArray();\n    }\n    /**\n     * Collect all elements of this dataset into an array with prefetching 100\n     * elements. This is useful for testing, because the prefetch changes the\n     * order in which the Promises are resolved along the processing pipeline.\n     * This may help expose bugs where results are dependent on the order of\n     * Promise resolution rather than on the logical order of the stream (i.e.,\n     * due to hidden mutable state).\n     *\n     * @returns A Promise for an array of elements, which will resolve\n     *   when a new stream has been obtained and fully consumed.\n     */\n    async toArrayForTest() {\n        if (this.size === Infinity) {\n            throw new Error('Can not convert infinite data stream to array.');\n        }\n        return (await this.iterator()).toArrayForTest();\n    }\n}\n// TODO(soergel): deep sharded shuffle, where supported\nDataset.MAX_BUFFER_SIZE = 10000;\n/**\n * Create a `Dataset` defined by a provided iterator() function.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const iter = tf.data.iteratorFromFunction(func);\n * const ds = tf.data.datasetFromIteratorFn(iter);\n * await ds.forEachAsync(e => console.log(e));\n * ```\n */\nexport function datasetFromIteratorFn(iteratorFn, size = null) {\n    return new class extends Dataset {\n        constructor() {\n            super(...arguments);\n            this.size = size;\n        }\n        /*\n         * Provide a new stream of elements.  Note this will also start new streams\n         * from any underlying `Dataset`s.\n         */\n        async iterator() {\n            return iteratorFn();\n        }\n    }();\n}\n/**\n * Create a `Dataset` from an array of elements.\n *\n * Create a Dataset from an array of objects:\n * ```js\n * const a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n *\n * Create a Dataset from an array of numbers:\n * ```js\n * const a = tf.data.array([4, 5, 6]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n * @param items An array of elements that will be parsed as items in a dataset.\n *\n * @doc {heading: 'Data', subheading: 'Creation', namespace: 'data'}\n */\nexport function array(items) {\n    return datasetFromIteratorFn(async () => iteratorFromItems(items), items.length);\n}\n/**\n * Create a `Dataset` by zipping together an array, dict, or nested\n * structure of `Dataset`s (and perhaps additional constants).\n * The underlying datasets must provide elements in a consistent order such that\n * they correspond.\n *\n * The number of elements in the resulting dataset is the same as the size of\n * the smallest dataset in datasets.\n *\n * The nested structure of the `datasets` argument determines the\n * structure of elements in the resulting iterator.\n *\n * Note this means that, given an array of two datasets that produce dict\n * elements, the result is a dataset that produces elements that are arrays\n * of two dicts:\n *\n * Zip an array of datasets:\n * ```js\n * console.log('Zip two datasets of objects:');\n * const ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const ds3 = tf.data.zip([ds1, ds2]);\n * await ds3.forEachAsync(e => console.log(JSON.stringify(e)));\n *\n * // If the goal is to merge the dicts in order to produce elements like\n * // {a: ..., b: ...}, this requires a second step such as:\n * console.log('Merge the objects:');\n * const ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\n * await ds4.forEachAsync(e => console.log(e));\n * ```\n *\n * Zip a dict of datasets:\n * ```js\n * const a = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const b = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const c = tf.data.zip({c: a, d: b});\n * await c.forEachAsync(e => console.log(JSON.stringify(e)));\n * ```\n *\n * @doc {heading: 'Data', subheading: 'Operations', namespace: 'data'}\n */\nexport function zip(datasets) {\n    // manually type-check the argument for JS users\n    if (!isIterable(datasets)) {\n        throw new Error('The argument to zip() must be an object or array.');\n    }\n    let size;\n    if (Array.isArray(datasets)) {\n        for (let i = 0; i < datasets.length; i++) {\n            size = size == null ? datasets[i].size :\n                Math.min(size, datasets[i].size);\n        }\n    }\n    else if (datasets instanceof Object) {\n        for (const ds in datasets) {\n            size = size == null ? datasets[ds].size :\n                Math.min(size, datasets[ds].size);\n        }\n    }\n    return datasetFromIteratorFn(async () => {\n        const streams = await deepMapAndAwaitAll(datasets, d => {\n            if (d instanceof Dataset) {\n                return { value: d.iterator(), recurse: false };\n            }\n            else if (isIterable(d)) {\n                return { value: null, recurse: true };\n            }\n            else {\n                throw new Error('Leaves of the structure passed to zip() must be Datasets, ' +\n                    'not primitives.');\n            }\n        });\n        return iteratorFromZipped(streams, ZipMismatchMode.SHORTEST);\n    }, size);\n}\n/**\n * A zip function for use with deepZip, passed via the columnMajorBatch call.\n *\n * Accepts an array of identically-structured nested elements and either batches\n * them (if they are primitives, numeric arrays, or Tensors) or requests\n * recursion (if not).\n */\n// tslint:disable-next-line:no-any\nfunction deepBatchConcat(rows) {\n    if (rows === null) {\n        return null;\n    }\n    // use the first item to decide whether to recurse or batch here.\n    const exampleRow = rows[0];\n    if (canTensorify(exampleRow)) {\n        // rows is an array of primitives, Tensors, or arrays.  Batch them.\n        const value = batchConcat(rows);\n        return { value, recurse: false };\n    }\n    // the example row is an object, so recurse into it.\n    return { value: null, recurse: true };\n}\n/**\n * Assembles a list of same-shaped numbers, number arrays, or Tensors\n * into a single new Tensor where axis 0 is the batch dimension.\n */\nfunction batchConcat(arrays) {\n    if (arrays.length === 0) {\n        // We can't return an empty Tensor because we don't know the element shape.\n        throw new Error('Can\\'t make a batch of zero elements.');\n    }\n    if (arrays[0] instanceof tf.Tensor) {\n        // Input is an array of Tensors\n        return tf.stack(arrays);\n    }\n    else {\n        // Input is a possibly-nested array of numbers.\n        return tf.tensor(arrays);\n    }\n}\n"]},"metadata":{},"sourceType":"module"}