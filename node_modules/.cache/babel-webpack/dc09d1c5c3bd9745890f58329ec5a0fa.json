{"ast":null,"code":"import _asyncToGenerator from \"/Users/ryanliang/Downloads/main_movir_picker/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/ops';\nimport { square } from '../ops/square';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class AdadeltaOptimizer extends Optimizer {\n  constructor(learningRate, rho, epsilon = null) {\n    super();\n    this.learningRate = learningRate;\n    this.rho = rho;\n    this.epsilon = epsilon;\n    this.accumulatedGrads = [];\n    this.accumulatedUpdates = [];\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n\n      if (this.accumulatedGrads[i] == null) {\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accum_grad`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      if (this.accumulatedUpdates[i] == null) {\n        this.accumulatedUpdates[i] = {\n          originalName: `${name}/accum_var`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n      tidy(() => {\n        const newAccumulatedGrad = add(mul(accumulatedGrad, this.rho), mul(square(gradient), 1 - this.rho));\n        const updates = mul(div(sqrt(add(accumulatedUpdate, this.epsilon)), sqrt(add(accumulatedGrad, this.epsilon))), gradient);\n        const newAccumulatedUpdate = add(mul(accumulatedUpdate, this.rho), mul(square(updates), 1 - this.rho));\n        accumulatedGrad.assign(newAccumulatedGrad);\n        accumulatedUpdate.assign(newAccumulatedUpdate);\n        const newValue = add(mul(updates, -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    if (this.accumulatedUpdates != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n      dispose(this.accumulatedUpdates.map(v => v.variable));\n    }\n  }\n\n  getWeights() {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      // Order matters for Python compatibility.\n      const variables = [..._this.accumulatedGrads, ..._this.accumulatedUpdates];\n      return [yield _this.saveIterations()].concat(variables.map(v => ({\n        name: v.originalName,\n        tensor: v.variable\n      })));\n    })();\n  }\n\n  setWeights(weightValues) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      weightValues = yield _this2.extractIterations(weightValues);\n      const variableCount = weightValues.length / 2;\n      const trainable = false;\n      _this2.accumulatedGrads = weightValues.slice(0, variableCount).map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n      _this2.accumulatedUpdates = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'rho': this.rho,\n      'epsilon': this.epsilon\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['rho'], config['epsilon']);\n  }\n\n}\n/** @nocollapse */\n\nAdadeltaOptimizer.className = 'Adadelta'; // Name matters for Python compatibility.\n\nregisterClass(AdadeltaOptimizer);","map":{"version":3,"sources":["/Users/ryanliang/Downloads/main_movir_picker/node_modules/@tensorflow/tfjs-core/dist/optimizers/adadelta_optimizer.js"],"names":["ENGINE","dispose","tidy","add","div","mul","sqrt","square","zerosLike","registerClass","Optimizer","AdadeltaOptimizer","constructor","learningRate","rho","epsilon","accumulatedGrads","accumulatedUpdates","backend","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","originalName","variable","gradient","tensor","accumulatedGrad","accumulatedUpdate","newAccumulatedGrad","updates","newAccumulatedUpdate","assign","newValue","incrementIterations","v","getWeights","variables","saveIterations","concat","setWeights","weightValues","extractIterations","variableCount","length","slice","getConfig","fromConfig","cls","config","className"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,MAAT,QAAuB,WAAvB;AACA,SAASC,OAAT,EAAkBC,IAAlB,QAA8B,YAA9B;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,IAAT,QAAqB,YAArB;AACA,SAASC,MAAT,QAAuB,eAAvB;AACA,SAASC,SAAT,QAA0B,mBAA1B;AACA,SAASC,aAAT,QAA8B,kBAA9B;AACA,SAASC,SAAT,QAA0B,aAA1B;AACA;;AACA,OAAO,MAAMC,iBAAN,SAAgCD,SAAhC,CAA0C;AAC7CE,EAAAA,WAAW,CAACC,YAAD,EAAeC,GAAf,EAAoBC,OAAO,GAAG,IAA9B,EAAoC;AAC3C;AACA,SAAKF,YAAL,GAAoBA,YAApB;AACA,SAAKC,GAAL,GAAWA,GAAX;AACA,SAAKC,OAAL,GAAeA,OAAf;AACA,SAAKC,gBAAL,GAAwB,EAAxB;AACA,SAAKC,kBAAL,GAA0B,EAA1B;;AACA,QAAIF,OAAO,IAAI,IAAf,EAAqB;AACjB,WAAKA,OAAL,GAAef,MAAM,CAACkB,OAAP,CAAeH,OAAf,EAAf;AACH;AACJ;;AACDI,EAAAA,cAAc,CAACC,iBAAD,EAAoB;AAC9B,UAAMC,aAAa,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IAClBA,iBAAiB,CAACI,GAAlB,CAAsBC,IAAI,IAAIA,IAAI,CAACC,IAAnC,CADkB,GAElBC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;AAGAC,IAAAA,aAAa,CAACQ,OAAd,CAAsB,CAACH,IAAD,EAAOI,CAAP,KAAa;AAC/B,YAAMC,KAAK,GAAG/B,MAAM,CAACgC,mBAAP,CAA2BN,IAA3B,CAAd;AACA,YAAMO,SAAS,GAAG,KAAlB;;AACA,UAAI,KAAKjB,gBAAL,CAAsBc,CAAtB,KAA4B,IAAhC,EAAsC;AAClC,aAAKd,gBAAL,CAAsBc,CAAtB,IAA2B;AACvBI,UAAAA,YAAY,EAAG,GAAER,IAAK,aADC;AAEvBS,UAAAA,QAAQ,EAAEjC,IAAI,CAAC,MAAMM,SAAS,CAACuB,KAAD,CAAT,CAAiBI,QAAjB,CAA0BF,SAA1B,CAAP;AAFS,SAA3B;AAIH;;AACD,UAAI,KAAKhB,kBAAL,CAAwBa,CAAxB,KAA8B,IAAlC,EAAwC;AACpC,aAAKb,kBAAL,CAAwBa,CAAxB,IAA6B;AACzBI,UAAAA,YAAY,EAAG,GAAER,IAAK,YADG;AAEzBS,UAAAA,QAAQ,EAAEjC,IAAI,CAAC,MAAMM,SAAS,CAACuB,KAAD,CAAT,CAAiBI,QAAjB,CAA0BF,SAA1B,CAAP;AAFW,SAA7B;AAIH;;AACD,YAAMG,QAAQ,GAAGd,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACU,CAAD,CAAjB,CAAqBO,MADR,GAEbjB,iBAAiB,CAACM,IAAD,CAFrB;;AAGA,UAAIU,QAAQ,IAAI,IAAhB,EAAsB;AAClB;AACH;;AACD,YAAME,eAAe,GAAG,KAAKtB,gBAAL,CAAsBc,CAAtB,EAAyBK,QAAjD;AACA,YAAMI,iBAAiB,GAAG,KAAKtB,kBAAL,CAAwBa,CAAxB,EAA2BK,QAArD;AACAjC,MAAAA,IAAI,CAAC,MAAM;AACP,cAAMsC,kBAAkB,GAAGrC,GAAG,CAACE,GAAG,CAACiC,eAAD,EAAkB,KAAKxB,GAAvB,CAAJ,EAAiCT,GAAG,CAACE,MAAM,CAAC6B,QAAD,CAAP,EAAmB,IAAI,KAAKtB,GAA5B,CAApC,CAA9B;AACA,cAAM2B,OAAO,GAAGpC,GAAG,CAACD,GAAG,CAACE,IAAI,CAACH,GAAG,CAACoC,iBAAD,EAAoB,KAAKxB,OAAzB,CAAJ,CAAL,EAA6CT,IAAI,CAACH,GAAG,CAACmC,eAAD,EAAkB,KAAKvB,OAAvB,CAAJ,CAAjD,CAAJ,EAA4FqB,QAA5F,CAAnB;AACA,cAAMM,oBAAoB,GAAGvC,GAAG,CAACE,GAAG,CAACkC,iBAAD,EAAoB,KAAKzB,GAAzB,CAAJ,EAAmCT,GAAG,CAACE,MAAM,CAACkC,OAAD,CAAP,EAAkB,IAAI,KAAK3B,GAA3B,CAAtC,CAAhC;AACAwB,QAAAA,eAAe,CAACK,MAAhB,CAAuBH,kBAAvB;AACAD,QAAAA,iBAAiB,CAACI,MAAlB,CAAyBD,oBAAzB;AACA,cAAME,QAAQ,GAAGzC,GAAG,CAACE,GAAG,CAACoC,OAAD,EAAU,CAAC,KAAK5B,YAAhB,CAAJ,EAAmCkB,KAAnC,CAApB;AACAA,QAAAA,KAAK,CAACY,MAAN,CAAaC,QAAb;AACH,OARG,CAAJ;AASH,KAhCD;AAiCA,SAAKC,mBAAL;AACH;;AACD5C,EAAAA,OAAO,GAAG;AACN,QAAI,KAAKgB,kBAAL,IAA2B,IAA/B,EAAqC;AACjChB,MAAAA,OAAO,CAAC,KAAKe,gBAAL,CAAsBQ,GAAtB,CAA0BsB,CAAC,IAAIA,CAAC,CAACX,QAAjC,CAAD,CAAP;AACAlC,MAAAA,OAAO,CAAC,KAAKgB,kBAAL,CAAwBO,GAAxB,CAA4BsB,CAAC,IAAIA,CAAC,CAACX,QAAnC,CAAD,CAAP;AACH;AACJ;;AACKY,EAAAA,UAAU,GAAG;AAAA;;AAAA;AACf;AACA,YAAMC,SAAS,GAAG,CAAC,GAAG,KAAI,CAAChC,gBAAT,EAA2B,GAAG,KAAI,CAACC,kBAAnC,CAAlB;AACA,aAAO,OAAO,KAAI,CAACgC,cAAL,EAAP,EAA8BC,MAA9B,CAAqCF,SAAS,CAACxB,GAAV,CAAcsB,CAAC,KAAK;AAAEpB,QAAAA,IAAI,EAAEoB,CAAC,CAACZ,YAAV;AAAwBG,QAAAA,MAAM,EAAES,CAAC,CAACX;AAAlC,OAAL,CAAf,CAArC,CAAP;AAHe;AAIlB;;AACKgB,EAAAA,UAAU,CAACC,YAAD,EAAe;AAAA;;AAAA;AAC3BA,MAAAA,YAAY,SAAS,MAAI,CAACC,iBAAL,CAAuBD,YAAvB,CAArB;AACA,YAAME,aAAa,GAAGF,YAAY,CAACG,MAAb,GAAsB,CAA5C;AACA,YAAMtB,SAAS,GAAG,KAAlB;AACA,MAAA,MAAI,CAACjB,gBAAL,GACIoC,YAAY,CAACI,KAAb,CAAmB,CAAnB,EAAsBF,aAAtB,EAAqC9B,GAArC,CAAyCsB,CAAC,KAAK;AAC3CZ,QAAAA,YAAY,EAAEY,CAAC,CAACpB,IAD2B;AAE3CS,QAAAA,QAAQ,EAAEW,CAAC,CAACT,MAAF,CAASF,QAAT,CAAkBF,SAAlB;AAFiC,OAAL,CAA1C,CADJ;AAKA,MAAA,MAAI,CAAChB,kBAAL,GACImC,YAAY,CAACI,KAAb,CAAmBF,aAAnB,EAAkCA,aAAa,GAAG,CAAlD,EACK9B,GADL,CACSsB,CAAC,KAAK;AACXZ,QAAAA,YAAY,EAAEY,CAAC,CAACpB,IADL;AAEXS,QAAAA,QAAQ,EAAEW,CAAC,CAACT,MAAF,CAASF,QAAT,CAAkBF,SAAlB;AAFC,OAAL,CADV,CADJ;AAT2B;AAe9B;;AACDwB,EAAAA,SAAS,GAAG;AACR,WAAO;AACH,sBAAgB,KAAK5C,YADlB;AAEH,aAAO,KAAKC,GAFT;AAGH,iBAAW,KAAKC;AAHb,KAAP;AAKH;AACD;;;AACiB,SAAV2C,UAAU,CAACC,GAAD,EAAMC,MAAN,EAAc;AAC3B,WAAO,IAAID,GAAJ,CAAQC,MAAM,CAAC,cAAD,CAAd,EAAgCA,MAAM,CAAC,KAAD,CAAtC,EAA+CA,MAAM,CAAC,SAAD,CAArD,CAAP;AACH;;AAxF4C;AA0FjD;;AACAjD,iBAAiB,CAACkD,SAAlB,GAA8B,UAA9B,C,CAA0C;;AAC1CpD,aAAa,CAACE,iBAAD,CAAb","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/ops';\nimport { square } from '../ops/square';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class AdadeltaOptimizer extends Optimizer {\n    constructor(learningRate, rho, epsilon = null) {\n        super();\n        this.learningRate = learningRate;\n        this.rho = rho;\n        this.epsilon = epsilon;\n        this.accumulatedGrads = [];\n        this.accumulatedUpdates = [];\n        if (epsilon == null) {\n            this.epsilon = ENGINE.backend.epsilon();\n        }\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        variableNames.forEach((name, i) => {\n            const value = ENGINE.registeredVariables[name];\n            const trainable = false;\n            if (this.accumulatedGrads[i] == null) {\n                this.accumulatedGrads[i] = {\n                    originalName: `${name}/accum_grad`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            if (this.accumulatedUpdates[i] == null) {\n                this.accumulatedUpdates[i] = {\n                    originalName: `${name}/accum_var`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const accumulatedGrad = this.accumulatedGrads[i].variable;\n            const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n            tidy(() => {\n                const newAccumulatedGrad = add(mul(accumulatedGrad, this.rho), mul(square(gradient), 1 - this.rho));\n                const updates = mul(div(sqrt(add(accumulatedUpdate, this.epsilon)), sqrt(add(accumulatedGrad, this.epsilon))), gradient);\n                const newAccumulatedUpdate = add(mul(accumulatedUpdate, this.rho), mul(square(updates), 1 - this.rho));\n                accumulatedGrad.assign(newAccumulatedGrad);\n                accumulatedUpdate.assign(newAccumulatedUpdate);\n                const newValue = add(mul(updates, -this.learningRate), value);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        if (this.accumulatedUpdates != null) {\n            dispose(this.accumulatedGrads.map(v => v.variable));\n            dispose(this.accumulatedUpdates.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        const variables = [...this.accumulatedGrads, ...this.accumulatedUpdates];\n        return [await this.saveIterations()].concat(variables.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        const variableCount = weightValues.length / 2;\n        const trainable = false;\n        this.accumulatedGrads =\n            weightValues.slice(0, variableCount).map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n        this.accumulatedUpdates =\n            weightValues.slice(variableCount, variableCount * 2)\n                .map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'rho': this.rho,\n            'epsilon': this.epsilon\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['rho'], config['epsilon']);\n    }\n}\n/** @nocollapse */\nAdadeltaOptimizer.className = 'Adadelta'; // Name matters for Python compatibility.\nregisterClass(AdadeltaOptimizer);\n"]},"metadata":{},"sourceType":"module"}