{"ast":null,"code":"import _asyncToGenerator from \"/Users/ryanliang/Downloads/main_movir_picker/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\n\nexport class Optimizer extends Serializable {\n  /**\n   * Executes `f()` and minimizes the scalar output of `f()` by computing\n   * gradients of y with respect to the list of trainable variables provided by\n   * `varList`. If no list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to minimize.\n   * @param returnCost Whether to return the scalar cost value produced by\n   * executing `f()`.\n   * @param varList An optional list of variables to update. If specified, only\n   * the trainable variables in varList will be updated by minimize. Defaults to\n   * all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  minimize(f, returnCost = false, varList) {\n    const {\n      value,\n      grads\n    } = this.computeGradients(f, varList);\n\n    if (varList != null) {\n      const gradArray = varList.map(v => ({\n        name: v.name,\n        tensor: grads[v.name]\n      }));\n      this.applyGradients(gradArray);\n    } else {\n      this.applyGradients(grads);\n    } // Dispose gradients.\n\n\n    dispose(grads);\n\n    if (returnCost) {\n      return value;\n    } else {\n      value.dispose();\n      return null;\n    }\n  }\n  /**\n   * The number of iterations that this optimizer instance has been invoked for.\n   */\n\n\n  get iterations() {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n\n    return this.iterations_;\n  }\n\n  incrementIterations() {\n    this.iterations_ = this.iterations + 1;\n  }\n  /**\n   * Executes f() and computes the gradient of the scalar output of f() with\n   * respect to the list of trainable variables provided by `varList`. If no\n   * list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to use for computing\n   * gradients with respect to variables.\n   * @param varList An optional list of variables to compute gradients with\n   * respect to. If specified, only the trainable variables in varList will have\n   * gradients computed with respect to. Defaults to all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n\n\n  computeGradients(f, varList) {\n    return variableGrads(f, varList);\n  }\n  /**\n   * Dispose the variables (if any) owned by this optimizer instance.\n   */\n\n\n  dispose() {\n    if (this.iterations_ != null) {\n      dispose(this.iterations_);\n    }\n  }\n\n  saveIterations() {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this.iterations_ == null) {\n        _this.iterations_ = 0;\n      }\n\n      return {\n        name: 'iter',\n        // TODO(cais): Use 'int64' type when available.\n        tensor: scalar(_this.iterations_, 'int32')\n      };\n    })();\n  }\n\n  getWeights() {\n    return _asyncToGenerator(function* () {\n      throw new Error('getWeights() is not implemented for this optimizer yet.');\n    })();\n  }\n\n  setWeights(weightValues) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      throw new Error(`setWeights() is not implemented for this optimizer class ` + `${_this2.getClassName()}`);\n    })();\n  }\n  /**\n   * Extract the first element of the weight values and set it\n   * as the iterations counter variable of this instance of optimizer.\n   *\n   * @param weightValues\n   * @returns Weight values with the first element consumed and excluded.\n   */\n\n\n  extractIterations(weightValues) {\n    var _this3 = this;\n\n    return _asyncToGenerator(function* () {\n      _this3.iterations_ = (yield weightValues[0].tensor.data())[0];\n      return weightValues.slice(1);\n    })();\n  }\n\n}\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: instance => {\n    return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;\n  }\n});","map":{"version":3,"sources":["/Users/ryanliang/Downloads/main_movir_picker/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer.js"],"names":["dispose","variableGrads","scalar","Serializable","Optimizer","minimize","f","returnCost","varList","value","grads","computeGradients","gradArray","map","v","name","tensor","applyGradients","iterations","iterations_","incrementIterations","saveIterations","getWeights","Error","setWeights","weightValues","getClassName","extractIterations","data","slice","Object","defineProperty","Symbol","hasInstance","instance"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,OAAT,QAAwB,YAAxB;AACA,SAASC,aAAT,QAA8B,cAA9B;AACA,SAASC,MAAT,QAAuB,YAAvB;AACA,SAASC,YAAT,QAA6B,kBAA7B;AACA;;AACA,OAAO,MAAMC,SAAN,SAAwBD,YAAxB,CAAqC;AACxC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACIE,EAAAA,QAAQ,CAACC,CAAD,EAAIC,UAAU,GAAG,KAAjB,EAAwBC,OAAxB,EAAiC;AACrC,UAAM;AAAEC,MAAAA,KAAF;AAASC,MAAAA;AAAT,QAAmB,KAAKC,gBAAL,CAAsBL,CAAtB,EAAyBE,OAAzB,CAAzB;;AACA,QAAIA,OAAO,IAAI,IAAf,EAAqB;AACjB,YAAMI,SAAS,GAAGJ,OAAO,CAACK,GAAR,CAAYC,CAAC,KAAK;AAAEC,QAAAA,IAAI,EAAED,CAAC,CAACC,IAAV;AAAgBC,QAAAA,MAAM,EAAEN,KAAK,CAACI,CAAC,CAACC,IAAH;AAA7B,OAAL,CAAb,CAAlB;AACA,WAAKE,cAAL,CAAoBL,SAApB;AACH,KAHD,MAIK;AACD,WAAKK,cAAL,CAAoBP,KAApB;AACH,KARoC,CASrC;;;AACAV,IAAAA,OAAO,CAACU,KAAD,CAAP;;AACA,QAAIH,UAAJ,EAAgB;AACZ,aAAOE,KAAP;AACH,KAFD,MAGK;AACDA,MAAAA,KAAK,CAACT,OAAN;AACA,aAAO,IAAP;AACH;AACJ;AACD;AACJ;AACA;;;AACkB,MAAVkB,UAAU,GAAG;AACb,QAAI,KAAKC,WAAL,IAAoB,IAAxB,EAA8B;AAC1B,WAAKA,WAAL,GAAmB,CAAnB;AACH;;AACD,WAAO,KAAKA,WAAZ;AACH;;AACDC,EAAAA,mBAAmB,GAAG;AAClB,SAAKD,WAAL,GAAmB,KAAKD,UAAL,GAAkB,CAArC;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACIP,EAAAA,gBAAgB,CAACL,CAAD,EAAIE,OAAJ,EAAa;AACzB,WAAOP,aAAa,CAACK,CAAD,EAAIE,OAAJ,CAApB;AACH;AACD;AACJ;AACA;;;AACIR,EAAAA,OAAO,GAAG;AACN,QAAI,KAAKmB,WAAL,IAAoB,IAAxB,EAA8B;AAC1BnB,MAAAA,OAAO,CAAC,KAAKmB,WAAN,CAAP;AACH;AACJ;;AACKE,EAAAA,cAAc,GAAG;AAAA;;AAAA;AACnB,UAAI,KAAI,CAACF,WAAL,IAAoB,IAAxB,EAA8B;AAC1B,QAAA,KAAI,CAACA,WAAL,GAAmB,CAAnB;AACH;;AACD,aAAO;AACHJ,QAAAA,IAAI,EAAE,MADH;AAEH;AACAC,QAAAA,MAAM,EAAEd,MAAM,CAAC,KAAI,CAACiB,WAAN,EAAmB,OAAnB;AAHX,OAAP;AAJmB;AAStB;;AACKG,EAAAA,UAAU,GAAG;AAAA;AACf,YAAM,IAAIC,KAAJ,CAAU,yDAAV,CAAN;AADe;AAElB;;AACKC,EAAAA,UAAU,CAACC,YAAD,EAAe;AAAA;;AAAA;AAC3B,YAAM,IAAIF,KAAJ,CAAW,2DAAD,GACX,GAAE,MAAI,CAACG,YAAL,EAAoB,EADrB,CAAN;AAD2B;AAG9B;AACD;AACJ;AACA;AACA;AACA;AACA;AACA;;;AACUC,EAAAA,iBAAiB,CAACF,YAAD,EAAe;AAAA;;AAAA;AAClC,MAAA,MAAI,CAACN,WAAL,GAAmB,OAAOM,YAAY,CAAC,CAAD,CAAZ,CAAgBT,MAAhB,CAAuBY,IAAvB,EAAP,EAAsC,CAAtC,CAAnB;AACA,aAAOH,YAAY,CAACI,KAAb,CAAmB,CAAnB,CAAP;AAFkC;AAGrC;;AAjGuC;AAmG5CC,MAAM,CAACC,cAAP,CAAsB3B,SAAtB,EAAiC4B,MAAM,CAACC,WAAxC,EAAqD;AACjDxB,EAAAA,KAAK,EAAGyB,QAAD,IAAc;AACjB,WAAOA,QAAQ,CAAC7B,QAAT,IAAqB,IAArB,IAA6B6B,QAAQ,CAACvB,gBAAT,IAA6B,IAA1D,IACHuB,QAAQ,CAACjB,cAAT,IAA2B,IAD/B;AAEH;AAJgD,CAArD","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\nexport class Optimizer extends Serializable {\n    /**\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\n     * gradients of y with respect to the list of trainable variables provided by\n     * `varList`. If no list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to minimize.\n     * @param returnCost Whether to return the scalar cost value produced by\n     * executing `f()`.\n     * @param varList An optional list of variables to update. If specified, only\n     * the trainable variables in varList will be updated by minimize. Defaults to\n     * all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    minimize(f, returnCost = false, varList) {\n        const { value, grads } = this.computeGradients(f, varList);\n        if (varList != null) {\n            const gradArray = varList.map(v => ({ name: v.name, tensor: grads[v.name] }));\n            this.applyGradients(gradArray);\n        }\n        else {\n            this.applyGradients(grads);\n        }\n        // Dispose gradients.\n        dispose(grads);\n        if (returnCost) {\n            return value;\n        }\n        else {\n            value.dispose();\n            return null;\n        }\n    }\n    /**\n     * The number of iterations that this optimizer instance has been invoked for.\n     */\n    get iterations() {\n        if (this.iterations_ == null) {\n            this.iterations_ = 0;\n        }\n        return this.iterations_;\n    }\n    incrementIterations() {\n        this.iterations_ = this.iterations + 1;\n    }\n    /**\n     * Executes f() and computes the gradient of the scalar output of f() with\n     * respect to the list of trainable variables provided by `varList`. If no\n     * list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to use for computing\n     * gradients with respect to variables.\n     * @param varList An optional list of variables to compute gradients with\n     * respect to. If specified, only the trainable variables in varList will have\n     * gradients computed with respect to. Defaults to all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    computeGradients(f, varList) {\n        return variableGrads(f, varList);\n    }\n    /**\n     * Dispose the variables (if any) owned by this optimizer instance.\n     */\n    dispose() {\n        if (this.iterations_ != null) {\n            dispose(this.iterations_);\n        }\n    }\n    async saveIterations() {\n        if (this.iterations_ == null) {\n            this.iterations_ = 0;\n        }\n        return {\n            name: 'iter',\n            // TODO(cais): Use 'int64' type when available.\n            tensor: scalar(this.iterations_, 'int32')\n        };\n    }\n    async getWeights() {\n        throw new Error('getWeights() is not implemented for this optimizer yet.');\n    }\n    async setWeights(weightValues) {\n        throw new Error(`setWeights() is not implemented for this optimizer class ` +\n            `${this.getClassName()}`);\n    }\n    /**\n     * Extract the first element of the weight values and set it\n     * as the iterations counter variable of this instance of optimizer.\n     *\n     * @param weightValues\n     * @returns Weight values with the first element consumed and excluded.\n     */\n    async extractIterations(weightValues) {\n        this.iterations_ = (await weightValues[0].tensor.data())[0];\n        return weightValues.slice(1);\n    }\n}\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n    value: (instance) => {\n        return instance.minimize != null && instance.computeGradients != null &&\n            instance.applyGradients != null;\n    }\n});\n"]},"metadata":{},"sourceType":"module"}